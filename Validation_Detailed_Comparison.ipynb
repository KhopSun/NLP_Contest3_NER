{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detailed Validation Set Comparison\n",
    "\n",
    "Shows detailed per-entity-type results for:\n",
    "- Your model (KB-NER)\n",
    "- Friend's model (RoBERTa)\n",
    "- Hybrid Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Imports loaded\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import utils\n",
    "\n",
    "print(\"‚úì Imports loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10036 ground truth examples\n"
     ]
    }
   ],
   "source": [
    "# Load ground truth\n",
    "ground_truth = {}\n",
    "tokens_dict = {}\n",
    "\n",
    "with open('val_split.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        ex = json.loads(line)\n",
    "        ground_truth[ex['id']] = ex['ner_tags']\n",
    "        tokens_dict[ex['id']] = ex['tokens']\n",
    "\n",
    "print(f\"Loaded {len(ground_truth)} ground truth examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded predictions from all models\n"
     ]
    }
   ],
   "source": [
    "# Load predictions from all three models\n",
    "your_predictions = {}\n",
    "with open('val_predictions_xlmr_crf.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        ex = json.loads(line)\n",
    "        your_predictions[ex['id']] = ex['ner_tags']\n",
    "\n",
    "friend_predictions = {}\n",
    "with open('val_data_roberta_predictions.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        ex = json.loads(line)\n",
    "        friend_predictions[ex['id']] = ex['ner_tags']\n",
    "\n",
    "ensemble_predictions = {}\n",
    "with open('val_predictions_ensemble_improved.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        ex = json.loads(line)\n",
    "        ensemble_predictions[ex['id']] = ex['ner_tags']\n",
    "\n",
    "print(f\"Loaded predictions from all models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned examples: 10036\n"
     ]
    }
   ],
   "source": [
    "# Get aligned IDs\n",
    "aligned_ids = sorted(set(ground_truth.keys()) & \n",
    "                     set(your_predictions.keys()) & \n",
    "                     set(friend_predictions.keys()) & \n",
    "                     set(ensemble_predictions.keys()))\n",
    "\n",
    "print(f\"Aligned examples: {len(aligned_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Prepared data for evaluation\n"
     ]
    }
   ],
   "source": [
    "# Prepare lists for evaluation\n",
    "gt_tags_list = [ground_truth[id] for id in aligned_ids]\n",
    "tokens_list = [tokens_dict[id] for id in aligned_ids]\n",
    "\n",
    "your_pred_list = [your_predictions[id] for id in aligned_ids]\n",
    "friend_pred_list = [friend_predictions[id] for id in aligned_ids]\n",
    "ensemble_pred_list = [ensemble_predictions[id] for id in aligned_ids]\n",
    "\n",
    "print(\"‚úì Prepared data for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Model (KB-NER) - Detailed Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "YOUR MODEL: Knowledge-Augmented XLM-RoBERTa--CRF\n",
      "================================================================================\n",
      "================================================================================\n",
      "ENTITY-SPAN LEVEL EVALUATION REPORT: Your Model (KB-NER)\n",
      "================================================================================\n",
      "\n",
      "OVERALL METRICS:\n",
      "  Precision: 0.8249\n",
      "  Recall:    0.8275\n",
      "  F1 Score:  0.8262\n",
      "\n",
      "  True Positives:  11151\n",
      "  False Positives: 2367\n",
      "  False Negatives: 2324\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "PER-ENTITY-TYPE METRICS:\n",
      "--------------------------------------------------------------------------------\n",
      "Entity Type          Precision    Recall       F1           Support   \n",
      "--------------------------------------------------------------------------------\n",
      "Artist               0.8154       0.8420       0.8285       2849      \n",
      "Facility             0.8428       0.8440       0.8434       1487      \n",
      "HumanSettlement      0.9575       0.9586       0.9580       3476      \n",
      "ORG                  0.8176       0.8193       0.8185       1893      \n",
      "OtherPER             0.6540       0.6863       0.6698       1779      \n",
      "Politician           0.7516       0.6690       0.7079       1402      \n",
      "PublicCorp           0.7647       0.7725       0.7686       589       \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"YOUR MODEL: Knowledge-Augmented XLM-RoBERTa--CRF\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "utils.print_evaluation_report(gt_tags_list, your_pred_list, tokens_list, \"Your Model (KB-NER)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Friend's Model (RoBERTa) - Detailed Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FRIEND'S MODEL: RoBERTa-based NER\n",
      "================================================================================\n",
      "================================================================================\n",
      "ENTITY-SPAN LEVEL EVALUATION REPORT: Friend's Model (RoBERTa)\n",
      "================================================================================\n",
      "\n",
      "OVERALL METRICS:\n",
      "  Precision: 0.7984\n",
      "  Recall:    0.8056\n",
      "  F1 Score:  0.8020\n",
      "\n",
      "  True Positives:  10856\n",
      "  False Positives: 2742\n",
      "  False Negatives: 2619\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "PER-ENTITY-TYPE METRICS:\n",
      "--------------------------------------------------------------------------------\n",
      "Entity Type          Precision    Recall       F1           Support   \n",
      "--------------------------------------------------------------------------------\n",
      "Artist               0.8009       0.8287       0.8146       2849      \n",
      "Facility             0.8121       0.7848       0.7982       1487      \n",
      "HumanSettlement      0.9297       0.9436       0.9366       3476      \n",
      "ORG                  0.7499       0.7950       0.7718       1893      \n",
      "OtherPER             0.6446       0.6565       0.6505       1779      \n",
      "Politician           0.7218       0.6755       0.6979       1402      \n",
      "PublicCorp           0.7726       0.7267       0.7489       589       \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FRIEND'S MODEL: RoBERTa-based NER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "utils.print_evaluation_report(gt_tags_list, friend_pred_list, tokens_list, \"Friend's Model (RoBERTa)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Ensemble - Detailed Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HYBRID ENSEMBLE\n",
      "================================================================================\n",
      "================================================================================\n",
      "ENTITY-SPAN LEVEL EVALUATION REPORT: Hybrid Ensemble\n",
      "================================================================================\n",
      "\n",
      "OVERALL METRICS:\n",
      "  Precision: 0.8242\n",
      "  Recall:    0.8278\n",
      "  F1 Score:  0.8260\n",
      "\n",
      "  True Positives:  11155\n",
      "  False Positives: 2379\n",
      "  False Negatives: 2320\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "PER-ENTITY-TYPE METRICS:\n",
      "--------------------------------------------------------------------------------\n",
      "Entity Type          Precision    Recall       F1           Support   \n",
      "--------------------------------------------------------------------------------\n",
      "Artist               0.8154       0.8417       0.8283       2849      \n",
      "Facility             0.8434       0.8440       0.8437       1487      \n",
      "HumanSettlement      0.9577       0.9583       0.9580       3476      \n",
      "ORG                  0.8190       0.8151       0.8171       1893      \n",
      "OtherPER             0.6540       0.6863       0.6698       1779      \n",
      "Politician           0.7522       0.6690       0.7082       1402      \n",
      "PublicCorp           0.7456       0.7963       0.7701       589       \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYBRID ENSEMBLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "utils.print_evaluation_report(gt_tags_list, ensemble_pred_list, tokens_list, \"Hybrid Ensemble\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Side-by-Side Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Calculated per-type metrics for all models\n"
     ]
    }
   ],
   "source": [
    "# Get all entity types\n",
    "entity_types = set()\n",
    "for tags in ground_truth.values():\n",
    "    for tag in tags:\n",
    "        if tag != 'O' and tag.startswith('B-'):\n",
    "            entity_types.add(tag[2:])\n",
    "\n",
    "entity_types = sorted(entity_types)\n",
    "\n",
    "# Calculate per-entity-type metrics for all models\n",
    "def calculate_per_type_metrics(gt_tags_list, pred_tags_list, tokens_list, entity_types):\n",
    "    type_metrics = {}\n",
    "    \n",
    "    for entity_type in entity_types:\n",
    "        tp = fp = fn = 0\n",
    "        \n",
    "        for gt_tags, pred_tags, tokens in zip(gt_tags_list, pred_tags_list, tokens_list):\n",
    "            gt_spans = utils.get_entity_spans(tokens, gt_tags)\n",
    "            pred_spans = utils.get_entity_spans(tokens, pred_tags)\n",
    "            \n",
    "            # Filter for current entity type\n",
    "            gt_type_spans = {(s, e, t) for s, e, t in gt_spans if t == entity_type}\n",
    "            pred_type_spans = {(s, e, t) for s, e, t in pred_spans if t == entity_type}\n",
    "            \n",
    "            tp += len(gt_type_spans & pred_type_spans)\n",
    "            fp += len(pred_type_spans - gt_type_spans)\n",
    "            fn += len(gt_type_spans - pred_type_spans)\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        \n",
    "        type_metrics[entity_type] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'support': tp + fn\n",
    "        }\n",
    "    \n",
    "    return type_metrics\n",
    "\n",
    "your_type_metrics = calculate_per_type_metrics(gt_tags_list, your_pred_list, tokens_list, entity_types)\n",
    "friend_type_metrics = calculate_per_type_metrics(gt_tags_list, friend_pred_list, tokens_list, entity_types)\n",
    "ensemble_type_metrics = calculate_per_type_metrics(gt_tags_list, ensemble_pred_list, tokens_list, entity_types)\n",
    "\n",
    "print(\"‚úì Calculated per-type metrics for all models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================================================\n",
      "SIDE-BY-SIDE COMPARISON: F1 SCORES PER ENTITY TYPE\n",
      "========================================================================================================================\n",
      "Entity Type               Your Model    Friend Model    Hybrid Ensemble         Best     Improvement\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Artist                        0.8285          0.8146             0.8283         Your         -0.0002\n",
      "                                                                                     (n=2849)\n",
      "Facility                      0.8434          0.7982             0.8437   üèÜ Ensemble         +0.0003\n",
      "                                                                                     (n=1487)\n",
      "HumanSettlement               0.9580          0.9366             0.9580         Your         -0.0000\n",
      "                                                                                     (n=3476)\n",
      "ORG                           0.8185          0.7718             0.8171         Your         -0.0014\n",
      "                                                                                     (n=1893)\n",
      "OtherPER                      0.6698          0.6505             0.6698   üèÜ Ensemble          0.0000\n",
      "                                                                                     (n=1779)\n",
      "Politician                    0.7079          0.6979             0.7082   üèÜ Ensemble         +0.0003\n",
      "                                                                                     (n=1402)\n",
      "PublicCorp                    0.7686          0.7489             0.7701   üèÜ Ensemble         +0.0015\n",
      "                                                                                     (n=589)\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Display side-by-side comparison\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"SIDE-BY-SIDE COMPARISON: F1 SCORES PER ENTITY TYPE\")\n",
    "print(\"=\"*120)\n",
    "print(f\"{'Entity Type':<20} {'Your Model':>15} {'Friend Model':>15} {'Hybrid Ensemble':>18} {'Best':>12} {'Improvement':>15}\")\n",
    "print(\"-\"*120)\n",
    "\n",
    "for entity_type in entity_types:\n",
    "    your_f1 = your_type_metrics[entity_type]['f1']\n",
    "    friend_f1 = friend_type_metrics[entity_type]['f1']\n",
    "    ensemble_f1 = ensemble_type_metrics[entity_type]['f1']\n",
    "    support = your_type_metrics[entity_type]['support']\n",
    "    \n",
    "    # Determine best individual model\n",
    "    best_individual_f1 = max(your_f1, friend_f1)\n",
    "    best_individual_name = 'Your' if your_f1 > friend_f1 else 'Friend'\n",
    "    \n",
    "    # Calculate improvement\n",
    "    improvement = ensemble_f1 - best_individual_f1\n",
    "    \n",
    "    # Determine overall best\n",
    "    best_overall = max(your_f1, friend_f1, ensemble_f1)\n",
    "    if best_overall == ensemble_f1:\n",
    "        best_marker = 'üèÜ Ensemble'\n",
    "    elif best_overall == your_f1:\n",
    "        best_marker = 'Your'\n",
    "    else:\n",
    "        best_marker = 'Friend'\n",
    "    \n",
    "    improvement_str = f\"{improvement:+.4f}\" if improvement != 0 else \"0.0000\"\n",
    "    \n",
    "    print(f\"{entity_type:<20} {your_f1:>15.4f} {friend_f1:>15.4f} {ensemble_f1:>18.4f} {best_marker:>12} {improvement_str:>15}\")\n",
    "    print(f\"{'':20} {'':15} {'':15} {'':18} {'':12} (n={support})\")\n",
    "\n",
    "print(\"=\"*120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "OVERALL METRICS COMPARISON\n",
      "====================================================================================================\n",
      "Model                                     Precision          Recall              F1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "üèÜ Your Model (KB-NER)                       0.8249          0.8275          0.8262\n",
      "2. Hybrid Ensemble                           0.8242          0.8278          0.8260\n",
      "3. Friend Model (RoBERTa)                    0.7984          0.8056          0.8020\n",
      "====================================================================================================\n",
      "\n",
      "Ensemble vs Best Individual Model:\n",
      "  Improvement: -0.0002 (-0.02%)\n",
      "  ‚ö†Ô∏è  Best individual model outperforms ensemble\n"
     ]
    }
   ],
   "source": [
    "# Calculate overall metrics\n",
    "your_overall = utils.evaluate_entity_spans(gt_tags_list, your_pred_list, tokens_list)\n",
    "friend_overall = utils.evaluate_entity_spans(gt_tags_list, friend_pred_list, tokens_list)\n",
    "ensemble_overall = utils.evaluate_entity_spans(gt_tags_list, ensemble_pred_list, tokens_list)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"OVERALL METRICS COMPARISON\")\n",
    "print(\"=\"*100)\n",
    "print(f\"{'Model':<35} {'Precision':>15} {'Recall':>15} {'F1':>15}\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "all_models = [\n",
    "    ('Your Model (KB-NER)', your_overall),\n",
    "    ('Friend Model (RoBERTa)', friend_overall),\n",
    "    ('Hybrid Ensemble', ensemble_overall)\n",
    "]\n",
    "\n",
    "# Sort by F1\n",
    "all_models.sort(key=lambda x: x[1]['f1'], reverse=True)\n",
    "\n",
    "for i, (name, metrics) in enumerate(all_models, 1):\n",
    "    marker = \"üèÜ\" if i == 1 else f\"{i}.\"\n",
    "    print(f\"{marker} {name:<32} {metrics['precision']:>15.4f} {metrics['recall']:>15.4f} {metrics['f1']:>15.4f}\")\n",
    "\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Show improvement\n",
    "best_individual_f1 = max(your_overall['f1'], friend_overall['f1'])\n",
    "ensemble_f1 = ensemble_overall['f1']\n",
    "improvement = ensemble_f1 - best_individual_f1\n",
    "\n",
    "print(f\"\\nEnsemble vs Best Individual Model:\")\n",
    "print(f\"  Improvement: {improvement:+.4f} ({improvement*100:+.2f}%)\")\n",
    "\n",
    "if improvement > 0:\n",
    "    print(f\"  ‚úÖ Ensemble outperforms both individual models\")\n",
    "elif improvement < 0:\n",
    "    print(f\"  ‚ö†Ô∏è  Best individual model outperforms ensemble\")\n",
    "else:\n",
    "    print(f\"  ü§ù Ensemble performs similarly to best individual model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Types Where Ensemble Helps Most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ENTITY TYPES WHERE ENSEMBLE HELPS MOST\n",
      "================================================================================\n",
      "Entity Type              Improvement    Best Individual        Ensemble\n",
      "--------------------------------------------------------------------------------\n",
      "‚úÖ PublicCorp                  0.0015             0.7686          0.7701\n",
      "‚ûñ Facility                    0.0003             0.8434          0.8437\n",
      "‚ûñ Politician                  0.0003             0.7079          0.7082\n",
      "‚ûñ OtherPER                    0.0000             0.6698          0.6698\n",
      "‚ûñ HumanSettlement            -0.0000             0.9580          0.9580\n",
      "‚ûñ Artist                     -0.0002             0.8285          0.8283\n",
      "‚ö†Ô∏è ORG                        -0.0014             0.8185          0.8171\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Calculate improvement per entity type\n",
    "improvements = []\n",
    "\n",
    "for entity_type in entity_types:\n",
    "    your_f1 = your_type_metrics[entity_type]['f1']\n",
    "    friend_f1 = friend_type_metrics[entity_type]['f1']\n",
    "    ensemble_f1 = ensemble_type_metrics[entity_type]['f1']\n",
    "    \n",
    "    best_individual_f1 = max(your_f1, friend_f1)\n",
    "    improvement = ensemble_f1 - best_individual_f1\n",
    "    \n",
    "    improvements.append((entity_type, improvement, ensemble_f1, best_individual_f1))\n",
    "\n",
    "# Sort by improvement (largest first)\n",
    "improvements.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENTITY TYPES WHERE ENSEMBLE HELPS MOST\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Entity Type':<20} {'Improvement':>15} {'Best Individual':>18} {'Ensemble':>15}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for entity_type, improvement, ensemble_f1, best_individual_f1 in improvements:\n",
    "    marker = \"‚úÖ\" if improvement > 0.001 else (\"‚ö†Ô∏è\" if improvement < -0.001 else \"‚ûñ\")\n",
    "    print(f\"{marker} {entity_type:<18} {improvement:>15.4f} {best_individual_f1:>18.4f} {ensemble_f1:>15.4f}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
