{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M10: KB-NER (Knowledge-Based Named Entity Recognition)\n",
    "\n",
    "## Overview\n",
    "Implementation of Alibaba DAMO-NLP's winning system from SemEval 2022 MultiCoNER.\n",
    "\n",
    "**Winner**: 10 out of 13 tracks (2022)  \n",
    "**Expected F1**: 83-85%  \n",
    "**Award**: Best System Paper at SemEval 2022\n",
    "\n",
    "## Architecture:\n",
    "1. **Knowledge Base**: Wikipedia-based multilingual knowledge\n",
    "2. **Retrieval**: Find related Wikipedia contexts for entities\n",
    "3. **Augmentation**: Append context to input sentences\n",
    "4. **Model**: XLM-RoBERTa + CRF with augmented context\n",
    "\n",
    "## GPU Efficiency Strategy:\n",
    "- **Phase 1 (CPU)**: Build knowledge base, retrieve contexts (1-2 hours)\n",
    "- **Phase 2 (GPU)**: Train model (1.5 hours)\n",
    "- **Total GPU**: ~1.5 hours only!\n",
    "\n",
    "## Reference:\n",
    "- Paper: https://arxiv.org/abs/2203.00545\n",
    "- GitHub: https://github.com/Alibaba-NLP/KB-NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.57.3)\n",
      "Requirement already satisfied: huggingface-hub in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.36.0)\n",
      "Collecting huggingface-hub\n",
      "  Using cached huggingface_hub-1.2.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (2.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (2025.8.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-crf in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.7.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia-api in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.8.1)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from wikipedia-api) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->wikipedia-api) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->wikipedia-api) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->wikipedia-api) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->wikipedia-api) (2025.8.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "‚úì All packages installed\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "# Let pip resolve to compatible versions with Colab's environment\n",
    "!pip install --upgrade transformers huggingface-hub\n",
    "!pip install pytorch-crf\n",
    "!pip install wikipedia-api\n",
    "\n",
    "print(\"‚úì All packages installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import XLMRobertaTokenizerFast, XLMRobertaModel, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW  # Changed: Use PyTorch's AdamW instead of transformers'\n",
    "from torchcrf import CRF\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import wikipediaapi\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "CONFIG = {\n",
    "    'model_name': 'xlm-roberta-base',  # 270M params, multilingual\n",
    "    'max_length': 128,  # Max tokens per example\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 2e-5,\n",
    "    'num_epochs': 5,  # Changed back to 5 for better training\n",
    "    'warmup_ratio': 0.1,\n",
    "    'weight_decay': 0.01,\n",
    "    'dropout': 0.1,\n",
    "    'gradient_clip': 1.0,\n",
    "    \n",
    "    # KB-NER specific\n",
    "    'use_knowledge': True,  # Set False for baseline XLM-R only\n",
    "    'max_context_length': 50,  # Max tokens from Wikipedia context\n",
    "    'wiki_lang': 'en',  # Wikipedia language\n",
    "}\n",
    "\n",
    "# Entity types\n",
    "ENTITY_TYPES = [\n",
    "    'O',\n",
    "    'B-Artist', 'I-Artist',\n",
    "    'B-Politician', 'I-Politician',\n",
    "    'B-HumanSettlement', 'I-HumanSettlement',\n",
    "    'B-PublicCorp', 'I-PublicCorp',\n",
    "    'B-ORG', 'I-ORG',\n",
    "    'B-Facility', 'I-Facility',\n",
    "    'B-OtherPER', 'I-OtherPER'\n",
    "]\n",
    "\n",
    "tag2id = {tag: idx for idx, tag in enumerate(ENTITY_TYPES)}\n",
    "id2tag = {idx: tag for tag, idx in tag2id.items()}\n",
    "num_tags = len(ENTITY_TYPES)\n",
    "\n",
    "print(f\"Configuration loaded\")\n",
    "print(f\"Number of entity types: {num_tags}\")\n",
    "print(f\"Knowledge base enabled: {CONFIG['use_knowledge']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Knowledge Base Construction (CPU)\n",
    "\n",
    "This phase runs on CPU and builds the Wikipedia knowledge base.  \n",
    "**No GPU credits used here!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì WikipediaKB class defined\n"
     ]
    }
   ],
   "source": [
    "class WikipediaKB:\n",
    "    \"\"\"\n",
    "    Wikipedia Knowledge Base for entity context retrieval.\n",
    "    Simplified version of KB-NER's approach.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lang='en', cache_file='wiki_cache.json'):\n",
    "        self.wiki = wikipediaapi.Wikipedia(\n",
    "            language=lang,\n",
    "            user_agent='MultiCoNER-KB-NER/1.0'\n",
    "        )\n",
    "        self.cache_file = cache_file\n",
    "        self.cache = self.load_cache()\n",
    "    \n",
    "    def load_cache(self):\n",
    "        \"\"\"Load cached Wikipedia queries\"\"\"\n",
    "        if os.path.exists(self.cache_file):\n",
    "            with open(self.cache_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "    \n",
    "    def save_cache(self):\n",
    "        \"\"\"Save cache to disk\"\"\"\n",
    "        with open(self.cache_file, 'w') as f:\n",
    "            json.dump(self.cache, f)\n",
    "    \n",
    "    def get_entity_context(self, entity_text, entity_type=None, max_length=100):\n",
    "        \"\"\"\n",
    "        Retrieve Wikipedia context for an entity.\n",
    "        \n",
    "        Args:\n",
    "            entity_text: Entity surface form (e.g., \"Barack Obama\")\n",
    "            entity_type: Entity type hint (e.g., \"Politician\")\n",
    "            max_length: Max characters from Wikipedia\n",
    "        \n",
    "        Returns:\n",
    "            Context string or empty string if not found\n",
    "        \"\"\"\n",
    "        # Check cache first\n",
    "        cache_key = entity_text.lower()\n",
    "        if cache_key in self.cache:\n",
    "            return self.cache[cache_key]\n",
    "        \n",
    "        # Query Wikipedia\n",
    "        try:\n",
    "            page = self.wiki.page(entity_text)\n",
    "            \n",
    "            if page.exists():\n",
    "                # Get first paragraph as context\n",
    "                summary = page.summary[:max_length]\n",
    "                self.cache[cache_key] = summary\n",
    "                return summary\n",
    "            else:\n",
    "                self.cache[cache_key] = \"\"\n",
    "                return \"\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {entity_text}: {e}\")\n",
    "            self.cache[cache_key] = \"\"\n",
    "            return \"\"\n",
    "    \n",
    "    def extract_entities_from_text(self, tokens, tags):\n",
    "        \"\"\"\n",
    "        Extract entity spans from BIO tags.\n",
    "        \n",
    "        Returns:\n",
    "            List of (entity_text, entity_type) tuples\n",
    "        \"\"\"\n",
    "        entities = []\n",
    "        current_entity = []\n",
    "        current_type = None\n",
    "        \n",
    "        for token, tag in zip(tokens, tags):\n",
    "            if tag.startswith('B-'):\n",
    "                # Save previous entity\n",
    "                if current_entity:\n",
    "                    entities.append((' '.join(current_entity), current_type))\n",
    "                # Start new entity\n",
    "                current_entity = [token]\n",
    "                current_type = tag[2:]  # Remove 'B-'\n",
    "            elif tag.startswith('I-'):\n",
    "                if current_entity:\n",
    "                    current_entity.append(token)\n",
    "            else:  # 'O' tag\n",
    "                if current_entity:\n",
    "                    entities.append((' '.join(current_entity), current_type))\n",
    "                    current_entity = []\n",
    "                    current_type = None\n",
    "        \n",
    "        # Don't forget last entity\n",
    "        if current_entity:\n",
    "            entities.append((' '.join(current_entity), current_type))\n",
    "        \n",
    "        return entities\n",
    "\n",
    "print(\"‚úì WikipediaKB class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data_with_knowledge(data_file, kb, output_file, max_examples=None, checkpoint_every=500):\n",
    "    \"\"\"\n",
    "    Augment dataset with Wikipedia contexts with checkpoint support.\n",
    "    \n",
    "    Args:\n",
    "        data_file: Input .jsonl file\n",
    "        kb: WikipediaKB instance\n",
    "        output_file: Output .jsonl with added contexts\n",
    "        max_examples: Limit for testing (None = all)\n",
    "        checkpoint_every: Save progress every N examples\n",
    "    \"\"\"\n",
    "    print(f\"\\nAugmenting {data_file} with Wikipedia knowledge...\")\n",
    "    \n",
    "    # Check if partial output exists (resume from checkpoint)\n",
    "    start_idx = 0\n",
    "    if os.path.exists(output_file):\n",
    "        with open(output_file, 'r') as f:\n",
    "            existing_lines = f.readlines()\n",
    "            start_idx = len(existing_lines)\n",
    "        print(f\"üìÅ Found existing progress: {start_idx} examples already processed\")\n",
    "        print(f\"   Resuming from example {start_idx}...\")\n",
    "    \n",
    "    df = pd.read_json(data_file, lines=True)\n",
    "    if max_examples:\n",
    "        df = df.head(max_examples)\n",
    "    \n",
    "    total_examples = len(df)  # Store original total before skipping\n",
    "    \n",
    "    # Skip already processed examples\n",
    "    if start_idx > 0:\n",
    "        df = df.iloc[start_idx:]\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(\"‚úì All examples already processed!\")\n",
    "        return\n",
    "    \n",
    "    # Open file in append mode\n",
    "    mode = 'a' if start_idx > 0 else 'w'\n",
    "    \n",
    "    with open(output_file, mode) as f:\n",
    "        # Fixed: total should be original total, not remaining\n",
    "        for idx, row in tqdm(df.iterrows(), total=total_examples, desc=\"Retrieving contexts\", initial=start_idx):\n",
    "            tokens = row['tokens']\n",
    "            tags = row.get('ner_tags', ['O'] * len(tokens))\n",
    "            \n",
    "            # Extract entities\n",
    "            entities = kb.extract_entities_from_text(tokens, tags)\n",
    "            \n",
    "            # Retrieve contexts\n",
    "            contexts = []\n",
    "            for entity_text, entity_type in entities:\n",
    "                context = kb.get_entity_context(entity_text, entity_type)\n",
    "                if context:\n",
    "                    contexts.append(context)\n",
    "                \n",
    "                # Rate limit\n",
    "                time.sleep(0.1)\n",
    "            \n",
    "            # Combine contexts\n",
    "            combined_context = \" \".join(contexts)[:500]\n",
    "            \n",
    "            # Create augmented example\n",
    "            augmented = {\n",
    "                'id': row['id'],\n",
    "                'tokens': tokens,\n",
    "                'ner_tags': tags,\n",
    "                'context': combined_context\n",
    "            }\n",
    "            \n",
    "            # Write immediately (don't accumulate in memory)\n",
    "            f.write(json.dumps(augmented) + '\\n')\n",
    "            \n",
    "            # Save cache periodically\n",
    "            if (idx + 1) % checkpoint_every == 0:\n",
    "                f.flush()  # Force write to disk\n",
    "                kb.save_cache()\n",
    "                print(f\"\\nüíæ Checkpoint saved at {idx + 1} examples\")\n",
    "    \n",
    "    # Final cache save\n",
    "    kb.save_cache()\n",
    "    \n",
    "    print(f\"\\n‚úì Augmented data saved to {output_file}\")\n",
    "    print(f\"  Total examples processed: {total_examples}\")\n",
    "    print(f\"  Cache size: {len(kb.cache)} entities\")\n",
    "\n",
    "print(\"‚úì Augmentation function defined (with checkpointing)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Knowledge Base Augmentation (CPU Phase)\n",
    "\n",
    "**IMPORTANT**: This cell runs on CPU. Do this BEFORE enabling GPU to save credits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Running on FULL DATASET\n",
      "  This will take 1-2 hours on CPU\n",
      "\n",
      "Augmenting train_split.jsonl with Wikipedia knowledge...\n",
      "üìÅ Found existing progress: 26 examples already processed\n",
      "   Resuming from example 26...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "188ac40a1c894bc4901b2dca35b89466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrieving contexts:   0%|          | 26/90294 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úì Running on FULL DATASET\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  This will take 1-2 hours on CPU\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m     \u001b[43maugment_data_with_knowledge\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_split.jsonl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_split_kb.jsonl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     augment_data_with_knowledge(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_split.jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m, kb, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_split_kb.jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m‚úì Knowledge base augmentation complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[29], line 49\u001b[0m, in \u001b[0;36maugment_data_with_knowledge\u001b[0;34m(data_file, kb, output_file, max_examples, checkpoint_every)\u001b[0m\n\u001b[1;32m     47\u001b[0m contexts \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entity_text, entity_type \u001b[38;5;129;01min\u001b[39;00m entities:\n\u001b[0;32m---> 49\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43mkb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_entity_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentity_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentity_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m context:\n\u001b[1;32m     51\u001b[0m         contexts\u001b[38;5;241m.\u001b[39mappend(context)\n",
      "Cell \u001b[0;32mIn[28], line 48\u001b[0m, in \u001b[0;36mWikipediaKB.get_entity_context\u001b[0;34m(self, entity_text, entity_type, max_length)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     page \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwiki\u001b[38;5;241m.\u001b[39mpage(entity_text)\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mpage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;66;03m# Get first paragraph as context\u001b[39;00m\n\u001b[1;32m     50\u001b[0m         summary \u001b[38;5;241m=\u001b[39m page\u001b[38;5;241m.\u001b[39msummary[:max_length]\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache[cache_key] \u001b[38;5;241m=\u001b[39m summary\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/wikipediaapi/__init__.py:999\u001b[0m, in \u001b[0;36mWikipediaPage.exists\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexists\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    994\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;124;03m    Returns `True` if the current page exists, otherwise `False`.\u001b[39;00m\n\u001b[1;32m    996\u001b[0m \n\u001b[1;32m    997\u001b[0m \u001b[38;5;124;03m    :return: if current page existst or not\u001b[39;00m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 999\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpageid\u001b[49m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/wikipediaapi/__init__.py:953\u001b[0m, in \u001b[0;36mWikipediaPage.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m call \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mATTRIBUTES_MAPPING[name]:\n\u001b[1;32m    952\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_called[call]:\n\u001b[0;32m--> 953\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    954\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attributes[name]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/wikipediaapi/__init__.py:1153\u001b[0m, in \u001b[0;36mWikipediaPage._fetch\u001b[0;34m(self, call)\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, call) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWikipediaPage\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1152\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fetches some data?.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1153\u001b[0m     \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwiki\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_called[call] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/wikipediaapi/__init__.py:340\u001b[0m, in \u001b[0;36mWikipedia.info\u001b[0;34m(self, page)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;124;03mhttps://www.mediawiki.org/w/api.php?action=help&modules=query%2Binfo\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;124;03mhttps://www.mediawiki.org/wiki/API:Info\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    319\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprop\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    338\u001b[0m     ),\n\u001b[1;32m    339\u001b[0m }\n\u001b[0;32m--> 340\u001b[0m raw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_common_attributes(raw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m], page)\n\u001b[1;32m    342\u001b[0m pages \u001b[38;5;241m=\u001b[39m raw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpages\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/wikipediaapi/__init__.py:539\u001b[0m, in \u001b[0;36mWikipedia._query\u001b[0;34m(self, page, params)\u001b[0m\n\u001b[1;32m    530\u001b[0m used_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_params(page, params)\n\u001b[1;32m    532\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequest URL: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    534\u001b[0m     base_url\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([k \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m used_params\u001b[38;5;241m.\u001b[39mitems()]),\n\u001b[1;32m    537\u001b[0m )\n\u001b[0;32m--> 539\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mused_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/requests/sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \n\u001b[1;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    513\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:1419\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1418\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1419\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1420\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1421\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1253\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1250\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1251\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1252\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1105\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1105\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1106\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1107\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize Knowledge Base\n",
    "kb = WikipediaKB(lang='en', cache_file='wiki_cache_multiconer.json')\n",
    "\n",
    "# Augment training data\n",
    "# NOTE: Start with small subset for testing, then run on full data\n",
    "TEST_MODE = False  # Set False for full dataset\n",
    "\n",
    "if TEST_MODE:\n",
    "    print(\"‚ö†Ô∏è  Running in TEST MODE (100 examples)\")\n",
    "    print(\"   Set TEST_MODE=False for full dataset\")\n",
    "    augment_data_with_knowledge(\n",
    "        'train_split.jsonl',\n",
    "        kb,\n",
    "        'train_split_kb.jsonl',\n",
    "        max_examples=100\n",
    "    )\n",
    "    augment_data_with_knowledge(\n",
    "        'val_split.jsonl',\n",
    "        kb,\n",
    "        'val_split_kb.jsonl',\n",
    "        max_examples=100\n",
    "    )\n",
    "else:\n",
    "    print(\"‚úì Running on FULL DATASET\")\n",
    "    print(\"  This will take 1-2 hours on CPU\")\n",
    "    augment_data_with_knowledge('train_split.jsonl', kb, 'train_split_kb.jsonl')\n",
    "    augment_data_with_knowledge('val_split.jsonl', kb, 'val_split_kb.jsonl')\n",
    "\n",
    "print(\"\\n‚úì Knowledge base augmentation complete!\")\n",
    "print(\"  Next: Move to GPU phase for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Model Definition\n",
    "\n",
    "XLM-RoBERTa + CRF with knowledge-augmented inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì KB_NER_Dataset class defined\n"
     ]
    }
   ],
   "source": [
    "class KB_NER_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset with knowledge-augmented inputs.\n",
    "    Format: [CLS] tokens [SEP] context [SEP]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_file, tokenizer, max_length=128, tag2id=None):\n",
    "        self.data = []\n",
    "        with open(data_file, 'r') as f:\n",
    "            for line in f:\n",
    "                self.data.append(json.loads(line))\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.tag2id = tag2id\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.data[idx]\n",
    "        tokens = example['tokens']\n",
    "        tags = example['ner_tags']\n",
    "        context = example.get('context', '')\n",
    "        \n",
    "        # Tokenize sentence first (with is_split_into_words=True)\n",
    "        sentence_encoding = self.tokenizer(\n",
    "            tokens,\n",
    "            is_split_into_words=True,\n",
    "            add_special_tokens=False,  # We'll add manually with context\n",
    "            truncation=False,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        # If we have context, tokenize it separately\n",
    "        if context:\n",
    "            context_encoding = self.tokenizer(\n",
    "                context,\n",
    "                is_split_into_words=False,  # Context is a string, not pre-tokenized\n",
    "                add_special_tokens=False,\n",
    "                truncation=True,\n",
    "                max_length=50,  # Limit context length\n",
    "                return_tensors=None\n",
    "            )\n",
    "            \n",
    "            # Combine: [CLS] sentence_tokens [SEP] context_tokens [SEP]\n",
    "            input_ids = (\n",
    "                [self.tokenizer.cls_token_id] + \n",
    "                sentence_encoding['input_ids'] + \n",
    "                [self.tokenizer.sep_token_id] + \n",
    "                context_encoding['input_ids'] + \n",
    "                [self.tokenizer.sep_token_id]\n",
    "            )\n",
    "        else:\n",
    "            # No context: [CLS] sentence_tokens [SEP]\n",
    "            input_ids = (\n",
    "                [self.tokenizer.cls_token_id] + \n",
    "                sentence_encoding['input_ids'] + \n",
    "                [self.tokenizer.sep_token_id]\n",
    "            )\n",
    "        \n",
    "        # Truncate if too long\n",
    "        if len(input_ids) > self.max_length:\n",
    "            input_ids = input_ids[:self.max_length]\n",
    "        \n",
    "        # Pad to max_length\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        padding_length = self.max_length - len(input_ids)\n",
    "        input_ids = input_ids + [self.tokenizer.pad_token_id] * padding_length\n",
    "        attention_mask = attention_mask + [0] * padding_length\n",
    "        \n",
    "        # Align labels with tokenized input\n",
    "        # Get word_ids from sentence encoding only\n",
    "        word_ids_sentence = sentence_encoding.word_ids()\n",
    "        \n",
    "        # Build full word_ids: [None(CLS)] + word_ids_sentence + [None(SEP)] + [None...](context) + [None(SEP)] + [None...](padding)\n",
    "        word_ids = [None]  # CLS\n",
    "        word_ids.extend(word_ids_sentence)\n",
    "        word_ids.append(None)  # SEP after sentence\n",
    "        \n",
    "        if context:\n",
    "            # Add None for context tokens and final SEP\n",
    "            context_length = len(context_encoding['input_ids']) + 1  # +1 for SEP\n",
    "            word_ids.extend([None] * context_length)\n",
    "        \n",
    "        # Add None for padding\n",
    "        word_ids.extend([None] * padding_length)\n",
    "        \n",
    "        # Create labels aligned with input_ids\n",
    "        labels = []\n",
    "        previous_word_idx = None\n",
    "        \n",
    "        for word_idx in word_ids[:self.max_length]:\n",
    "            if word_idx is None:\n",
    "                labels.append(-100)  # Ignore special tokens, context, padding\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # First subword of a word - use the tag\n",
    "                labels.append(self.tag2id[tags[word_idx]])\n",
    "            else:\n",
    "                # Continuation subword - use same tag\n",
    "                labels.append(self.tag2id[tags[word_idx]])\n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long),\n",
    "            'word_ids': word_ids,\n",
    "            'original_tokens': tokens,\n",
    "            'original_tags': tags\n",
    "        }\n",
    "\n",
    "print(\"‚úì KB_NER_Dataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLMRobertaCRF(nn.Module):\n",
    "    \"\"\"\n",
    "    XLM-RoBERTa + CRF for NER.\n",
    "    Based on KB-NER architecture.\n",
    "    \n",
    "    IMPORTANT: During inference, pass word_mask to only decode word positions!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name, num_tags, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.xlmr = XLMRobertaModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(self.xlmr.config.hidden_size, num_tags)\n",
    "        self.crf = CRF(num_tags, batch_first=True)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None, word_mask=None):\n",
    "        # Get XLM-R embeddings\n",
    "        outputs = self.xlmr(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        \n",
    "        # Project to tag space\n",
    "        emissions = self.classifier(sequence_output)\n",
    "        \n",
    "        if labels is not None:\n",
    "            # TRAINING: Create mask from labels (only real word tokens)\n",
    "            mask = (labels != -100).byte()\n",
    "            \n",
    "            # IMPORTANT: CRF requires first position to be valid\n",
    "            mask[:, 0] = 1\n",
    "            \n",
    "            # Replace -100 with 0 for CRF\n",
    "            labels_for_crf = labels.clone()\n",
    "            labels_for_crf[labels_for_crf == -100] = 0\n",
    "            \n",
    "            # CRF loss\n",
    "            log_likelihood = self.crf(emissions, labels_for_crf, mask=mask, reduction='mean')\n",
    "            loss = -log_likelihood\n",
    "            \n",
    "            return loss, emissions\n",
    "        else:\n",
    "            # INFERENCE: Use word_mask if provided, otherwise fall back to attention_mask\n",
    "            if word_mask is not None:\n",
    "                mask = word_mask.byte()\n",
    "            else:\n",
    "                mask = attention_mask.byte()\n",
    "            \n",
    "            predictions = self.crf.decode(emissions, mask=mask)\n",
    "            return predictions\n",
    "\n",
    "print(\"‚úì XLMRobertaCRF model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Training (GPU Phase)\n",
    "\n",
    "**‚ö†Ô∏è START GPU HERE** - This is where GPU credits are used!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Datasets loaded\n",
      "  Training examples: 100\n",
      "  Validation examples: 100\n"
     ]
    }
   ],
   "source": [
    "# Custom collate function to handle word_ids (which contains None)\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to handle word_ids (which are lists with None values)\"\"\"\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    labels = torch.stack([item['labels'] for item in batch])\n",
    "    \n",
    "    # Keep word_ids, original_tokens, and original_tags as lists (not tensors)\n",
    "    word_ids = [item['word_ids'] for item in batch]\n",
    "    original_tokens = [item['original_tokens'] for item in batch]\n",
    "    original_tags = [item['original_tags'] for item in batch]\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels,\n",
    "        'word_ids': word_ids,\n",
    "        'original_tokens': original_tokens,\n",
    "        'original_tags': original_tags\n",
    "    }\n",
    "\n",
    "# Initialize tokenizer and datasets\n",
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained(CONFIG['model_name'])\n",
    "\n",
    "train_dataset = KB_NER_Dataset(\n",
    "    'train_split_kb.jsonl',\n",
    "    tokenizer,\n",
    "    max_length=CONFIG['max_length'],\n",
    "    tag2id=tag2id\n",
    ")\n",
    "\n",
    "val_dataset = KB_NER_Dataset(\n",
    "    'val_split_kb.jsonl',\n",
    "    tokenizer,\n",
    "    max_length=CONFIG['max_length'],\n",
    "    tag2id=tag2id\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn  # Use custom collate function\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn  # Use custom collate function\n",
    ")\n",
    "\n",
    "print(f\"‚úì Datasets loaded\")\n",
    "print(f\"  Training examples: {len(train_dataset)}\")\n",
    "print(f\"  Validation examples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Model initialized\n",
      "  Parameters: 278.1M\n",
      "  Device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = XLMRobertaCRF(\n",
    "    CONFIG['model_name'],\n",
    "    num_tags,\n",
    "    dropout=CONFIG['dropout']\n",
    ").to(device)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay']\n",
    ")\n",
    "\n",
    "total_steps = len(train_loader) * CONFIG['num_epochs']\n",
    "warmup_steps = int(total_steps * CONFIG['warmup_ratio'])\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"‚úì Model initialized\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")\n",
    "print(f\"  Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss, _ = model(input_ids, attention_mask, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['gradient_clip'])\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate_epoch(model, dataloader, device, id2tag):\n",
    "    \"\"\"\n",
    "    Validate model and return predictions for F1 calculation.\n",
    "    \n",
    "    CRITICAL FIX: Create word_mask from word_ids to only decode word positions!\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_ground_truth = []\n",
    "    all_tokens = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc=\"Validating\")\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            word_ids_batch = batch['word_ids']  # List of lists with None values\n",
    "            \n",
    "            # Get loss\n",
    "            loss, _ = model(input_ids, attention_mask, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # CRITICAL FIX: Create word_mask for CRF decoding\n",
    "            # Only mark positions that correspond to actual words (not [CLS], [SEP], context, padding)\n",
    "            word_mask = torch.zeros_like(attention_mask)\n",
    "            for i, word_ids in enumerate(word_ids_batch):\n",
    "                for j, word_id in enumerate(word_ids):\n",
    "                    if word_id is not None:  # This is a real word position\n",
    "                        word_mask[i, j] = 1\n",
    "            \n",
    "            # CRF requires first position to be valid - force it\n",
    "            word_mask[:, 0] = 1\n",
    "            \n",
    "            word_mask = word_mask.to(device)\n",
    "            \n",
    "            # Get predictions with word_mask\n",
    "            predictions = model(input_ids, attention_mask, word_mask=word_mask)\n",
    "            \n",
    "            # Convert to tags\n",
    "            for pred, word_ids, orig_tokens, orig_tags in zip(\n",
    "                predictions,\n",
    "                batch['word_ids'],\n",
    "                batch['original_tokens'],\n",
    "                batch['original_tags']\n",
    "            ):\n",
    "                # Align predictions with original tokens\n",
    "                token_preds = []\n",
    "                prev_word_idx = None\n",
    "                \n",
    "                for pred_id, word_idx in zip(pred, word_ids):\n",
    "                    if word_idx is not None and word_idx != prev_word_idx:\n",
    "                        if word_idx < len(orig_tokens):\n",
    "                            token_preds.append(id2tag[pred_id])\n",
    "                    prev_word_idx = word_idx\n",
    "                \n",
    "                # Ensure same length\n",
    "                if len(token_preds) != len(orig_tokens):\n",
    "                    token_preds = token_preds[:len(orig_tokens)]\n",
    "                    if len(token_preds) < len(orig_tokens):\n",
    "                        token_preds.extend(['O'] * (len(orig_tokens) - len(token_preds)))\n",
    "                \n",
    "                all_predictions.append(token_preds)\n",
    "                all_ground_truth.append(list(orig_tags))\n",
    "                all_tokens.append(list(orig_tokens))\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return all_predictions, all_ground_truth, all_tokens, avg_loss\n",
    "\n",
    "print(\"‚úì Training and validation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load utils for F1 calculation during training\n",
    "try:\n",
    "    import utils\n",
    "    print(\"‚úì utils.py loaded for validation\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è utils.py not found - will save based on loss instead of F1\")\n",
    "    utils = None\n",
    "\n",
    "# Training loop\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING TRAINING (GPU PHASE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_f1 = 0.0\n",
    "best_val_loss = float('inf')\n",
    "training_start_time = time.time()\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "for epoch in range(CONFIG['num_epochs']):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Epoch {epoch + 1}/{CONFIG['num_epochs']}\")\n",
    "    print('='*80)\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # Validate\n",
    "    print(\"\\nValidating...\")\n",
    "    predictions, ground_truth, tokens, val_loss = validate_epoch(model, val_loader, device, id2tag)\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    if utils is not None:\n",
    "        results = utils.evaluate_entity_spans(ground_truth, predictions, tokens)\n",
    "        val_f1 = results['f1']\n",
    "        val_precision = results['precision']\n",
    "        val_recall = results['recall']\n",
    "        \n",
    "        print(f\"\\nValidation Results:\")\n",
    "        print(f\"  Loss:      {val_loss:.4f}\")\n",
    "        print(f\"  Precision: {val_precision:.4f} ({val_precision*100:.2f}%)\")\n",
    "        print(f\"  Recall:    {val_recall:.4f} ({val_recall*100:.2f}%)\")\n",
    "        print(f\"  F1 Score:  {val_f1:.4f} ({val_f1*100:.2f}%)\")\n",
    "        \n",
    "        # Save best model based on F1 score\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'val_f1': val_f1,\n",
    "                'val_precision': val_precision,\n",
    "                'val_recall': val_recall,\n",
    "                'config': CONFIG,\n",
    "                'tag2id': tag2id,\n",
    "                'id2tag': id2tag,\n",
    "            }, 'models/kb_ner_best.pt')\n",
    "            print(f\"  ‚úì New best model saved! (F1: {val_f1:.4f})\")\n",
    "        else:\n",
    "            print(f\"  (Best F1 so far: {best_f1:.4f})\")\n",
    "    else:\n",
    "        # Fallback: save based on validation loss if utils not available\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'config': CONFIG,\n",
    "                'tag2id': tag2id,\n",
    "                'id2tag': id2tag,\n",
    "            }, 'models/kb_ner_best.pt')\n",
    "            print(f\"‚úì Saved best model (loss: {val_loss:.4f})\")\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print('='*80)\n",
    "print(f\"Total time: {training_time / 60:.1f} minutes\")\n",
    "print(f\"Time per epoch: {training_time / CONFIG['num_epochs'] / 60:.1f} minutes\")\n",
    "if utils is not None:\n",
    "    print(f\"Best validation F1: {best_f1:.4f} ({best_f1*100:.2f}%)\")\n",
    "print(f\"Best model saved to: models/kb_ner_best.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Evaluate on validation set using entity-span F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load utils for evaluation\n",
    "try:\n",
    "    import utils\n",
    "    print(\"‚úì utils.py loaded\")\n",
    "except ImportError:\n",
    "    print(\"‚úó utils.py not found. Upload it to evaluate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device, id2tag):\n",
    "    \"\"\"\n",
    "    Evaluate model and return predictions.\n",
    "    \n",
    "    CRITICAL FIX: Create word_mask from word_ids to only decode word positions!\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_ground_truth = []\n",
    "    all_tokens = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            word_ids_batch = batch['word_ids']\n",
    "            \n",
    "            # CRITICAL FIX: Create word_mask for CRF decoding\n",
    "            # Only mark positions that correspond to actual words (not [CLS], [SEP], context, padding)\n",
    "            word_mask = torch.zeros_like(attention_mask)\n",
    "            for i, word_ids in enumerate(word_ids_batch):\n",
    "                for j, word_id in enumerate(word_ids):\n",
    "                    if word_id is not None:  # This is a real word position\n",
    "                        word_mask[i, j] = 1\n",
    "            \n",
    "            # CRF requires first position to be valid - force it\n",
    "            word_mask[:, 0] = 1\n",
    "            \n",
    "            word_mask = word_mask.to(device)\n",
    "            \n",
    "            # Get predictions with word_mask\n",
    "            predictions = model(input_ids, attention_mask, word_mask=word_mask)\n",
    "            \n",
    "            # Convert to tags\n",
    "            for pred, word_ids, orig_tokens, orig_tags in zip(\n",
    "                predictions,\n",
    "                batch['word_ids'],\n",
    "                batch['original_tokens'],\n",
    "                batch['original_tags']\n",
    "            ):\n",
    "                # Align predictions with original tokens\n",
    "                token_preds = []\n",
    "                prev_word_idx = None\n",
    "                \n",
    "                for pred_id, word_idx in zip(pred, word_ids):\n",
    "                    if word_idx is not None and word_idx != prev_word_idx:\n",
    "                        if word_idx < len(orig_tokens):\n",
    "                            token_preds.append(id2tag[pred_id])\n",
    "                    prev_word_idx = word_idx\n",
    "                \n",
    "                # Ensure same length\n",
    "                if len(token_preds) != len(orig_tokens):\n",
    "                    token_preds = token_preds[:len(orig_tokens)]\n",
    "                    if len(token_preds) < len(orig_tokens):\n",
    "                        token_preds.extend(['O'] * (len(orig_tokens) - len(token_preds)))\n",
    "                \n",
    "                all_predictions.append(token_preds)\n",
    "                all_ground_truth.append(list(orig_tags))\n",
    "                all_tokens.append(list(orig_tokens))\n",
    "    \n",
    "    return all_predictions, all_ground_truth, all_tokens\n",
    "\n",
    "print(\"‚úì Evaluation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "print(\"\\nEvaluating on validation set...\")\n",
    "\n",
    "predictions, ground_truth, tokens = evaluate_model(model, val_loader, device, id2tag)\n",
    "\n",
    "# Calculate metrics using utils.py\n",
    "results = utils.evaluate_entity_spans(ground_truth, predictions, tokens)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"M10: KB-NER RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Precision: {results['precision']:.4f} ({results['precision']*100:.2f}%)\")\n",
    "print(f\"Recall:    {results['recall']:.4f} ({results['recall']*100:.2f}%)\")\n",
    "print(f\"F1 Score:  {results['f1']:.4f} ({results['f1']*100:.2f}%)\")\n",
    "print(f\"\\nTrue Positives:  {results['true_positives']}\")\n",
    "print(f\"False Positives: {results['false_positives']}\")\n",
    "print(f\"False Negatives: {results['false_negatives']}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Detailed report\n",
    "utils.print_evaluation_report(ground_truth, predictions, tokens, \"M10: KB-NER\")\n",
    "\n",
    "# Save results\n",
    "results_data = {\n",
    "    'model': 'M10: KB-NER (XLM-RoBERTa + CRF + Knowledge Base)',\n",
    "    'architecture': 'XLM-RoBERTa-base + CRF',\n",
    "    'knowledge_source': 'Wikipedia',\n",
    "    'precision': results['precision'],\n",
    "    'recall': results['recall'],\n",
    "    'f1': results['f1'],\n",
    "    'training_time': training_time,\n",
    "    'num_epochs': CONFIG['num_epochs'],\n",
    "    'parameters': sum(p.numel() for p in model.parameters()),\n",
    "    'config': CONFIG\n",
    "}\n",
    "\n",
    "with open('models/kb_ner_results.json', 'w') as f:\n",
    "    json.dump(results_data, f, indent=2)\n",
    "\n",
    "print(\"\\n‚úì Results saved to models/kb_ner_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### M10: KB-NER Performance\n",
    "\n",
    "**Architecture**: XLM-RoBERTa-base (270M params) + CRF + Wikipedia Knowledge Base\n",
    "\n",
    "**Expected F1**: 83-85%\n",
    "\n",
    "**Comparison**:\n",
    "- Gemini Few-Shot: 68% F1\n",
    "- M4 v2 (BiLSTM-CRF): 75.94% F1\n",
    "- Friends (BERT): 77-79% F1\n",
    "- **M10 (KB-NER)**: ~83-85% F1 ‚úÖ\n",
    "- M8 (RoBERTa): ~85-88% F1 (simpler, no KB needed)\n",
    "\n",
    "### GPU Usage:\n",
    "- Phase 1 (KB construction): CPU only (~1-2 hours)\n",
    "- Phase 2 (Training): GPU (~1.5 hours for 5 epochs)\n",
    "- **Total GPU**: ~1.5 hours\n",
    "\n",
    "### Next Steps:\n",
    "1. If F1 ‚â• 83%, generate test predictions\n",
    "2. Compare with M8 (simpler but similar F1)\n",
    "3. Consider ensemble: M4 v2 + M10 + M8\n",
    "\n",
    "### Notes:\n",
    "- KB-NER adds 5-8% over baseline BERT\n",
    "- Most gain comes from XLM-RoBERTa, KB adds 1-2%\n",
    "- For simplicity, M8 (pure RoBERTa) might be better\n",
    "- KB-NER shines on ambiguous entities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
