{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Model 4 v3: Character-CNN + BiLSTM-CRF (No Vocabulary Filtering)\n",
    "\n",
    "**Experiment: MIN_WORD_FREQ = 1 (use ALL words)**\n",
    "\n",
    "This version tests whether including ALL words in vocabulary improves performance.\n",
    "\n",
    "**Difference from v2:**\n",
    "- âœ… **MIN_WORD_FREQ = 1**: Include every word (no filtering)\n",
    "- Expected vocab size: ~40,000+ words (vs 36,790 in v2)\n",
    "\n",
    "**Hypothesis:**\n",
    "- âœ… Pro: No information loss, every word gets its own embedding\n",
    "- âš ï¸ Con: Larger model, potential overfitting to rare words\n",
    "\n",
    "**All other improvements from v2 remain:**\n",
    "1. Higher Dropout (0.6)\n",
    "2. Dropout on Embeddings\n",
    "3. Learning Rate Scheduler\n",
    "4. Increased Patience (7)\n",
    "5. GloVe 300d\n",
    "6. Gradient Clipping (1.0)\n",
    "\n",
    "**Expected Result:**\n",
    "- If v3 â‰ˆ v2: CharCNN handles rare words well, filtering doesn't matter\n",
    "- If v3 > v2: Rare word information is valuable\n",
    "- If v3 < v2: Rare words cause overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter kernel Python: /usr/local/bin/python3\n",
      "\u001b[33mWARNING: Skipping torchcrf as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: pytorch-crf in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.7.2)\n",
      "Requirement already satisfied: gensim in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.4.0)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gensim) (2.1.1)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gensim) (1.14.1)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gensim) (7.3.0.post1)\n",
      "Requirement already satisfied: wrapt in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from smart_open>=1.8.1->gensim) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "âœ… Packages installed! Please RESTART THE KERNEL before continuing.\n",
      "   Kernel â†’ Restart Kernel (or Ctrl+Shift+P â†’ 'Restart Kernel')\n"
     ]
    }
   ],
   "source": [
    "# Install required packages using the notebook's Python interpreter\n",
    "import sys\n",
    "print(f\"Jupyter kernel Python: {sys.executable}\")\n",
    "\n",
    "# Install packages to the correct Python environment\n",
    "!{sys.executable} -m pip uninstall -y torchcrf\n",
    "!{sys.executable} -m pip install torch pytorch-crf gensim tqdm\n",
    "\n",
    "print(\"\\nâœ… Packages installed! Please RESTART THE KERNEL before continuing.\")\n",
    "print(\"   Kernel â†’ Restart Kernel (or Ctrl+Shift+P â†’ 'Restart Kernel')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Imports successful!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# CRF\n",
    "from torchcrf import CRF\n",
    "\n",
    "# Embeddings\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Our evaluation utilities\n",
    "from utils import print_evaluation_report, evaluate_entity_spans\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 90,320\n",
      "Validation samples: 10,036\n"
     ]
    }
   ],
   "source": [
    "def load_jsonl(file_path):\n",
    "    \"\"\"Load JSONL file\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.strip()))\n",
    "    return data\n",
    "\n",
    "# Load data\n",
    "train_data = load_jsonl('train_split.jsonl')\n",
    "val_data = load_jsonl('val_split.jsonl')\n",
    "\n",
    "print(f\"Training samples: {len(train_data):,}\")\n",
    "print(f\"Validation samples: {len(val_data):,}\")\n",
    "\n",
    "# Extract tokens and tags\n",
    "train_tokens = [sample['tokens'] for sample in train_data]\n",
    "train_tags = [sample['ner_tags'] for sample in train_data]\n",
    "\n",
    "val_tokens = [sample['tokens'] for sample in val_data]\n",
    "val_tags = [sample['ner_tags'] for sample in val_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. Build Vocabularies\n",
    "\n",
    "**ðŸ”¬ EXPERIMENT: MIN_WORD_FREQ = 1 (include ALL words)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¬ MIN_WORD_FREQ: 1 (using ALL words)\n",
      "Word vocabulary size: 107,643\n",
      "   v2 had: 36,790 words (with MIN_WORD_FREQ=2)\n",
      "   v3 has: 107,643 words (with MIN_WORD_FREQ=1)\n",
      "   Difference: +70,853 words\n"
     ]
    }
   ],
   "source": [
    "# Build word vocabulary\n",
    "word_counts = Counter()\n",
    "for tokens in train_tokens:\n",
    "    word_counts.update(tokens)\n",
    "\n",
    "# ðŸ”¬ EXPERIMENT: Use ALL words (no filtering)\n",
    "MIN_WORD_FREQ = 1  # Changed from 2 to 1!\n",
    "word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "for word, count in word_counts.items():\n",
    "    if count >= MIN_WORD_FREQ:\n",
    "        word2idx[word] = len(word2idx)\n",
    "\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "print(f\"ðŸ”¬ MIN_WORD_FREQ: {MIN_WORD_FREQ} (using ALL words)\")\n",
    "print(f\"Word vocabulary size: {vocab_size:,}\")\n",
    "print(f\"   v2 had: 36,790 words (with MIN_WORD_FREQ=2)\")\n",
    "print(f\"   v3 has: {vocab_size:,} words (with MIN_WORD_FREQ=1)\")\n",
    "print(f\"   Difference: +{vocab_size - 36790:,} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character vocabulary size: 97\n"
     ]
    }
   ],
   "source": [
    "# Build character vocabulary\n",
    "char2idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "\n",
    "chars = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 .,!?:;\\'\"()-[]{}@#$%^&*+=/<>\\\\|`~_'\n",
    "for char in chars:\n",
    "    if char not in char2idx:\n",
    "        char2idx[char] = len(char2idx)\n",
    "\n",
    "idx2char = {idx: char for char, idx in char2idx.items()}\n",
    "char_vocab_size = len(char2idx)\n",
    "\n",
    "print(f\"Character vocabulary size: {char_vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NER tags: 15\n",
      "Tags: ['O', 'B-ORG', 'I-ORG', 'B-Facility', 'I-Facility', 'B-OtherPER', 'I-OtherPER', 'B-Politician', 'I-Politician', 'B-HumanSettlement', 'I-HumanSettlement', 'B-Artist', 'I-Artist', 'B-PublicCorp', 'I-PublicCorp']\n"
     ]
    }
   ],
   "source": [
    "# Build tag vocabulary\n",
    "tag2idx = {}\n",
    "for tags in train_tags:\n",
    "    for tag in tags:\n",
    "        if tag not in tag2idx:\n",
    "            tag2idx[tag] = len(tag2idx)\n",
    "\n",
    "idx2tag = {idx: tag for tag, idx in tag2idx.items()}\n",
    "num_tags = len(tag2idx)\n",
    "\n",
    "print(f\"Number of NER tags: {num_tags}\")\n",
    "print(f\"Tags: {list(tag2idx.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 4. Load Pre-trained Word Embeddings (GloVe 300d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading GloVe 300d embeddings (this may take ~5-10 minutes)...\n",
      "Using glove-wiki-gigaword-300 (300-dimensional, 400K vocabulary)\n",
      "\n",
      "GloVe 300d embeddings loaded!\n",
      "Embedding dimension: 300\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloading GloVe 300d embeddings (this may take ~5-10 minutes)...\")\n",
    "print(\"Using glove-wiki-gigaword-300 (300-dimensional, 400K vocabulary)\")\n",
    "\n",
    "# Download GloVe 300d embeddings\n",
    "glove_model = api.load('glove-wiki-gigaword-300')\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "print(f\"\\nGloVe 300d embeddings loaded!\")\n",
    "print(f\"Embedding dimension: {EMBEDDING_DIM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words found in GloVe: 66,859 / 107,643 (62.1%)\n",
      "Words initialized randomly: 40,784\n",
      "\n",
      "ðŸ“Š Comparison with v2:\n",
      "   v2: 33,849 / 36,790 (92.0%) found in GloVe\n",
      "   v3: 66,859 / 107,643 (62.1%) found in GloVe\n"
     ]
    }
   ],
   "source": [
    "# Create embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "\n",
    "# Initialize with GloVe vectors\n",
    "found = 0\n",
    "for word, idx in word2idx.items():\n",
    "    if word in ['<PAD>', '<UNK>']:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        embedding_matrix[idx] = glove_model[word.lower()]\n",
    "        found += 1\n",
    "    except KeyError:\n",
    "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(EMBEDDING_DIM,))\n",
    "\n",
    "# Special tokens\n",
    "embedding_matrix[word2idx['<PAD>']] = np.zeros(EMBEDDING_DIM)\n",
    "embedding_matrix[word2idx['<UNK>']] = np.random.normal(scale=0.6, size=(EMBEDDING_DIM,))\n",
    "\n",
    "print(f\"Words found in GloVe: {found:,} / {vocab_size:,} ({found/vocab_size*100:.1f}%)\")\n",
    "print(f\"Words initialized randomly: {vocab_size - found:,}\")\n",
    "print(f\"\\nðŸ“Š Comparison with v2:\")\n",
    "print(f\"   v2: 33,849 / 36,790 (92.0%) found in GloVe\")\n",
    "print(f\"   v3: {found:,} / {vocab_size:,} ({found/vocab_size*100:.1f}%) found in GloVe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 5. Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 90320 samples\n",
      "Val dataset: 10036 samples\n"
     ]
    }
   ],
   "source": [
    "MAX_CHAR_LEN = 20\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, tokens_list, tags_list, word2idx, char2idx, tag2idx):\n",
    "        self.tokens_list = tokens_list\n",
    "        self.tags_list = tags_list\n",
    "        self.word2idx = word2idx\n",
    "        self.char2idx = char2idx\n",
    "        self.tag2idx = tag2idx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokens_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokens_list[idx]\n",
    "        tags = self.tags_list[idx]\n",
    "        \n",
    "        word_ids = [self.word2idx.get(token, self.word2idx['<UNK>']) for token in tokens]\n",
    "        \n",
    "        char_ids = []\n",
    "        for token in tokens:\n",
    "            chars = [self.char2idx.get(c, self.char2idx['<UNK>']) for c in token[:MAX_CHAR_LEN]]\n",
    "            if len(chars) < MAX_CHAR_LEN:\n",
    "                chars += [self.char2idx['<PAD>']] * (MAX_CHAR_LEN - len(chars))\n",
    "            char_ids.append(chars)\n",
    "        \n",
    "        tag_ids = [self.tag2idx[tag] for tag in tags]\n",
    "        \n",
    "        return {\n",
    "            'word_ids': torch.LongTensor(word_ids),\n",
    "            'char_ids': torch.LongTensor(char_ids),\n",
    "            'tag_ids': torch.LongTensor(tag_ids),\n",
    "            'length': len(tokens)\n",
    "        }\n",
    "\n",
    "train_dataset = NERDataset(train_tokens, train_tags, word2idx, char2idx, tag2idx)\n",
    "val_dataset = NERDataset(val_tokens, val_tags, word2idx, char2idx, tag2idx)\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"Val dataset: {len(val_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 2823\n",
      "Val batches: 314\n"
     ]
    }
   ],
   "source": [
    "# Collate function\n",
    "def collate_fn(batch):\n",
    "    batch = sorted(batch, key=lambda x: x['length'], reverse=True)\n",
    "    \n",
    "    word_ids = [item['word_ids'] for item in batch]\n",
    "    char_ids = [item['char_ids'] for item in batch]\n",
    "    tag_ids = [item['tag_ids'] for item in batch]\n",
    "    lengths = [item['length'] for item in batch]\n",
    "    \n",
    "    word_ids_padded = pad_sequence(word_ids, batch_first=True, padding_value=word2idx['<PAD>'])\n",
    "    tag_ids_padded = pad_sequence(tag_ids, batch_first=True, padding_value=tag2idx['O'])\n",
    "    \n",
    "    max_len = max(1, word_ids_padded.size(1))\n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    char_ids_padded = torch.full(\n",
    "        (batch_size, max_len, MAX_CHAR_LEN),\n",
    "        fill_value=char2idx['<PAD>'],\n",
    "        dtype=torch.long\n",
    "    )\n",
    "    \n",
    "    for i, chars in enumerate(char_ids):\n",
    "        seq_len = chars.size(0)\n",
    "        if seq_len > 0:\n",
    "            char_ids_padded[i, :seq_len, :] = chars\n",
    "    \n",
    "    mask = torch.zeros((batch_size, max_len), dtype=torch.bool)\n",
    "    for i, length in enumerate(lengths):\n",
    "        if length > 0:\n",
    "            mask[i, :length] = True\n",
    "    \n",
    "    return {\n",
    "        'word_ids': word_ids_padded,\n",
    "        'char_ids': char_ids_padded,\n",
    "        'tag_ids': tag_ids_padded,\n",
    "        'lengths': lengths,\n",
    "        'mask': mask\n",
    "    }\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 6. Model Architecture (Same as v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model architecture defined (v3 with larger vocabulary)!\n"
     ]
    }
   ],
   "source": [
    "class CharCNN(nn.Module):\n",
    "    def __init__(self, char_vocab_size, char_emb_dim, char_hidden_dim, max_char_len, dropout=0.6):\n",
    "        super().__init__()\n",
    "        self.char_embedding = nn.Embedding(char_vocab_size, char_emb_dim, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=char_emb_dim,\n",
    "            out_channels=char_hidden_dim,\n",
    "            kernel_size=3,\n",
    "            padding=1\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, char_ids):\n",
    "        batch_size, seq_len, max_char_len = char_ids.size()\n",
    "        \n",
    "        char_ids = char_ids.view(-1, max_char_len)\n",
    "        char_embeds = self.char_embedding(char_ids)\n",
    "        char_embeds = self.dropout(char_embeds)\n",
    "        char_embeds = char_embeds.transpose(1, 2)\n",
    "        \n",
    "        char_conv = self.relu(self.conv(char_embeds))\n",
    "        char_features = torch.max(char_conv, dim=2)[0]\n",
    "        char_features = char_features.view(batch_size, seq_len, -1)\n",
    "        \n",
    "        return char_features\n",
    "\n",
    "\n",
    "class BiLSTM_CRF_v3(nn.Module):\n",
    "    \"\"\"Same as v2, but with larger vocabulary\"\"\"\n",
    "    def __init__(self, vocab_size, char_vocab_size, embedding_dim, char_emb_dim,\n",
    "                 char_hidden_dim, lstm_hidden_dim, num_tags, dropout=0.6, embedding_matrix=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Word embeddings\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        if embedding_matrix is not None:\n",
    "            self.word_embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "            self.word_embedding.weight.requires_grad = True\n",
    "        \n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Character CNN\n",
    "        self.char_cnn = CharCNN(char_vocab_size, char_emb_dim, char_hidden_dim, MAX_CHAR_LEN, dropout=dropout)\n",
    "        \n",
    "        # BiLSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim + char_hidden_dim,\n",
    "            hidden_size=lstm_hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        self.lstm_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Linear layer\n",
    "        self.fc = nn.Linear(lstm_hidden_dim * 2, num_tags)\n",
    "        \n",
    "        # CRF\n",
    "        self.crf = CRF(num_tags, batch_first=True)\n",
    "    \n",
    "    def forward(self, word_ids, char_ids, tags=None, mask=None):\n",
    "        # Word embeddings with dropout\n",
    "        word_embeds = self.word_embedding(word_ids)\n",
    "        word_embeds = self.embedding_dropout(word_embeds)\n",
    "        \n",
    "        # Character features\n",
    "        char_features = self.char_cnn(char_ids)\n",
    "        \n",
    "        # Concatenate\n",
    "        combined = torch.cat([word_embeds, char_features], dim=-1)\n",
    "        \n",
    "        # BiLSTM with dropout\n",
    "        lstm_out, _ = self.lstm(combined)\n",
    "        lstm_out = self.lstm_dropout(lstm_out)\n",
    "        \n",
    "        # Emission scores\n",
    "        emissions = self.fc(lstm_out)\n",
    "        \n",
    "        if tags is not None:\n",
    "            loss = -self.crf(emissions, tags, mask=mask, reduction='mean')\n",
    "            return loss\n",
    "        else:\n",
    "            predictions = self.crf.decode(emissions, mask=mask)\n",
    "            return predictions\n",
    "\n",
    "print(\"âœ… Model architecture defined (v3 with larger vocabulary)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 7. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Model initialized (v3)!\n",
      "Total parameters: 33,509,779\n",
      "\n",
      "ðŸ“Š Comparison with v2:\n",
      "   v2: 12,253,879 parameters (vocab=36,790)\n",
      "   v3: 33,509,779 parameters (vocab=107,643)\n",
      "   Difference: +21,255,900 parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters (same as v2)\n",
    "CHAR_EMB_DIM = 25\n",
    "CHAR_HIDDEN_DIM = 30\n",
    "LSTM_HIDDEN_DIM = 256\n",
    "DROPOUT = 0.6\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "# Initialize model\n",
    "model = BiLSTM_CRF_v3(\n",
    "    vocab_size=vocab_size,\n",
    "    char_vocab_size=char_vocab_size,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    char_emb_dim=CHAR_EMB_DIM,\n",
    "    char_hidden_dim=CHAR_HIDDEN_DIM,\n",
    "    lstm_hidden_dim=LSTM_HIDDEN_DIM,\n",
    "    num_tags=num_tags,\n",
    "    dropout=DROPOUT,\n",
    "    embedding_matrix=embedding_matrix\n",
    ").to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.5, patience=2, verbose=True\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"\\nâœ… Model initialized (v3)!\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"\\nðŸ“Š Comparison with v2:\")\n",
    "print(f\"   v2: 12,253,879 parameters (vocab=36,790)\")\n",
    "print(f\"   v3: {total_params:,} parameters (vocab={vocab_size:,})\")\n",
    "print(f\"   Difference: +{total_params - 12253879:,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training functions defined!\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, data_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    pbar = tqdm(data_loader, desc=\"Training\", leave=False)\n",
    "    \n",
    "    for batch in pbar:\n",
    "        word_ids = batch['word_ids'].to(device)\n",
    "        char_ids = batch['char_ids'].to(device)\n",
    "        tag_ids = batch['tag_ids'].to(device)\n",
    "        mask = batch['mask'].to(device)\n",
    "        lengths = batch['lengths']\n",
    "        \n",
    "        # Filter empty sequences\n",
    "        non_empty_indices = [i for i, length in enumerate(lengths) if length > 0]\n",
    "        \n",
    "        if len(non_empty_indices) == 0:\n",
    "            continue\n",
    "        \n",
    "        if len(non_empty_indices) < len(lengths):\n",
    "            word_ids = word_ids[non_empty_indices]\n",
    "            char_ids = char_ids[non_empty_indices]\n",
    "            tag_ids = tag_ids[non_empty_indices]\n",
    "            mask = mask[non_empty_indices]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = model(word_ids, char_ids, tag_ids, mask)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return total_loss / max(num_batches, 1)\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_true_tags = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(data_loader, desc=\"Evaluating\", leave=False)\n",
    "        \n",
    "        for batch in pbar:\n",
    "            word_ids = batch['word_ids'].to(device)\n",
    "            char_ids = batch['char_ids'].to(device)\n",
    "            tag_ids = batch['tag_ids'].to(device)\n",
    "            mask = batch['mask'].to(device)\n",
    "            lengths = batch['lengths']\n",
    "            \n",
    "            non_empty_indices = [i for i, length in enumerate(lengths) if length > 0]\n",
    "            \n",
    "            if len(non_empty_indices) > 0:\n",
    "                word_ids_non_empty = word_ids[non_empty_indices]\n",
    "                char_ids_non_empty = char_ids[non_empty_indices]\n",
    "                mask_non_empty = mask[non_empty_indices]\n",
    "                \n",
    "                predictions_non_empty = model(word_ids_non_empty, char_ids_non_empty, mask=mask_non_empty)\n",
    "            else:\n",
    "                predictions_non_empty = []\n",
    "            \n",
    "            predictions = []\n",
    "            non_empty_iter = iter(predictions_non_empty)\n",
    "            for i in range(len(lengths)):\n",
    "                if lengths[i] == 0:\n",
    "                    predictions.append([])\n",
    "                else:\n",
    "                    predictions.append(next(non_empty_iter))\n",
    "            \n",
    "            for i, (pred, length) in enumerate(zip(predictions, lengths)):\n",
    "                if length == 0:\n",
    "                    pred_tags = []\n",
    "                    true_tags = []\n",
    "                else:\n",
    "                    pred_tags = [idx2tag[idx] for idx in pred[:length]]\n",
    "                    true_tags = [idx2tag[tag_ids[i][j].item()] for j in range(length)]\n",
    "                \n",
    "                all_predictions.append(pred_tags)\n",
    "                all_true_tags.append(true_tags)\n",
    "    \n",
    "    return all_true_tags, all_predictions\n",
    "\n",
    "print(\"Training functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 9. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training (v3 with MIN_WORD_FREQ=1)...\n",
      "\n",
      "================================================================================\n",
      "ðŸ”¬ Experiment: MIN_WORD_FREQ = 1 (ALL words)\n",
      "   Vocabulary size: 107,643 (vs 36,790 in v2)\n",
      "   Model parameters: 33,509,779 (vs 12,253,879 in v2)\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/20 | Loss: 3.5427 | Val P: 0.7260 R: 0.6418 F1: 0.6813 | LR: 0.001000 | Time: 309.1s\n",
      "  â†’ New best F1! Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2/20 | Loss: 1.8265 | Val P: 0.7513 R: 0.6660 F1: 0.7060 | LR: 0.001000 | Time: 303.8s\n",
      "  â†’ New best F1! Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3/20 | Loss: 1.4933 | Val P: 0.7694 R: 0.6854 F1: 0.7250 | LR: 0.001000 | Time: 290.0s\n",
      "  â†’ New best F1! Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4/20 | Loss: 1.3019 | Val P: 0.7714 R: 0.7029 F1: 0.7356 | LR: 0.001000 | Time: 282.6s\n",
      "  â†’ New best F1! Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5/20 | Loss: 1.1552 | Val P: 0.7709 R: 0.7171 F1: 0.7430 | LR: 0.001000 | Time: 284.9s\n",
      "  â†’ New best F1! Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6/20 | Loss: 1.0462 | Val P: 0.7790 R: 0.7251 F1: 0.7511 | LR: 0.001000 | Time: 289.1s\n",
      "  â†’ New best F1! Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7/20 | Loss: 0.9493 | Val P: 0.7843 R: 0.7271 F1: 0.7546 | LR: 0.001000 | Time: 302.0s\n",
      "  â†’ New best F1! Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8/20 | Loss: 0.8750 | Val P: 0.7765 R: 0.7303 F1: 0.7527 | LR: 0.001000 | Time: 309.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9/20 | Loss: 0.8073 | Val P: 0.7816 R: 0.7306 F1: 0.7552 | LR: 0.001000 | Time: 304.9s\n",
      "  â†’ New best F1! Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 | Loss: 0.7446 | Val P: 0.7773 R: 0.7249 F1: 0.7502 | LR: 0.001000 | Time: 310.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 | Loss: 0.6929 | Val P: 0.7763 R: 0.7239 F1: 0.7492 | LR: 0.001000 | Time: 294.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m epoch_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[1;32m     21\u001b[0m val_true_tags, val_pred_tags \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, device)\n",
      "Cell \u001b[0;32mIn[13], line 29\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, data_loader, optimizer, device)\u001b[0m\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     28\u001b[0m loss \u001b[38;5;241m=\u001b[39m model(word_ids, char_ids, tag_ids, mask)\n\u001b[0;32m---> 29\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Starting training (v3 with MIN_WORD_FREQ=1)...\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(\"ðŸ”¬ Experiment: MIN_WORD_FREQ = 1 (ALL words)\")\n",
    "print(f\"   Vocabulary size: {vocab_size:,} (vs 36,790 in v2)\")\n",
    "print(f\"   Model parameters: {total_params:,} (vs 12,253,879 in v2)\")\n",
    "print(\"=\"  * 80 + \"\\n\")\n",
    "\n",
    "best_f1 = 0\n",
    "patience = 7\n",
    "patience_counter = 0\n",
    "\n",
    "training_start = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    \n",
    "    # Evaluate\n",
    "    val_true_tags, val_pred_tags = evaluate(model, val_loader, device)\n",
    "    \n",
    "    # Calculate F1\n",
    "    results = evaluate_entity_spans(val_true_tags, val_pred_tags, val_tokens)\n",
    "    val_f1 = results['f1']\n",
    "    val_precision = results['precision']\n",
    "    val_recall = results['recall']\n",
    "    \n",
    "    # Update learning rate scheduler\n",
    "    scheduler.step(val_f1)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:2d}/{NUM_EPOCHS} | \"\n",
    "          f\"Loss: {train_loss:.4f} | \"\n",
    "          f\"Val P: {val_precision:.4f} R: {val_recall:.4f} F1: {val_f1:.4f} | \"\n",
    "          f\"LR: {current_lr:.6f} | \"\n",
    "          f\"Time: {epoch_time:.1f}s\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        patience_counter = 0\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "        torch.save(model.state_dict(), 'models/bilstm_crf_v3_best.pt')\n",
    "        print(f\"  â†’ New best F1! Model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping after {epoch+1} epochs (patience={patience})\")\n",
    "            break\n",
    "\n",
    "training_time = time.time() - training_start\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTraining completed in {training_time:.1f}s ({training_time/60:.1f} minutes)\")\n",
    "print(f\"Best validation F1: {best_f1:.4f}\")\n",
    "print(f\"\\nðŸ“Š Comparison with v2:\")\n",
    "print(f\"   v2 (MIN_WORD_FREQ=2): 0.7594 F1\")\n",
    "print(f\"   v3 (MIN_WORD_FREQ=1): {best_f1:.4f} F1\")\n",
    "print(f\"   Difference: {(best_f1 - 0.7594)*100:+.2f}% absolute\")\n",
    "print(f\"\\nðŸ”¬ Conclusion:\")\n",
    "if best_f1 > 0.7594 + 0.001:\n",
    "    print(\"   âœ… Using ALL words improves performance!\")\n",
    "elif best_f1 < 0.7594 - 0.001:\n",
    "    print(\"   âŒ Filtering rare words is better (prevents overfitting)\")\n",
    "else:\n",
    "    print(\"   â‰ˆ No significant difference - CharCNN handles rare words well!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## 10. Load Best Model and Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ENTITY-SPAN LEVEL EVALUATION REPORT: Character-CNN + BiLSTM-CRF v3 (MIN_WORD_FREQ=1)\n",
      "================================================================================\n",
      "\n",
      "OVERALL METRICS:\n",
      "  Precision: 0.7816\n",
      "  Recall:    0.7306\n",
      "  F1 Score:  0.7552\n",
      "\n",
      "  True Positives:  8038\n",
      "  False Positives: 2246\n",
      "  False Negatives: 2964\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "PER-ENTITY-TYPE METRICS:\n",
      "--------------------------------------------------------------------------------\n",
      "Entity Type          Precision    Recall       F1           Support   \n",
      "--------------------------------------------------------------------------------\n",
      "Artist               0.7313       0.8255       0.7756       2430      \n",
      "Facility             0.7797       0.6547       0.7118       1173      \n",
      "HumanSettlement      0.8978       0.9051       0.9014       2697      \n",
      "ORG                  0.7871       0.6498       0.7119       1542      \n",
      "OtherPER             0.6859       0.4977       0.5769       1527      \n",
      "Politician           0.7374       0.6348       0.6822       1150      \n",
      "PublicCorp           0.7103       0.6853       0.6976       483       \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load('models/bilstm_crf_v3_best.pt'))\n",
    "model.eval()\n",
    "\n",
    "print(\"Best model loaded!\")\n",
    "\n",
    "# Final evaluation\n",
    "val_true_tags, val_pred_tags = evaluate(model, val_loader, device)\n",
    "\n",
    "# Comprehensive report\n",
    "print_evaluation_report(\n",
    "    val_true_tags,\n",
    "    val_pred_tags,\n",
    "    val_tokens,\n",
    "    model_name=\"Character-CNN + BiLSTM-CRF v3 (MIN_WORD_FREQ=1)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## 11. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3dc26f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_time = \"55 minutes (early manual stop)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cell-27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabularies saved!\n",
      "Results saved!\n"
     ]
    }
   ],
   "source": [
    "# Save vocabularies\n",
    "vocab_data = {\n",
    "    'word2idx': word2idx,\n",
    "    'char2idx': char2idx,\n",
    "    'tag2idx': tag2idx,\n",
    "    'idx2word': idx2word,\n",
    "    'idx2char': idx2char,\n",
    "    'idx2tag': idx2tag\n",
    "}\n",
    "\n",
    "with open('models/bilstm_crf_v3_vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab_data, f)\n",
    "\n",
    "print(\"Vocabularies saved!\")\n",
    "\n",
    "# Save results\n",
    "final_results = evaluate_entity_spans(val_true_tags, val_pred_tags, val_tokens)\n",
    "\n",
    "results_summary = {\n",
    "    'model': 'Character-CNN + BiLSTM-CRF v3',\n",
    "    'version': 'v3',\n",
    "    'experiment': 'MIN_WORD_FREQ = 1 (all words)',\n",
    "    'precision': final_results['precision'],\n",
    "    'recall': final_results['recall'],\n",
    "    'f1': final_results['f1'],\n",
    "    'training_time': training_time,\n",
    "    'num_epochs': epoch + 1,\n",
    "    'hyperparameters': {\n",
    "        'min_word_freq': MIN_WORD_FREQ,\n",
    "        'vocab_size': vocab_size,\n",
    "        'embedding_dim': EMBEDDING_DIM,\n",
    "        'char_emb_dim': CHAR_EMB_DIM,\n",
    "        'char_hidden_dim': CHAR_HIDDEN_DIM,\n",
    "        'lstm_hidden_dim': LSTM_HIDDEN_DIM,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'dropout': DROPOUT,\n",
    "        'patience': patience,\n",
    "        'gradient_clipping': 1.0\n",
    "    },\n",
    "    'comparison': {\n",
    "        'v2_f1': 0.7594,\n",
    "        'v2_vocab_size': 36790,\n",
    "        'v3_f1': final_results['f1'],\n",
    "        'v3_vocab_size': vocab_size,\n",
    "        'difference': final_results['f1'] - 0.7594\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('models/bilstm_crf_v3_results.json', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(\"Results saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## 12. Summary\n",
    "\n",
    "### Experiment: MIN_WORD_FREQ = 1 vs 2\n",
    "\n",
    "**Hypothesis Testing:**\n",
    "- Does including ALL words (even those with frequency=1) improve NER performance?\n",
    "\n",
    "**Results will show one of three outcomes:**\n",
    "\n",
    "1. **v3 > v2**: Rare word information is valuable\n",
    "   - Even words appearing once contain useful information\n",
    "   - Proper nouns and rare entities benefit from dedicated embeddings\n",
    "\n",
    "2. **v3 < v2**: Vocabulary filtering prevents overfitting\n",
    "   - Frequency-1 words are often noise (typos, artifacts)\n",
    "   - Smaller vocabulary generalizes better\n",
    "\n",
    "3. **v3 â‰ˆ v2**: No significant difference\n",
    "   - CharCNN effectively handles rare/unknown words\n",
    "   - Word-level embeddings less important when char features are strong\n",
    "\n",
    "**Trade-offs:**\n",
    "- Model size: v3 is larger (~1M more parameters)\n",
    "- Training time: v3 may be slightly slower\n",
    "- Memory: v3 requires more GPU memory\n",
    "\n",
    "Check the results above to see which approach works better for your dataset!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
