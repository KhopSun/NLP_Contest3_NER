{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity-Span Level Evaluation Metrics\n",
    "\n",
    "This notebook demonstrates how to use the evaluation functions from `utils.py` for entity-span level evaluation.\n",
    "\n",
    "## Why Entity-Span Level?\n",
    "\n",
    "The assignment requires **entity-span level evaluation**, not token-level:\n",
    "- A predicted entity is correct **ONLY** if the entire span matches (start, end, and type)\n",
    "- Partial matches do NOT count\n",
    "- This is \"strict\" evaluation\n",
    "\n",
    "## What's in utils.py?\n",
    "\n",
    "1. `extract_entities()` - Extract entity spans from BIO tags\n",
    "2. `evaluate_entity_spans()` - Calculate overall P/R/F1\n",
    "3. `evaluate_entity_spans_by_type()` - Calculate per-entity-type metrics\n",
    "4. `print_evaluation_report()` - Pretty print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation functions\n",
    "from utils import (\n",
    "    extract_entities,\n",
    "    evaluate_entity_spans,\n",
    "    evaluate_entity_spans_by_type,\n",
    "    print_evaluation_report\n",
    ")\n",
    "\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test Evaluation Functions\n",
    "\n",
    "Let's test the evaluation functions with simple examples to understand how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TEST CASE 1: Perfect Prediction\n",
      "================================================================================\n",
      "\n",
      "Tokens: ['Barack', 'Obama', 'visited', 'Paris', '.']\n",
      "True tags: ['B-Politician', 'I-Politician', 'O', 'B-HumanSettlement', 'O']\n",
      "Pred tags: ['B-Politician', 'I-Politician', 'O', 'B-HumanSettlement', 'O']\n",
      "\n",
      "True entities: [('Barack Obama', 'Politician', 0, 1), ('Paris', 'HumanSettlement', 3, 3)]\n",
      "Pred entities: [('Barack Obama', 'Politician', 0, 1), ('Paris', 'HumanSettlement', 3, 3)]\n",
      "\n",
      "Precision: 1.0000\n",
      "Recall:    1.0000\n",
      "F1 Score:  1.0000\n",
      "\n",
      "✅ Perfect match! F1 = 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TEST CASE 1: Perfect Prediction\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "tokens = [[\"Barack\", \"Obama\", \"visited\", \"Paris\", \".\"]]\n",
    "true_tags = [[\"B-Politician\", \"I-Politician\", \"O\", \"B-HumanSettlement\", \"O\"]]\n",
    "pred_tags = [[\"B-Politician\", \"I-Politician\", \"O\", \"B-HumanSettlement\", \"O\"]]\n",
    "\n",
    "print(\"\\nTokens:\", tokens[0])\n",
    "print(\"True tags:\", true_tags[0])\n",
    "print(\"Pred tags:\", pred_tags[0])\n",
    "\n",
    "# Extract entities\n",
    "true_entities = extract_entities(tokens[0], true_tags[0])\n",
    "pred_entities = extract_entities(tokens[0], pred_tags[0])\n",
    "\n",
    "print(\"\\nTrue entities:\", true_entities)\n",
    "print(\"Pred entities:\", pred_entities)\n",
    "\n",
    "# Evaluate\n",
    "result = evaluate_entity_spans(true_tags, pred_tags, tokens)\n",
    "print(f\"\\nPrecision: {result['precision']:.4f}\")\n",
    "print(f\"Recall:    {result['recall']:.4f}\")\n",
    "print(f\"F1 Score:  {result['f1']:.4f}\")\n",
    "print(\"\\n✅ Perfect match! F1 = 1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TEST CASE 2: Missed Entity (False Negative)\n",
      "================================================================================\n",
      "\n",
      "Tokens: ['Barack', 'Obama', 'visited', 'Paris', '.']\n",
      "True tags: ['B-Politician', 'I-Politician', 'O', 'B-HumanSettlement', 'O']\n",
      "Pred tags: ['B-Politician', 'I-Politician', 'O', 'O', 'O']\n",
      "\n",
      "True entities: [('Barack Obama', 'Politician', 0, 1), ('Paris', 'HumanSettlement', 3, 3)]\n",
      "Pred entities: [('Barack Obama', 'Politician', 0, 1)]\n",
      "\n",
      "Precision: 1.0000 (1 predicted, 1 correct)\n",
      "Recall:    0.5000 (2 true, 1 found)\n",
      "F1 Score:  0.6667\n",
      "\n",
      "⚠️ Missed 'Paris', so recall drops to 0.5\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TEST CASE 2: Missed Entity (False Negative)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "tokens = [[\"Barack\", \"Obama\", \"visited\", \"Paris\", \".\"]]\n",
    "true_tags = [[\"B-Politician\", \"I-Politician\", \"O\", \"B-HumanSettlement\", \"O\"]]\n",
    "pred_tags = [[\"B-Politician\", \"I-Politician\", \"O\", \"O\", \"O\"]]  # Missed Paris\n",
    "\n",
    "print(\"\\nTokens:\", tokens[0])\n",
    "print(\"True tags:\", true_tags[0])\n",
    "print(\"Pred tags:\", pred_tags[0])\n",
    "\n",
    "# Extract entities\n",
    "true_entities = extract_entities(tokens[0], true_tags[0])\n",
    "pred_entities = extract_entities(tokens[0], pred_tags[0])\n",
    "\n",
    "print(\"\\nTrue entities:\", true_entities)\n",
    "print(\"Pred entities:\", pred_entities)\n",
    "\n",
    "# Evaluate\n",
    "result = evaluate_entity_spans(true_tags, pred_tags, tokens)\n",
    "print(f\"\\nPrecision: {result['precision']:.4f} (1 predicted, 1 correct)\")\n",
    "print(f\"Recall:    {result['recall']:.4f} (2 true, 1 found)\")\n",
    "print(f\"F1 Score:  {result['f1']:.4f}\")\n",
    "print(\"\\n⚠️ Missed 'Paris', so recall drops to 0.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TEST CASE 3: Wrong Span Boundary (Incorrect Prediction)\n",
      "================================================================================\n",
      "\n",
      "Tokens: ['Barack', 'Hussein', 'Obama', 'Jr', '.']\n",
      "True tags: ['B-Politician', 'I-Politician', 'I-Politician', 'I-Politician', 'O']\n",
      "Pred tags: ['B-Politician', 'I-Politician', 'O', 'O', 'O']\n",
      "\n",
      "True entities: [('Barack Hussein Obama Jr', 'Politician', 0, 3)]\n",
      "Pred entities: [('Barack Hussein', 'Politician', 0, 1)]\n",
      "\n",
      "True Positives:  0\n",
      "False Positives: 1\n",
      "False Negatives: 1\n",
      "\n",
      "Precision: 0.0000\n",
      "Recall:    0.0000\n",
      "F1 Score:  0.0000\n",
      "\n",
      "❌ Span boundaries don't match! Counts as both FP and FN\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TEST CASE 3: Wrong Span Boundary (Incorrect Prediction)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "tokens = [[\"Barack\", \"Hussein\", \"Obama\", \"Jr\", \".\"]]\n",
    "true_tags = [[\"B-Politician\", \"I-Politician\", \"I-Politician\", \"I-Politician\", \"O\"]]\n",
    "pred_tags = [[\"B-Politician\", \"I-Politician\", \"O\", \"O\", \"O\"]]  # Span too short!\n",
    "\n",
    "print(\"\\nTokens:\", tokens[0])\n",
    "print(\"True tags:\", true_tags[0])\n",
    "print(\"Pred tags:\", pred_tags[0])\n",
    "\n",
    "# Extract entities\n",
    "true_entities = extract_entities(tokens[0], true_tags[0])\n",
    "pred_entities = extract_entities(tokens[0], pred_tags[0])\n",
    "\n",
    "print(\"\\nTrue entities:\", true_entities)\n",
    "print(\"Pred entities:\", pred_entities)\n",
    "\n",
    "# Evaluate\n",
    "result = evaluate_entity_spans(true_tags, pred_tags, tokens)\n",
    "print(f\"\\nTrue Positives:  {result['true_positives']}\")\n",
    "print(f\"False Positives: {result['false_positives']}\")\n",
    "print(f\"False Negatives: {result['false_negatives']}\")\n",
    "print(f\"\\nPrecision: {result['precision']:.4f}\")\n",
    "print(f\"Recall:    {result['recall']:.4f}\")\n",
    "print(f\"F1 Score:  {result['f1']:.4f}\")\n",
    "print(\"\\n❌ Span boundaries don't match! Counts as both FP and FN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TEST CASE 4: Wrong Entity Type\n",
      "================================================================================\n",
      "\n",
      "Tokens: ['Barack', 'Obama', '.']\n",
      "True tags: ['B-Politician', 'I-Politician', 'O']\n",
      "Pred tags: ['B-Artist', 'I-Artist', 'O']\n",
      "\n",
      "True entities: [('Barack Obama', 'Politician', 0, 1)]\n",
      "Pred entities: [('Barack Obama', 'Artist', 0, 1)]\n",
      "\n",
      "True Positives:  0\n",
      "False Positives: 1\n",
      "False Negatives: 1\n",
      "\n",
      "F1 Score: 0.0000\n",
      "\n",
      "❌ Span correct but type wrong! Still counts as incorrect\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TEST CASE 4: Wrong Entity Type\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "tokens = [[\"Barack\", \"Obama\", \".\"]]\n",
    "true_tags = [[\"B-Politician\", \"I-Politician\", \"O\"]]\n",
    "pred_tags = [[\"B-Artist\", \"I-Artist\", \"O\"]]  # Wrong type!\n",
    "\n",
    "print(\"\\nTokens:\", tokens[0])\n",
    "print(\"True tags:\", true_tags[0])\n",
    "print(\"Pred tags:\", pred_tags[0])\n",
    "\n",
    "# Extract entities\n",
    "true_entities = extract_entities(tokens[0], true_tags[0])\n",
    "pred_entities = extract_entities(tokens[0], pred_tags[0])\n",
    "\n",
    "print(\"\\nTrue entities:\", true_entities)\n",
    "print(\"Pred entities:\", pred_entities)\n",
    "\n",
    "# Evaluate\n",
    "result = evaluate_entity_spans(true_tags, pred_tags, tokens)\n",
    "print(f\"\\nTrue Positives:  {result['true_positives']}\")\n",
    "print(f\"False Positives: {result['false_positives']}\")\n",
    "print(f\"False Negatives: {result['false_negatives']}\")\n",
    "print(f\"\\nF1 Score: {result['f1']:.4f}\")\n",
    "print(\"\\n❌ Span correct but type wrong! Still counts as incorrect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Real Data and Test\n",
    "\n",
    "Now let's load the actual validation data and test the evaluation on a subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10,036 validation samples\n",
      "\n",
      "Example validation sample:\n",
      "Tokens: ['in', '1933', 'phil', 'spitalny', 'directed', 'the', 'orchestra', 'for', 'the']...\n",
      "Tags:   ['O', 'O', 'B-Artist', 'I-Artist', 'O', 'O', 'O', 'O', 'O']...\n"
     ]
    }
   ],
   "source": [
    "# Load validation data\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"Load JSONL file\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.strip()))\n",
    "    return data\n",
    "\n",
    "val_data = load_jsonl('val_split.jsonl')\n",
    "print(f\"Loaded {len(val_data):,} validation samples\")\n",
    "\n",
    "# Extract tokens and tags\n",
    "val_tokens = [sample['tokens'] for sample in val_data]\n",
    "val_true_tags = [sample['ner_tags'] for sample in val_data]\n",
    "\n",
    "print(f\"\\nExample validation sample:\")\n",
    "print(f\"Tokens: {val_tokens[0][:10]}...\")  # First 10 tokens\n",
    "print(f\"Tags:   {val_true_tags[0][:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simulate Model Predictions\n",
    "\n",
    "Let's create some dummy predictions to test the full evaluation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dummy predictions for testing...\n",
      "\n",
      "Evaluating perfect predictions (sanity check):\n",
      "  F1: 1.0000 (should be 1.0)\n",
      "  ✅ Sanity check passed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create dummy predictions (for testing)\n",
    "# In reality, this would come from your trained model\n",
    "\n",
    "print(\"Creating dummy predictions for testing...\\n\")\n",
    "\n",
    "# Scenario 1: Perfect predictions (sanity check)\n",
    "perfect_preds = val_true_tags.copy()\n",
    "\n",
    "print(\"Evaluating perfect predictions (sanity check):\")\n",
    "result = evaluate_entity_spans(val_true_tags, perfect_preds, val_tokens)\n",
    "print(f\"  F1: {result['f1']:.4f} (should be 1.0)\")\n",
    "assert result['f1'] == 1.0, \"Perfect predictions should have F1 = 1.0\"\n",
    "print(\"  ✅ Sanity check passed!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating random baseline (all O):\n",
      "  Precision: 0.0000\n",
      "  Recall:    0.0000\n",
      "  F1:        0.0000\n",
      "\n",
      "  (F1 should be 0.0 since we predict no entities)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scenario 2: Random baseline (predict all O)\n",
    "random_preds = [['O'] * len(tags) for tags in val_true_tags]\n",
    "\n",
    "print(\"Evaluating random baseline (all O):\")\n",
    "result = evaluate_entity_spans(val_true_tags, random_preds, val_tokens)\n",
    "print(f\"  Precision: {result['precision']:.4f}\")\n",
    "print(f\"  Recall:    {result['recall']:.4f}\")\n",
    "print(f\"  F1:        {result['f1']:.4f}\")\n",
    "print(\"\\n  (F1 should be 0.0 since we predict no entities)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating noisy predictions (80% token accuracy):\n",
      "  Precision: 0.6537\n",
      "  Recall:    0.6387\n",
      "  F1:        0.6461\n",
      "\n",
      "  (Note: 80% token accuracy != 80% span F1, because entire spans must match!)\n"
     ]
    }
   ],
   "source": [
    "# Scenario 3: Noisy predictions (80% correct)\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "noisy_preds = []\n",
    "for tags in val_true_tags:\n",
    "    noisy_tags = []\n",
    "    for tag in tags:\n",
    "        # 80% chance of correct tag, 20% chance of O\n",
    "        if random.random() < 0.8:\n",
    "            noisy_tags.append(tag)\n",
    "        else:\n",
    "            noisy_tags.append('O')\n",
    "    noisy_preds.append(noisy_tags)\n",
    "\n",
    "print(\"Evaluating noisy predictions (80% token accuracy):\")\n",
    "result = evaluate_entity_spans(val_true_tags, noisy_preds, val_tokens)\n",
    "print(f\"  Precision: {result['precision']:.4f}\")\n",
    "print(f\"  Recall:    {result['recall']:.4f}\")\n",
    "print(f\"  F1:        {result['f1']:.4f}\")\n",
    "print(\"\\n  (Note: 80% token accuracy != 80% span F1, because entire spans must match!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using print_evaluation_report()\n",
    "\n",
    "This function gives you a nice formatted report with both overall and per-entity-type metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ENTITY-SPAN LEVEL EVALUATION REPORT: Noisy Baseline (80% token accuracy)\n",
      "================================================================================\n",
      "\n",
      "OVERALL METRICS:\n",
      "  Precision: 0.6537\n",
      "  Recall:    0.6387\n",
      "  F1 Score:  0.6461\n",
      "\n",
      "  True Positives:  8607\n",
      "  False Positives: 4559\n",
      "  False Negatives: 4868\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "PER-ENTITY-TYPE METRICS:\n",
      "--------------------------------------------------------------------------------\n",
      "Entity Type          Precision    Recall       F1           Support   \n",
      "--------------------------------------------------------------------------------\n",
      "Artist               0.6609       0.6423       0.6515       2849      \n",
      "Facility             0.5382       0.5683       0.5528       1487      \n",
      "HumanSettlement      0.8281       0.7207       0.7707       3476      \n",
      "ORG                  0.5455       0.5800       0.5622       1893      \n",
      "OtherPER             0.6029       0.6060       0.6044       1779      \n",
      "Politician           0.5821       0.6020       0.5919       1402      \n",
      "PublicCorp           0.7387       0.6910       0.7140       589       \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Print detailed evaluation report\n",
    "print_evaluation_report(\n",
    "    val_true_tags,\n",
    "    noisy_preds,\n",
    "    val_tokens,\n",
    "    model_name=\"Noisy Baseline (80% token accuracy)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Per-Entity-Type Analysis\n",
    "\n",
    "Understanding which entity types your model struggles with is crucial for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-Entity-Type Performance:\n",
      "\n",
      "Entity Type          F1         Support   \n",
      "----------------------------------------\n",
      "HumanSettlement      0.7707     3476      \n",
      "PublicCorp           0.7140     589       \n",
      "Artist               0.6515     2849      \n",
      "OtherPER             0.6044     1779      \n",
      "Politician           0.5919     1402      \n",
      "ORG                  0.5622     1893      \n",
      "Facility             0.5528     1487      \n",
      "\n",
      "Insights:\n",
      "- Entity types with low F1 might need more training data or better features\n",
      "- Entity types with low support might benefit from data augmentation\n"
     ]
    }
   ],
   "source": [
    "# Get per-type metrics\n",
    "by_type = evaluate_entity_spans_by_type(val_true_tags, noisy_preds, val_tokens)\n",
    "\n",
    "print(\"Per-Entity-Type Performance:\\n\")\n",
    "print(f\"{'Entity Type':<20} {'F1':<10} {'Support':<10}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Sort by F1 score\n",
    "sorted_types = sorted(by_type.items(), key=lambda x: x[1]['f1'], reverse=True)\n",
    "\n",
    "for entity_type, metrics in sorted_types:\n",
    "    print(f\"{entity_type:<20} {metrics['f1']:<10.4f} {metrics['support']:<10}\")\n",
    "\n",
    "print(\"\\nInsights:\")\n",
    "print(\"- Entity types with low F1 might need more training data or better features\")\n",
    "print(\"- Entity types with low support might benefit from data augmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. How to Use in Your Model Notebooks\n",
    "\n",
    "When you train a model, use these functions like this:\n",
    "\n",
    "```python\n",
    "# In your model notebook (e.g., 3_HMM.ipynb, 4_CRF.ipynb, etc.)\n",
    "\n",
    "from utils import print_evaluation_report\n",
    "\n",
    "# 1. Train your model\n",
    "model.fit(train_data)\n",
    "\n",
    "# 2. Make predictions on validation set\n",
    "val_predictions = model.predict(val_tokens)\n",
    "\n",
    "# 3. Evaluate with one line!\n",
    "print_evaluation_report(\n",
    "    val_true_tags,\n",
    "    val_predictions,\n",
    "    val_tokens,\n",
    "    model_name=\"My Model Name\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Entity-span level evaluation is STRICT**:\n",
    "   - Entire span must match (start, end, type)\n",
    "   - Partial matches don't count\n",
    "   - 80% token accuracy ≠ 80% span F1\n",
    "\n",
    "2. **Functions available in utils.py**:\n",
    "   - `extract_entities()` - Extract spans from BIO tags\n",
    "   - `evaluate_entity_spans()` - Overall P/R/F1\n",
    "   - `evaluate_entity_spans_by_type()` - Per-type metrics\n",
    "   - `print_evaluation_report()` - Pretty print results\n",
    "\n",
    "3. **How to use**:\n",
    "   - Import functions from utils.py\n",
    "   - Pass true tags, predicted tags, and tokens\n",
    "   - Get comprehensive evaluation report\n",
    "\n",
    "4. **For your report**:\n",
    "   - Always report entity-span F1 (not token-level!)\n",
    "   - Include per-entity-type breakdown\n",
    "   - Analyze which entity types are challenging\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "Now you're ready to start building models! Each model notebook should:\n",
    "1. Import evaluation functions from utils.py\n",
    "2. Train the model\n",
    "3. Predict on validation set\n",
    "4. Evaluate using `print_evaluation_report()`\n",
    "5. Save results for comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
