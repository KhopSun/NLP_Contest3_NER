{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiCoNER 2 - Few-Shot NER with Gemini API (Optimized + Full Prompt)\n",
    "\n",
    "## Optimizations Applied:\n",
    "- **Batch Processing**: 10 examples per API call (10x faster)\n",
    "- **Structured JSON Output**: No parsing errors\n",
    "- **Original Full Prompt**: Keeping your detailed 2,500 token prompt for robustness\n",
    "- **Expected Time**: ~2 hours for 10K examples (vs 16+ hours before)\n",
    "- **Expected Cost**: ~$80-100 (vs $574 before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import google.generativeai as genai\n",
    "from google.colab import userdata\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(\"✓ All libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Gemini API with Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get API key from Colab secrets\n",
    "GOOGLE_API_KEY = userdata.get('gemini_api_key')\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# Configure for structured JSON output (prevents parsing errors)\n",
    "generation_config = {\n",
    "    \"response_mime_type\": \"application/json\",\n",
    "}\n",
    "\n",
    "# Initialize Gemini model with structured output\n",
    "gemini_model = genai.GenerativeModel(\n",
    "    'gemini-2.5-flash',\n",
    "    generation_config=generation_config\n",
    ")\n",
    "\n",
    "print(\"✓ Gemini API configured with structured JSON output\")\n",
    "print(f\"  Model: {gemini_model._model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Full System Prompt (Original Detailed Version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_SYSTEM_PROMPT = \"\"\"### SYSTEM INSTRUCTION\n",
    "**MODE: ACCURACY-FIRST / RESEARCH-ENABLED**\n",
    "You are a slow, deliberate reasoning agent. You must NOT guess.\n",
    "1. **Scan**: Identify every proper noun or capitalized token in the input.\n",
    "2. **Verify**: If you are not 100% sure of an entity's type (e.g., is \"Finisterre\" a place or a book?), you MUST pause and search the internet.\n",
    "3. **Classify**: Apply the strict class definitions below based on your search results.\n",
    "4. **Format**: Output only the final JSON list.\n",
    "\n",
    "---\n",
    "\n",
    "### Role\n",
    "You are an expert linguist and data labeling specialist specifically trained for the MultiCoNER 2 Shared Task. You possess deep knowledge of fine-grained Named Entity Recognition (NER). **Crucially, you act as a researcher who verifies facts using the internet when faced with ambiguity.**\n",
    "\n",
    "### Context\n",
    "The user will provide a list of text tokens (words/sub-words) derived from search queries, social media, or noisy web text. Your task is to analyze these tokens and map each one to a specific Named Entity Recognition tag. The data contains ambiguity, typos (e.g., \"united stats\"), and lacks capitalization cues.\n",
    "\n",
    "### Rules\n",
    "1. **Input**: A JSON list of tokens (e.g., `[\"new\", \"york\", \"is\", \"big\"]`).\n",
    "2. **Task**: Assign a BIO (Begin, Inside, Outside) tag to every single token.\n",
    "3. **Classes**: You must strictly use only the following 7 entity categories:\n",
    "   - **Artist**: Musicians, bands, actors, authors, directors, painters. (e.g., \"simon mayo\", \"picasso\")\n",
    "   - **Politician**: Government officials, politicians, heads of state. (e.g., \"obama\", \"frank d. o'connor\")\n",
    "   - **HumanSettlement**: Cities, towns, villages, states, countries, counties. (e.g., \"busan\", \"cleveland\", \"ohio\")\n",
    "   - **PublicCorp**: Commercial companies, businesses, brands. (e.g., \"safeway\", \"mcdonald 's\", \"s&p global ratings\")\n",
    "   - **ORG**: Non-commercial organizations, government agencies, political parties, sports teams, unions. (e.g., \"democrat\", \"united stats census bureau\", \"real madrid\")\n",
    "   - **Facility**: Buildings, stadiums, airports, highways, public places. (e.g., \"village hall\", \"lanxess arena\")\n",
    "   - **OtherPER**: Persons who are not artists or politicians (e.g., athletes, scientists, soldiers, fictional characters, or general people). (e.g., \"zcrny\", \"peter bourne\")\n",
    "   - **O**: Tokens that are not part of a named entity (CRITICAL: This includes Books, Movies, Songs, Albums, and Products).\n",
    "4. **Tagging Scheme**:\n",
    "   - Use `B-<Category>` for the first token of an entity.\n",
    "   - Use `I-<Category>` for all subsequent tokens of the same entity.\n",
    "   - Use `O` for non-entities.\n",
    "\n",
    "### Verification Strategy (CRITICAL)\n",
    "**If you are unsure about a proper noun, you MUST SEARCH THE INTERNET.**\n",
    "* **Ambiguity**: If a word looks like a name (e.g., \"finisterre\", \"wclv\", \"zcrny\") but you don't know it, pause and search for it.\n",
    "* **Distinction Logic**:\n",
    "    * If search shows it is a **Book, Movie, Album, or Product** -> Tag as **O**. (We do not have tags for these in this specific task).\n",
    "    * If search shows it is a **Company** -> Check if it is commercial (**PublicCorp**) or non-profit/sports (**ORG**).\n",
    "    * If search shows it is a **Person** -> Check if they are a politician (**Politician**), creator (**Artist**), or other (**OtherPER**).\n",
    "\n",
    "### Constraints\n",
    "1. **Length Consistency**: The output list MUST have exactly the same number of items as the input list.\n",
    "2. **Format**: Output ONLY a raw JSON list of strings. Do not include markdown formatting, explanations, or notes.\n",
    "3. **Robustness**: Treat lower-cased proper nouns as entities (e.g., \"paris\" -> B-HumanSettlement). Context is king.\n",
    "\n",
    "### Examples\n",
    "**Input**:\n",
    "[\"frank\", \"d.\", \"o'connor\", \"(\", \"1909\", \"–\", \"1992\", \")\", \"lawyer\", \"judge\", \"and\", \"politician\", \"−\", \"head\", \"trauma\"]\n",
    "**Output**:\n",
    "[\"B-Politician\", \"I-Politician\", \"I-Politician\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"]\n",
    "\n",
    "**Input**:\n",
    "[\"prior\", \"to\", \"the\", \"stabbings\", \"he\", \"was\", \"an\", \"employee\", \"of\", \"safeway\", \".\"]\n",
    "**Output**:\n",
    "[\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-PublicCorp\", \"O\"]\n",
    "\n",
    "**Input**:\n",
    "[\"its\", \"current\", \"representative\", \"is\", \"democrat\", \"bruce\", \"antone\", \".\"]\n",
    "**Output**:\n",
    "[\"O\", \"O\", \"O\", \"O\", \"B-ORG\", \"B-Politician\", \"I-Politician\", \"O\"]\n",
    "\n",
    "**Input**:\n",
    "[\"finisterre\", \"a\", \"1943\", \"poetry\", \"collection\", \"by\", \"eugenio\", \"montale\"]\n",
    "**Output**:\n",
    "[\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-Artist\", \"I-Artist\"]\n",
    "*(Note: Search reveals 'Finisterre' is a book, so it is tagged 'O')*\n",
    "\n",
    "---\n",
    "\n",
    "### Batch Processing Instructions\n",
    "I will provide multiple examples as a JSON object with numbered keys (\"0\", \"1\", \"2\", etc.).\n",
    "Each value is a list of tokens to tag.\n",
    "\n",
    "You must return a JSON object with the SAME keys, where each value is a list of BIO tags corresponding to the input tokens.\n",
    "\n",
    "**Example Batch Input**:\n",
    "{\n",
    "  \"0\": [\"frank\", \"d.\", \"o'connor\"],\n",
    "  \"1\": [\"safeway\", \"is\", \"big\"],\n",
    "  \"2\": [\"he\", \"lives\", \"in\", \"paris\"]\n",
    "}\n",
    "\n",
    "**Example Batch Output**:\n",
    "{\n",
    "  \"0\": [\"B-Politician\", \"I-Politician\", \"I-Politician\"],\n",
    "  \"1\": [\"B-PublicCorp\", \"O\", \"O\"],\n",
    "  \"2\": [\"O\", \"O\", \"O\", \"B-HumanSettlement\"]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "print(\"✓ Full system prompt defined\")\n",
    "print(f\"  Prompt length: {len(FULL_SYSTEM_PROMPT)} characters (~2,500 tokens)\")\n",
    "print(f\"  Includes: System instructions, verification strategy, 4 examples + batch instructions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Batch Prediction Function (10x Faster!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_predictions(model, system_prompt, batch_tokens_list, delay_seconds=2.0):\n",
    "    \"\"\"\n",
    "    Process multiple examples in a single API call.\n",
    "    \n",
    "    Args:\n",
    "        model: The initialized Gemini model\n",
    "        system_prompt: The system prompt with instructions\n",
    "        batch_tokens_list: List of token lists, e.g.:\n",
    "            [[\"frank\", \"d.\", \"o'connor\"], [\"he\", \"is\", \"from\", \"busan\"], ...]\n",
    "        delay_seconds: Delay before API call (default 2.0 for batching)\n",
    "    \n",
    "    Returns:\n",
    "        List of predicted tag lists, or error strings for failed examples\n",
    "    \"\"\"\n",
    "    # Add delay to avoid rate limiting\n",
    "    time.sleep(delay_seconds)\n",
    "    \n",
    "    # Format batch as numbered JSON object\n",
    "    batch_input = {\n",
    "        str(i): tokens \n",
    "        for i, tokens in enumerate(batch_tokens_list)\n",
    "    }\n",
    "    \n",
    "    # Construct prompt\n",
    "    final_prompt = f\"\"\"{system_prompt}\n",
    "\n",
    "Process these {len(batch_tokens_list)} examples:\n",
    "\n",
    "Input:\n",
    "{json.dumps(batch_input, indent=2)}\n",
    "\n",
    "Output (JSON object with same keys):\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Call Gemini API\n",
    "        response = model.generate_content(final_prompt)\n",
    "        raw_response = response.text.strip()\n",
    "        \n",
    "        # Parse JSON (should be clean with structured output)\n",
    "        predictions_dict = json.loads(raw_response)\n",
    "        \n",
    "        # Extract predictions in order\n",
    "        results = []\n",
    "        for i in range(len(batch_tokens_list)):\n",
    "            key = str(i)\n",
    "            if key in predictions_dict:\n",
    "                pred = predictions_dict[key]\n",
    "                if isinstance(pred, list) and all(isinstance(tag, str) for tag in pred):\n",
    "                    results.append(pred)\n",
    "                else:\n",
    "                    results.append(f\"Error: Invalid format for example {i}\")\n",
    "            else:\n",
    "                results.append(f\"Error: Missing key {i} in response\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        # If batch fails, return errors for all examples\n",
    "        error_msg = f\"Error: JSON parsing failed: {e}\"\n",
    "        return [error_msg] * len(batch_tokens_list)\n",
    "    \n",
    "    except Exception as e:\n",
    "        # If API call fails, return errors for all examples\n",
    "        error_msg = f\"Error: API call failed: {e}\"\n",
    "        return [error_msg] * len(batch_tokens_list)\n",
    "\n",
    "print(\"✓ Batch prediction function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Helper Function to Process Dataset in Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset_in_batches(model, prompt, tokens_list, batch_size=10, delay=2.0):\n",
    "    \"\"\"\n",
    "    Process entire dataset in batches with progress bar.\n",
    "    \n",
    "    Args:\n",
    "        model: Gemini model\n",
    "        prompt: System prompt\n",
    "        tokens_list: List of all token sequences\n",
    "        batch_size: Number of examples per API call (default 10)\n",
    "        delay: Delay between API calls in seconds\n",
    "    \n",
    "    Returns:\n",
    "        List of predictions for all examples\n",
    "    \"\"\"\n",
    "    all_predictions = []\n",
    "    total_batches = (len(tokens_list) + batch_size - 1) // batch_size\n",
    "    \n",
    "    print(f\"Processing {len(tokens_list)} examples in {total_batches} batches (batch_size={batch_size})\")\n",
    "    print(f\"Estimated time: ~{total_batches * delay / 60:.1f} minutes\\n\")\n",
    "    \n",
    "    # Process in batches with progress bar\n",
    "    for i in tqdm(range(0, len(tokens_list), batch_size), desc=\"Processing batches\"):\n",
    "        batch = tokens_list[i:i+batch_size]\n",
    "        batch_predictions = get_batch_predictions(model, prompt, batch, delay)\n",
    "        all_predictions.extend(batch_predictions)\n",
    "    \n",
    "    # Count errors\n",
    "    errors = sum(1 for pred in all_predictions if isinstance(pred, str) and pred.startswith(\"Error\"))\n",
    "    valid = len(all_predictions) - errors\n",
    "    \n",
    "    print(f\"\\n✓ Complete! Valid: {valid}, Errors: {errors}\")\n",
    "    \n",
    "    return all_predictions\n",
    "\n",
    "print(\"✓ Batch processing function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation split\n",
    "try:\n",
    "    val_data = pd.read_json('val_split.jsonl', lines=True)\n",
    "    print(f\"✓ Loaded val_split.jsonl: {len(val_data)} examples\")\n",
    "    print(f\"  Columns: {list(val_data.columns)}\")\n",
    "    display(val_data.head(3))\n",
    "except FileNotFoundError:\n",
    "    print(\"✗ Error: val_split.jsonl not found\")\n",
    "    val_data = pd.DataFrame()\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading val_split.jsonl: {e}\")\n",
    "    val_data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "try:\n",
    "    test_data = pd.read_json('test_data.jsonl', lines=True)\n",
    "    print(f\"✓ Loaded test_data.jsonl: {len(test_data)} examples\")\n",
    "    print(f\"  Columns: {list(test_data.columns)}\")\n",
    "    display(test_data.head(3))\n",
    "except FileNotFoundError:\n",
    "    print(\"✗ Error: test_data.jsonl not found\")\n",
    "    test_data = pd.DataFrame()\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading test_data.jsonl: {e}\")\n",
    "    test_data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Predictions on Validation Set (Batched)\n",
    "\n",
    "**Configuration**:\n",
    "- Batch size: 10 examples per API call\n",
    "- Delay: 2 seconds between calls\n",
    "- Using full detailed prompt for robustness\n",
    "- Expected time: ~2 hours for 10,000 examples\n",
    "- Expected cost: ~$80-100\n",
    "\n",
    "**vs Original (single example)**:\n",
    "- Original: 16+ hours, $574\n",
    "- Optimized with batching: 2 hours, $90 (8x faster, 6x cheaper!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not val_data.empty and 'tokens' in val_data.columns:\n",
    "    # Configuration\n",
    "    BATCH_SIZE = 10  # Adjust if needed (5-20 recommended)\n",
    "    DELAY = 2.0      # Adjust if hitting rate limits\n",
    "    \n",
    "    print(\"Starting batch prediction on validation set...\")\n",
    "    print(f\"Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"Delay: {DELAY}s per batch\\n\")\n",
    "    \n",
    "    # Process in batches\n",
    "    val_data['predicted_tags'] = process_dataset_in_batches(\n",
    "        gemini_model,\n",
    "        FULL_SYSTEM_PROMPT,\n",
    "        val_data['tokens'].tolist(),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        delay=DELAY\n",
    "    )\n",
    "    \n",
    "    print(\"\\n✓ Predictions complete!\")\n",
    "    display(val_data[['id', 'tokens', 'ner_tags', 'predicted_tags']].head())\n",
    "else:\n",
    "    print(\"✗ Cannot generate predictions: val_data is empty or missing 'tokens' column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Validation Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not val_data.empty and 'predicted_tags' in val_data.columns:\n",
    "    output_file = 'val_split_predictions_batch_fullprompt.jsonl'\n",
    "    val_data.to_json(output_file, orient='records', lines=True)\n",
    "    print(f\"✓ Validation predictions saved to {output_file}\")\n",
    "    print(f\"  Total examples: {len(val_data)}\")\n",
    "    \n",
    "    # Count valid vs errors\n",
    "    valid = val_data['predicted_tags'].apply(lambda x: isinstance(x, list)).sum()\n",
    "    errors = len(val_data) - valid\n",
    "    print(f\"  Valid predictions: {valid}\")\n",
    "    print(f\"  Errors: {errors}\")\n",
    "else:\n",
    "    print(\"✗ No predictions to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import utils\n",
    "    print(\"✓ utils.py imported successfully\")\n",
    "    \n",
    "    # Load predictions\n",
    "    predictions_df = pd.read_json('val_split_predictions_batch_fullprompt.jsonl', lines=True)\n",
    "    \n",
    "    # Filter out error messages (keep only valid predictions)\n",
    "    valid_df = predictions_df[predictions_df['predicted_tags'].apply(lambda x: isinstance(x, list))]\n",
    "    \n",
    "    if len(valid_df) > 0:\n",
    "        ground_truth = valid_df['ner_tags'].tolist()\n",
    "        predicted = valid_df['predicted_tags'].tolist()\n",
    "        tokens = valid_df['tokens'].tolist()\n",
    "        \n",
    "        print(f\"\\nEvaluating {len(valid_df)} valid predictions...\")\n",
    "        print(f\"Skipped {len(predictions_df) - len(valid_df)} errors\\n\")\n",
    "        \n",
    "        # Evaluate\n",
    "        results = utils.evaluate_entity_spans(ground_truth, predicted, tokens)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"EVALUATION RESULTS - Gemini Few-Shot (Batched + Full Prompt)\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Precision: {results['precision']:.4f} ({results['precision']*100:.2f}%)\")\n",
    "        print(f\"Recall:    {results['recall']:.4f} ({results['recall']*100:.2f}%)\")\n",
    "        print(f\"F1 Score:  {results['f1']:.4f} ({results['f1']*100:.2f}%)\")\n",
    "        print(f\"\\nTrue Positives:  {results['true_positives']}\")\n",
    "        print(f\"False Positives: {results['false_positives']}\")\n",
    "        print(f\"False Negatives: {results['false_negatives']}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Detailed report\n",
    "        print(\"\\n\")\n",
    "        utils.print_evaluation_report(ground_truth, predicted, tokens,\n",
    "                                     model_name=\"Gemini 2.5 Flash (Batched + Full Prompt)\")\n",
    "    else:\n",
    "        print(\"✗ No valid predictions to evaluate\")\n",
    "        \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"✗ File not found: {e}\")\n",
    "    print(\"  Make sure predictions were saved successfully.\")\n",
    "except ImportError:\n",
    "    print(\"✗ Error: utils.py not found\")\n",
    "    print(\"  Make sure utils.py is uploaded to Colab.\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error during evaluation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate Predictions on Test Set (Batched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not test_data.empty and 'tokens' in test_data.columns:\n",
    "    # Configuration (same as validation)\n",
    "    BATCH_SIZE = 10\n",
    "    DELAY = 2.0\n",
    "    \n",
    "    print(\"Starting batch prediction on test set...\")\n",
    "    print(f\"Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"Delay: {DELAY}s per batch\\n\")\n",
    "    \n",
    "    # Process in batches\n",
    "    test_data['predicted_tags'] = process_dataset_in_batches(\n",
    "        gemini_model,\n",
    "        FULL_SYSTEM_PROMPT,\n",
    "        test_data['tokens'].tolist(),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        delay=DELAY\n",
    "    )\n",
    "    \n",
    "    print(\"\\n✓ Predictions complete!\")\n",
    "    display(test_data[['id', 'tokens', 'predicted_tags']].head())\n",
    "else:\n",
    "    print(\"✗ Cannot generate predictions: test_data is empty or missing 'tokens' column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not test_data.empty and 'predicted_tags' in test_data.columns:\n",
    "    output_file = 'test_data_predictions_batch_fullprompt.jsonl'\n",
    "    test_data.to_json(output_file, orient='records', lines=True)\n",
    "    print(f\"✓ Test predictions saved to {output_file}\")\n",
    "    print(f\"  Total examples: {len(test_data)}\")\n",
    "    \n",
    "    # Count valid vs errors\n",
    "    valid = test_data['predicted_tags'].apply(lambda x: isinstance(x, list)).sum()\n",
    "    errors = len(test_data) - valid\n",
    "    print(f\"  Valid predictions: {valid}\")\n",
    "    print(f\"  Errors: {errors}\")\n",
    "else:\n",
    "    print(\"✗ No predictions to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Configuration:\n",
    "✅ **Batch Processing**: 10 examples per API call  \n",
    "✅ **Structured JSON Output**: Eliminates parsing errors  \n",
    "✅ **Full Detailed Prompt**: Your original 2,500 token robust prompt  \n",
    "\n",
    "### Performance vs Original:\n",
    "| Metric | Original (Single) | This Version (Batched) | Improvement |\n",
    "|--------|-------------------|------------------------|-------------|\n",
    "| Time (10K examples) | 16.7 hours | ~2 hours | **8x faster** |\n",
    "| Cost (10K examples) | $574 | ~$90 | **6x cheaper** |\n",
    "| Reliability | Manual JSON parsing | Structured output | **More reliable** |\n",
    "| Prompt quality | Full detailed | Same full detailed | **Same robustness** |\n",
    "\n",
    "### Files Generated:\n",
    "- `val_split_predictions_batch_fullprompt.jsonl` - Validation with predictions\n",
    "- `test_data_predictions_batch_fullprompt.jsonl` - Test with predictions\n",
    "\n",
    "### Trade-offs vs Shortened Prompt:\n",
    "| Aspect | This (Full Prompt) | Shortened Prompt Version |\n",
    "|--------|-------------------|-------------------------|\n",
    "| Robustness | ✅ Better | ⚠️ Good |\n",
    "| Cost | ~$90 | ~$45 |\n",
    "| Time | ~2 hours | ~1.5 hours |\n",
    "| Accuracy | Likely +1-2% F1 | Baseline |\n",
    "\n",
    "### Next Steps:\n",
    "1. Review validation F1 score\n",
    "2. Compare with traditional models (M4 v2: 75.94%)\n",
    "3. If F1 ≥ 75%, consider submitting\n",
    "4. For best results: Train M8 (RoBERTa) on GPU → 85-88% F1\n",
    "\n",
    "### Expected Performance:\n",
    "- Few-shot with full prompt: **65-72% F1** (better than short prompt by ~2-3%)\n",
    "- Your M4 v2: 75.94% F1\n",
    "- Friends' BERT: 77-79% F1\n",
    "- M8/M9 on GPU: 85-88% F1 (recommended for competition)\n",
    "- State-of-the-art: 85%+ F1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
