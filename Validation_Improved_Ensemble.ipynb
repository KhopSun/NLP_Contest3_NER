{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved Hybrid Ensemble - Validation Set\n",
    "\n",
    "This notebook:\n",
    "1. Loads validation predictions from both models (yours and friend's)\n",
    "2. Implements improved ensemble strategies (union, intersection, voting, hybrid)\n",
    "3. Evaluates each strategy on validation set\n",
    "4. Compares ensemble performance vs individual models\n",
    "5. Finds the best strategy per entity type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Imports loaded\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from typing import List, Set, Tuple\n",
    "import utils\n",
    "\n",
    "print(\"‚úì Imports loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Validation Ground Truth and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10036 ground truth examples\n"
     ]
    }
   ],
   "source": [
    "# Load ground truth from validation set\n",
    "ground_truth = {}\n",
    "tokens_dict = {}\n",
    "\n",
    "with open('val_split.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        ex = json.loads(line)\n",
    "        ground_truth[ex['id']] = ex['ner_tags']\n",
    "        tokens_dict[ex['id']] = ex['tokens']\n",
    "\n",
    "print(f\"Loaded {len(ground_truth)} ground truth examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10036 predictions from your model (KB-NER)\n",
      "Loaded 10036 predictions from friend's model (RoBERTa)\n"
     ]
    }
   ],
   "source": [
    "# Load your model predictions (KB-NER)\n",
    "your_predictions = {}\n",
    "with open('val_predictions_xlmr_crf.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        ex = json.loads(line)\n",
    "        your_predictions[ex['id']] = ex['ner_tags']\n",
    "\n",
    "print(f\"Loaded {len(your_predictions)} predictions from your model (KB-NER)\")\n",
    "\n",
    "# Load friend's model predictions (RoBERTa)\n",
    "friend_predictions = {}\n",
    "with open('val_data_roberta_predictions.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        ex = json.loads(line)\n",
    "        friend_predictions[ex['id']] = ex['ner_tags']\n",
    "\n",
    "print(f\"Loaded {len(friend_predictions)} predictions from friend's model (RoBERTa)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aligned examples across all datasets: 10036\n"
     ]
    }
   ],
   "source": [
    "# Get aligned IDs (examples present in all three)\n",
    "aligned_ids = sorted(set(ground_truth.keys()) & set(your_predictions.keys()) & set(friend_predictions.keys()))\n",
    "\n",
    "print(f\"\\nAligned examples across all datasets: {len(aligned_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Individual Models on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Model (KB-NER) on Validation:\n",
      "  Precision: 0.8249\n",
      "  Recall:    0.8275\n",
      "  F1:        0.8262\n"
     ]
    }
   ],
   "source": [
    "# Evaluate your model\n",
    "your_gt = [ground_truth[id] for id in aligned_ids]\n",
    "your_pred = [your_predictions[id] for id in aligned_ids]\n",
    "your_tokens = [tokens_dict[id] for id in aligned_ids]\n",
    "\n",
    "your_results = utils.evaluate_entity_spans(your_gt, your_pred, your_tokens)\n",
    "\n",
    "print(\"Your Model (KB-NER) on Validation:\")\n",
    "print(f\"  Precision: {your_results['precision']:.4f}\")\n",
    "print(f\"  Recall:    {your_results['recall']:.4f}\")\n",
    "print(f\"  F1:        {your_results['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Friend's Model (RoBERTa) on Validation:\n",
      "  Precision: 0.7984\n",
      "  Recall:    0.8056\n",
      "  F1:        0.8020\n"
     ]
    }
   ],
   "source": [
    "# Evaluate friend's model\n",
    "friend_gt = [ground_truth[id] for id in aligned_ids]\n",
    "friend_pred = [friend_predictions[id] for id in aligned_ids]\n",
    "friend_tokens = [tokens_dict[id] for id in aligned_ids]\n",
    "\n",
    "friend_results = utils.evaluate_entity_spans(friend_gt, friend_pred, friend_tokens)\n",
    "\n",
    "print(\"\\nFriend's Model (RoBERTa) on Validation:\")\n",
    "print(f\"  Precision: {friend_results['precision']:.4f}\")\n",
    "print(f\"  Recall:    {friend_results['recall']:.4f}\")\n",
    "print(f\"  F1:        {friend_results['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Entity Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity types: ['Artist', 'Facility', 'HumanSettlement', 'ORG', 'OtherPER', 'Politician', 'PublicCorp']\n",
      "Total: 7 types\n"
     ]
    }
   ],
   "source": [
    "# Extract all entity types from ground truth\n",
    "entity_types = set()\n",
    "for tags in ground_truth.values():\n",
    "    for tag in tags:\n",
    "        if tag != 'O' and tag.startswith('B-'):\n",
    "            entity_types.add(tag[2:])\n",
    "\n",
    "entity_types = sorted(entity_types)\n",
    "print(f\"Entity types: {entity_types}\")\n",
    "print(f\"Total: {len(entity_types)} types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Ensemble Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Ensemble strategies defined\n"
     ]
    }
   ],
   "source": [
    "def union_ensemble(your_tags, friend_tags, tokens):\n",
    "    \"\"\"Take ALL entities from both models - maximizes recall\"\"\"\n",
    "    your_entities = utils.extract_entities(tokens, your_tags)\n",
    "    friend_entities = utils.extract_entities(tokens, friend_tags)\n",
    "    \n",
    "    # Combine all entities (using set to avoid duplicates)\n",
    "    all_entities = set()\n",
    "    for entity in your_entities:\n",
    "        # entity = (text, type, start, end)\n",
    "        all_entities.add((entity[2], entity[3], entity[1]))  # (start, end, type)\n",
    "    for entity in friend_entities:\n",
    "        all_entities.add((entity[2], entity[3], entity[1]))\n",
    "    \n",
    "    # Convert back to BIO tags\n",
    "    ensemble_tags = ['O'] * len(tokens)\n",
    "    for start, end, entity_type in all_entities:\n",
    "        ensemble_tags[start] = f'B-{entity_type}'\n",
    "        for i in range(start + 1, end + 1):\n",
    "            ensemble_tags[i] = f'I-{entity_type}'\n",
    "    \n",
    "    return ensemble_tags\n",
    "\n",
    "\n",
    "def intersection_ensemble(your_tags, friend_tags, tokens):\n",
    "    \"\"\"Only take entities BOTH models agree on - maximizes precision\"\"\"\n",
    "    your_entities = utils.extract_entities(tokens, your_tags)\n",
    "    friend_entities = utils.extract_entities(tokens, friend_tags)\n",
    "    \n",
    "    your_set = set((e[2], e[3], e[1]) for e in your_entities)\n",
    "    friend_set = set((e[2], e[3], e[1]) for e in friend_entities)\n",
    "    \n",
    "    # Only keep entities both agree on\n",
    "    agreed_entities = your_set & friend_set\n",
    "    \n",
    "    # Convert back to BIO tags\n",
    "    ensemble_tags = ['O'] * len(tokens)\n",
    "    for start, end, entity_type in agreed_entities:\n",
    "        ensemble_tags[start] = f'B-{entity_type}'\n",
    "        for i in range(start + 1, end + 1):\n",
    "            ensemble_tags[i] = f'I-{entity_type}'\n",
    "    \n",
    "    return ensemble_tags\n",
    "\n",
    "\n",
    "def voting_ensemble(your_tags, friend_tags, tokens):\n",
    "    \"\"\"Token-level majority voting\"\"\"\n",
    "    ensemble_tags = []\n",
    "    \n",
    "    for y_tag, f_tag in zip(your_tags, friend_tags):\n",
    "        if y_tag == f_tag:\n",
    "            ensemble_tags.append(y_tag)\n",
    "        elif y_tag == 'O':\n",
    "            ensemble_tags.append(f_tag)\n",
    "        elif f_tag == 'O':\n",
    "            ensemble_tags.append(y_tag)\n",
    "        else:\n",
    "            # Both predict entity but disagree on type - take your model's prediction\n",
    "            ensemble_tags.append(y_tag)\n",
    "    \n",
    "    return ensemble_tags\n",
    "\n",
    "\n",
    "def hybrid_ensemble(your_tags, friend_tags, tokens, strategy_map):\n",
    "    \"\"\"\n",
    "    Use different ensemble strategy for each entity type.\n",
    "    strategy_map: dict mapping entity_type -> strategy ('union', 'intersection', 'your', 'friend')\n",
    "    \"\"\"\n",
    "    your_entities = utils.extract_entities(tokens, your_tags)\n",
    "    friend_entities = utils.extract_entities(tokens, friend_tags)\n",
    "    \n",
    "    selected_entities = set()\n",
    "    \n",
    "    # Process each entity type with its best strategy\n",
    "    for entity_type in entity_types:\n",
    "        your_type_entities = set((e[2], e[3], e[1]) for e in your_entities if e[1] == entity_type)\n",
    "        friend_type_entities = set((e[2], e[3], e[1]) for e in friend_entities if e[1] == entity_type)\n",
    "        \n",
    "        strategy = strategy_map.get(entity_type, 'union')\n",
    "        \n",
    "        if strategy == 'union':\n",
    "            selected_entities.update(your_type_entities | friend_type_entities)\n",
    "        elif strategy == 'intersection':\n",
    "            selected_entities.update(your_type_entities & friend_type_entities)\n",
    "        elif strategy == 'your':\n",
    "            selected_entities.update(your_type_entities)\n",
    "        elif strategy == 'friend':\n",
    "            selected_entities.update(friend_type_entities)\n",
    "    \n",
    "    # Convert back to BIO tags\n",
    "    ensemble_tags = ['O'] * len(tokens)\n",
    "    for start, end, entity_type in selected_entities:\n",
    "        ensemble_tags[start] = f'B-{entity_type}'\n",
    "        for i in range(start + 1, end + 1):\n",
    "            ensemble_tags[i] = f'I-{entity_type}'\n",
    "    \n",
    "    return ensemble_tags\n",
    "\n",
    "print(\"‚úì Ensemble strategies defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test All Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing UNION ensemble...\n",
      "  F1: 0.8074\n"
     ]
    }
   ],
   "source": [
    "# Test union strategy\n",
    "print(\"Testing UNION ensemble...\")\n",
    "union_predictions = []\n",
    "for id in aligned_ids:\n",
    "    ensemble_tags = union_ensemble(\n",
    "        your_predictions[id],\n",
    "        friend_predictions[id],\n",
    "        tokens_dict[id]\n",
    "    )\n",
    "    union_predictions.append(ensemble_tags)\n",
    "\n",
    "union_results = utils.evaluate_entity_spans(your_gt, union_predictions, your_tokens)\n",
    "print(f\"  F1: {union_results['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing INTERSECTION ensemble...\n",
      "  F1: 0.8172\n"
     ]
    }
   ],
   "source": [
    "# Test intersection strategy\n",
    "print(\"\\nTesting INTERSECTION ensemble...\")\n",
    "intersection_predictions = []\n",
    "for id in aligned_ids:\n",
    "    ensemble_tags = intersection_ensemble(\n",
    "        your_predictions[id],\n",
    "        friend_predictions[id],\n",
    "        tokens_dict[id]\n",
    "    )\n",
    "    intersection_predictions.append(ensemble_tags)\n",
    "\n",
    "intersection_results = utils.evaluate_entity_spans(your_gt, intersection_predictions, your_tokens)\n",
    "print(f\"  F1: {intersection_results['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing VOTING ensemble...\n",
      "  F1: 0.8150\n"
     ]
    }
   ],
   "source": [
    "# Test voting strategy\n",
    "print(\"\\nTesting VOTING ensemble...\")\n",
    "voting_predictions = []\n",
    "for id in aligned_ids:\n",
    "    ensemble_tags = voting_ensemble(\n",
    "        your_predictions[id],\n",
    "        friend_predictions[id],\n",
    "        tokens_dict[id]\n",
    "    )\n",
    "    voting_predictions.append(ensemble_tags)\n",
    "\n",
    "voting_results = utils.evaluate_entity_spans(your_gt, voting_predictions, your_tokens)\n",
    "print(f\"  F1: {voting_results['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Best Strategy Per Entity Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finding best strategy for each entity type...\n",
      "\n",
      "Testing strategies for Artist...\n",
      "  union        -> F1: 0.8201\n",
      "  intersection -> F1: 0.8214\n",
      "  your         -> F1: 0.8268\n",
      "  friend       -> F1: 0.8166\n",
      "  ‚úì Best: your (F1: 0.8268)\n",
      "\n",
      "Testing strategies for Facility...\n",
      "  union        -> F1: 0.8140\n",
      "  intersection -> F1: 0.8245\n",
      "  your         -> F1: 0.8440\n",
      "  friend       -> F1: 0.8003\n",
      "  ‚úì Best: your (F1: 0.8440)\n",
      "\n",
      "Testing strategies for HumanSettlement...\n",
      "  union        -> F1: 0.9399\n",
      "  intersection -> F1: 0.9505\n",
      "  your         -> F1: 0.9539\n",
      "  friend       -> F1: 0.9381\n",
      "  ‚úì Best: your (F1: 0.9539)\n",
      "\n",
      "Testing strategies for ORG...\n",
      "  union        -> F1: 0.7824\n",
      "  intersection -> F1: 0.8096\n",
      "  your         -> F1: 0.8166\n",
      "  friend       -> F1: 0.7763\n",
      "  ‚úì Best: your (F1: 0.8166)\n",
      "\n",
      "Testing strategies for OtherPER...\n",
      "  union        -> F1: 0.6413\n",
      "  intersection -> F1: 0.6439\n",
      "  your         -> F1: 0.6500\n",
      "  friend       -> F1: 0.6349\n",
      "  ‚úì Best: your (F1: 0.6500)\n",
      "\n",
      "Testing strategies for Politician...\n",
      "  union        -> F1: 0.6998\n",
      "  intersection -> F1: 0.6917\n",
      "  your         -> F1: 0.7020\n",
      "  friend       -> F1: 0.6936\n",
      "  ‚úì Best: your (F1: 0.7020)\n",
      "\n",
      "Testing strategies for PublicCorp...\n",
      "  union        -> F1: 0.7433\n",
      "  intersection -> F1: 0.7414\n",
      "  your         -> F1: 0.7407\n",
      "  friend       -> F1: 0.7361\n",
      "  ‚úì Best: union (F1: 0.7433)\n",
      "\n",
      "\n",
      "============================================================\n",
      "BEST STRATEGY PER ENTITY TYPE\n",
      "============================================================\n",
      "  Artist               -> your\n",
      "  Facility             -> your\n",
      "  HumanSettlement      -> your\n",
      "  ORG                  -> your\n",
      "  OtherPER             -> your\n",
      "  Politician           -> your\n",
      "  PublicCorp           -> union\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFinding best strategy for each entity type...\\n\")\n",
    "\n",
    "best_strategy_per_type = {}\n",
    "\n",
    "for entity_type in entity_types:\n",
    "    print(f\"Testing strategies for {entity_type}...\")\n",
    "    \n",
    "    type_results = {}\n",
    "    \n",
    "    # Test each strategy for this entity type\n",
    "    for strategy_name in ['union', 'intersection', 'your', 'friend']:\n",
    "        # Create temporary strategy map with all types using this strategy\n",
    "        temp_strategy_map = {entity_type: strategy_name}\n",
    "        # For other types, use union as default\n",
    "        for other_type in entity_types:\n",
    "            if other_type != entity_type:\n",
    "                temp_strategy_map[other_type] = 'union'\n",
    "        \n",
    "        # Generate predictions\n",
    "        temp_predictions = []\n",
    "        for id in aligned_ids:\n",
    "            ensemble_tags = hybrid_ensemble(\n",
    "                your_predictions[id],\n",
    "                friend_predictions[id],\n",
    "                tokens_dict[id],\n",
    "                temp_strategy_map\n",
    "            )\n",
    "            temp_predictions.append(ensemble_tags)\n",
    "        \n",
    "        # Calculate F1 for this entity type only\n",
    "        tp = fp = fn = 0\n",
    "        for gt_tags, pred_tags, tokens in zip(your_gt, temp_predictions, your_tokens):\n",
    "            gt_spans = utils.get_entity_spans(tokens, gt_tags)\n",
    "            pred_spans = utils.get_entity_spans(tokens, pred_tags)\n",
    "            \n",
    "            # Filter for current entity type\n",
    "            gt_type_spans = {(s, e, t) for s, e, t in gt_spans if t == entity_type}\n",
    "            pred_type_spans = {(s, e, t) for s, e, t in pred_spans if t == entity_type}\n",
    "            \n",
    "            tp += len(gt_type_spans & pred_type_spans)\n",
    "            fp += len(pred_type_spans - gt_type_spans)\n",
    "            fn += len(gt_type_spans - pred_type_spans)\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        \n",
    "        type_results[strategy_name] = f1\n",
    "        print(f\"  {strategy_name:12} -> F1: {f1:.4f}\")\n",
    "    \n",
    "    # Pick best strategy for this type\n",
    "    best = max(type_results.items(), key=lambda x: x[1])\n",
    "    best_strategy_per_type[entity_type] = best[0]\n",
    "    print(f\"  ‚úì Best: {best[0]} (F1: {best[1]:.4f})\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BEST STRATEGY PER ENTITY TYPE\")\n",
    "print(\"=\"*60)\n",
    "for entity_type, strategy in sorted(best_strategy_per_type.items()):\n",
    "    print(f\"  {entity_type:20} -> {strategy}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Hybrid Ensemble Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating HYBRID ensemble predictions...\n",
      "  F1: 0.8260\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nGenerating HYBRID ensemble predictions...\")\n",
    "\n",
    "hybrid_predictions = []\n",
    "for id in aligned_ids:\n",
    "    ensemble_tags = hybrid_ensemble(\n",
    "        your_predictions[id],\n",
    "        friend_predictions[id],\n",
    "        tokens_dict[id],\n",
    "        best_strategy_per_type\n",
    "    )\n",
    "    hybrid_predictions.append(ensemble_tags)\n",
    "\n",
    "hybrid_results = utils.evaluate_entity_spans(your_gt, hybrid_predictions, your_tokens)\n",
    "print(f\"  F1: {hybrid_results['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare All Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "VALIDATION SET PERFORMANCE COMPARISON\n",
      "================================================================================\n",
      "Method                            Precision       Recall           F1\n",
      "--------------------------------------------------------------------------------\n",
      "üèÜ Your Model (KB-NER)               0.8249       0.8275       0.8262\n",
      "2. Hybrid Ensemble                   0.8242       0.8278       0.8260\n",
      "3. Intersection Ensemble             0.8990       0.7490       0.8172\n",
      "4. Voting Ensemble                   0.8024       0.8281       0.8150\n",
      "5. Union Ensemble                    0.7939       0.8214       0.8074\n",
      "6. Friend Model (RoBERTa)            0.7984       0.8056       0.8020\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VALIDATION SET PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Method':<30} {'Precision':>12} {'Recall':>12} {'F1':>12}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "all_results = [\n",
    "    ('Your Model (KB-NER)', your_results),\n",
    "    ('Friend Model (RoBERTa)', friend_results),\n",
    "    ('Union Ensemble', union_results),\n",
    "    ('Intersection Ensemble', intersection_results),\n",
    "    ('Voting Ensemble', voting_results),\n",
    "    ('Hybrid Ensemble', hybrid_results)\n",
    "]\n",
    "\n",
    "# Sort by F1\n",
    "all_results.sort(key=lambda x: x[1]['f1'], reverse=True)\n",
    "\n",
    "for i, (name, results) in enumerate(all_results, 1):\n",
    "    marker = \"üèÜ\" if i == 1 else f\"{i}.\"\n",
    "    print(f\"{marker} {name:<27} {results['precision']:>12.4f} {results['recall']:>12.4f} {results['f1']:>12.4f}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Best Ensemble Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ Best Ensemble Strategy: HYBRID\n",
      "   F1 Score: 0.8260\n",
      "\n",
      "‚úì Saved best ensemble predictions to val_predictions_ensemble_improved.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Determine which ensemble performed best\n",
    "ensemble_scores = {\n",
    "    'union': union_results['f1'],\n",
    "    'intersection': intersection_results['f1'],\n",
    "    'voting': voting_results['f1'],\n",
    "    'hybrid': hybrid_results['f1']\n",
    "}\n",
    "\n",
    "best_ensemble = max(ensemble_scores.items(), key=lambda x: x[1])\n",
    "\n",
    "print(f\"\\nüèÜ Best Ensemble Strategy: {best_ensemble[0].upper()}\")\n",
    "print(f\"   F1 Score: {best_ensemble[1]:.4f}\")\n",
    "\n",
    "# Save the best ensemble predictions\n",
    "if best_ensemble[0] == 'union':\n",
    "    best_predictions = union_predictions\n",
    "elif best_ensemble[0] == 'intersection':\n",
    "    best_predictions = intersection_predictions\n",
    "elif best_ensemble[0] == 'voting':\n",
    "    best_predictions = voting_predictions\n",
    "else:\n",
    "    best_predictions = hybrid_predictions\n",
    "\n",
    "output_file = 'val_predictions_ensemble_improved.jsonl'\n",
    "with open(output_file, 'w') as f:\n",
    "    for id, pred_tags in zip(aligned_ids, best_predictions):\n",
    "        output = {\n",
    "            'id': id,\n",
    "            'tokens': tokens_dict[id],\n",
    "            'ner_tags': pred_tags\n",
    "        }\n",
    "        f.write(json.dumps(output) + '\\n')\n",
    "\n",
    "print(f\"\\n‚úì Saved best ensemble predictions to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Strategy Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Saved strategy configuration to val_ensemble_strategy.json\n"
     ]
    }
   ],
   "source": [
    "# Save the strategy configuration for future reference\n",
    "strategy_config = {\n",
    "    'best_overall_ensemble': best_ensemble[0],\n",
    "    'best_strategy_per_type': best_strategy_per_type,\n",
    "    'results': {\n",
    "        'your_model': {\n",
    "            'precision': your_results['precision'],\n",
    "            'recall': your_results['recall'],\n",
    "            'f1': your_results['f1']\n",
    "        },\n",
    "        'friend_model': {\n",
    "            'precision': friend_results['precision'],\n",
    "            'recall': friend_results['recall'],\n",
    "            'f1': friend_results['f1']\n",
    "        },\n",
    "        'union': {\n",
    "            'precision': union_results['precision'],\n",
    "            'recall': union_results['recall'],\n",
    "            'f1': union_results['f1']\n",
    "        },\n",
    "        'intersection': {\n",
    "            'precision': intersection_results['precision'],\n",
    "            'recall': intersection_results['recall'],\n",
    "            'f1': intersection_results['f1']\n",
    "        },\n",
    "        'voting': {\n",
    "            'precision': voting_results['precision'],\n",
    "            'recall': voting_results['recall'],\n",
    "            'f1': voting_results['f1']\n",
    "        },\n",
    "        'hybrid': {\n",
    "            'precision': hybrid_results['precision'],\n",
    "            'recall': hybrid_results['recall'],\n",
    "            'f1': hybrid_results['f1']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('val_ensemble_strategy.json', 'w') as f:\n",
    "    json.dump(strategy_config, f, indent=2)\n",
    "\n",
    "print(\"\\n‚úì Saved strategy configuration to val_ensemble_strategy.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "ü§ù ENSEMBLE AND INDIVIDUAL PERFORM SIMILARLY\n",
      "   Best Individual: Your Model (KB-NER) (F1: 0.8262)\n",
      "   Best Ensemble: HYBRID (F1: 0.8260)\n",
      "   Difference: -0.0002\n",
      "   Either approach is viable.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Compare best ensemble vs best individual\n",
    "best_individual_f1 = max(your_results['f1'], friend_results['f1'])\n",
    "best_individual_name = 'Your Model (KB-NER)' if your_results['f1'] > friend_results['f1'] else 'Friend Model (RoBERTa)'\n",
    "\n",
    "best_ensemble_f1 = best_ensemble[1]\n",
    "\n",
    "improvement = best_ensemble_f1 - best_individual_f1\n",
    "\n",
    "if improvement > 0.001:  # 0.1% improvement\n",
    "    print(f\"\\n‚úÖ USE ENSEMBLE APPROACH\")\n",
    "    print(f\"   Best Ensemble: {best_ensemble[0].upper()} (F1: {best_ensemble_f1:.4f})\")\n",
    "    print(f\"   Best Individual: {best_individual_name} (F1: {best_individual_f1:.4f})\")\n",
    "    print(f\"   Improvement: +{improvement:.4f} ({improvement*100:.2f}%)\")\n",
    "    print(f\"\\n   üìÅ Use predictions from: val_predictions_ensemble_improved.jsonl\")\n",
    "elif improvement < -0.001:\n",
    "    print(f\"\\n‚ö†Ô∏è  USE INDIVIDUAL MODEL\")\n",
    "    print(f\"   Best Individual: {best_individual_name} (F1: {best_individual_f1:.4f})\")\n",
    "    print(f\"   Best Ensemble: {best_ensemble[0].upper()} (F1: {best_ensemble_f1:.4f})\")\n",
    "    print(f\"   Ensemble performs worse by: {abs(improvement):.4f} ({abs(improvement)*100:.2f}%)\")\n",
    "else:\n",
    "    print(f\"\\nü§ù ENSEMBLE AND INDIVIDUAL PERFORM SIMILARLY\")\n",
    "    print(f\"   Best Individual: {best_individual_name} (F1: {best_individual_f1:.4f})\")\n",
    "    print(f\"   Best Ensemble: {best_ensemble[0].upper()} (F1: {best_ensemble_f1:.4f})\")\n",
    "    print(f\"   Difference: {improvement:.4f}\")\n",
    "    print(f\"   Either approach is viable.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
