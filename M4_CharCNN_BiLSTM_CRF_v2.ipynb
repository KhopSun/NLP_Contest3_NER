{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4 v2: Character-CNN + BiLSTM-CRF (Improved)\n",
    "\n",
    "**Improvements over v1:**\n",
    "\n",
    "This version includes several optimizations to improve performance beyond 0.74 F1:\n",
    "\n",
    "1. ‚úÖ **Higher Dropout (0.6)**: Reduces overfitting (was 0.5)\n",
    "2. ‚úÖ **Dropout on Embeddings**: Applied to word and character embeddings\n",
    "3. ‚úÖ **Learning Rate Scheduler**: ReduceLROnPlateau for adaptive LR decay\n",
    "4. ‚úÖ **Increased Patience (7)**: Allows more training before early stopping (was 3)\n",
    "5. ‚úÖ **GloVe 300d**: Richer word representations (was 100d)\n",
    "6. ‚úÖ **Gradient Clipping (1.0)**: More aggressive clipping for stability (was 5.0)\n",
    "\n",
    "**Expected Improvement:** 0.74 ‚Üí 0.78-0.80 F1 (+4-6% absolute)\n",
    "\n",
    "**Target F1:** 78-80% (excellent for 15 entity types)\n",
    "\n",
    "**Original v1 Results:**\n",
    "- Epoch 3: 0.7484 F1 (peak)\n",
    "- Early stopping at epoch 6\n",
    "- Issue: Overfitting after epoch 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter kernel Python: /usr/local/bin/python3\n",
      "\u001b[33mWARNING: Skipping torchcrf as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: pytorch-crf in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.7.2)\n",
      "Requirement already satisfied: gensim in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.4.0)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gensim) (2.1.1)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gensim) (1.14.1)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gensim) (7.3.0.post1)\n",
      "Requirement already satisfied: wrapt in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from smart_open>=1.8.1->gensim) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "‚úÖ Packages installed! Please RESTART THE KERNEL before continuing.\n",
      "   Kernel ‚Üí Restart Kernel (or Ctrl+Shift+P ‚Üí 'Restart Kernel')\n"
     ]
    }
   ],
   "source": [
    "# Install required packages using the notebook's Python interpreter\n",
    "import sys\n",
    "print(f\"Jupyter kernel Python: {sys.executable}\")\n",
    "\n",
    "# Install packages to the correct Python environment\n",
    "!{sys.executable} -m pip uninstall -y torchcrf\n",
    "!{sys.executable} -m pip install torch pytorch-crf gensim tqdm\n",
    "\n",
    "print(\"\\n‚úÖ Packages installed! Please RESTART THE KERNEL before continuing.\")\n",
    "print(\"   Kernel ‚Üí Restart Kernel (or Ctrl+Shift+P ‚Üí 'Restart Kernel')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Imports successful!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# CRF\n",
    "from torchcrf import CRF\n",
    "\n",
    "# Embeddings\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Our evaluation utilities\n",
    "from utils import print_evaluation_report, evaluate_entity_spans\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 90,320\n",
      "Validation samples: 10,036\n"
     ]
    }
   ],
   "source": [
    "def load_jsonl(file_path):\n",
    "    \"\"\"Load JSONL file\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.strip()))\n",
    "    return data\n",
    "\n",
    "# Load data\n",
    "train_data = load_jsonl('train_split.jsonl')\n",
    "val_data = load_jsonl('val_split.jsonl')\n",
    "\n",
    "print(f\"Training samples: {len(train_data):,}\")\n",
    "print(f\"Validation samples: {len(val_data):,}\")\n",
    "\n",
    "# Extract tokens and tags\n",
    "train_tokens = [sample['tokens'] for sample in train_data]\n",
    "train_tags = [sample['ner_tags'] for sample in train_data]\n",
    "\n",
    "val_tokens = [sample['tokens'] for sample in val_data]\n",
    "val_tags = [sample['ner_tags'] for sample in val_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vocabulary size: 36,790\n"
     ]
    }
   ],
   "source": [
    "# Build word vocabulary\n",
    "word_counts = Counter()\n",
    "for tokens in train_tokens:\n",
    "    word_counts.update(tokens)\n",
    "\n",
    "# Keep words with frequency >= 2\n",
    "MIN_WORD_FREQ = 2\n",
    "word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "for word, count in word_counts.items():\n",
    "    if count >= MIN_WORD_FREQ:\n",
    "        word2idx[word] = len(word2idx)\n",
    "\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "print(f\"Word vocabulary size: {vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character vocabulary size: 97\n"
     ]
    }
   ],
   "source": [
    "# Build character vocabulary\n",
    "char2idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "\n",
    "chars = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 .,!?:;\\'\"()-[]{}@#$%^&*+=/<>\\\\|`~_'\n",
    "for char in chars:\n",
    "    if char not in char2idx:\n",
    "        char2idx[char] = len(char2idx)\n",
    "\n",
    "idx2char = {idx: char for char, idx in char2idx.items()}\n",
    "char_vocab_size = len(char2idx)\n",
    "\n",
    "print(f\"Character vocabulary size: {char_vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NER tags: 15\n",
      "Tags: ['O', 'B-ORG', 'I-ORG', 'B-Facility', 'I-Facility', 'B-OtherPER', 'I-OtherPER', 'B-Politician', 'I-Politician', 'B-HumanSettlement', 'I-HumanSettlement', 'B-Artist', 'I-Artist', 'B-PublicCorp', 'I-PublicCorp']\n"
     ]
    }
   ],
   "source": [
    "# Build tag vocabulary\n",
    "tag2idx = {}\n",
    "for tags in train_tags:\n",
    "    for tag in tags:\n",
    "        if tag not in tag2idx:\n",
    "            tag2idx[tag] = len(tag2idx)\n",
    "\n",
    "idx2tag = {idx: tag for tag, idx in tag2idx.items()}\n",
    "num_tags = len(tag2idx)\n",
    "\n",
    "print(f\"Number of NER tags: {num_tags}\")\n",
    "print(f\"Tags: {list(tag2idx.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Pre-trained Word Embeddings (GloVe 300d)\n",
    "\n",
    "**IMPROVEMENT #5: Using GloVe 300d instead of 100d**\n",
    "- Richer semantic representations\n",
    "- Expected improvement: +1-2% F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading GloVe 300d embeddings (this may take ~5-10 minutes)...\n",
      "Using glove-wiki-gigaword-300 (300-dimensional, 400K vocabulary)\n",
      "[==================================================] 100.0% 376.1/376.1MB downloaded\n",
      "\n",
      "GloVe 300d embeddings loaded!\n",
      "Embedding dimension: 300\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloading GloVe 300d embeddings (this may take ~5-10 minutes)...\")\n",
    "print(\"Using glove-wiki-gigaword-300 (300-dimensional, 400K vocabulary)\")\n",
    "\n",
    "# Download GloVe 300d embeddings\n",
    "glove_model = api.load('glove-wiki-gigaword-300')\n",
    "\n",
    "EMBEDDING_DIM = 300  # Upgraded from 100d!\n",
    "print(f\"\\nGloVe 300d embeddings loaded!\")\n",
    "print(f\"Embedding dimension: {EMBEDDING_DIM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words found in GloVe: 33,849 / 36,790 (92.0%)\n",
      "Words initialized randomly: 2,941\n"
     ]
    }
   ],
   "source": [
    "# Create embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "\n",
    "# Initialize with GloVe vectors\n",
    "found = 0\n",
    "for word, idx in word2idx.items():\n",
    "    if word in ['<PAD>', '<UNK>']:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        embedding_matrix[idx] = glove_model[word.lower()]\n",
    "        found += 1\n",
    "    except KeyError:\n",
    "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(EMBEDDING_DIM,))\n",
    "\n",
    "# Special tokens\n",
    "embedding_matrix[word2idx['<PAD>']] = np.zeros(EMBEDDING_DIM)\n",
    "embedding_matrix[word2idx['<UNK>']] = np.random.normal(scale=0.6, size=(EMBEDDING_DIM,))\n",
    "\n",
    "print(f\"Words found in GloVe: {found:,} / {vocab_size:,} ({found/vocab_size*100:.1f}%)\")\n",
    "print(f\"Words initialized randomly: {vocab_size - found:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 90320 samples\n",
      "Val dataset: 10036 samples\n"
     ]
    }
   ],
   "source": [
    "MAX_CHAR_LEN = 20\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, tokens_list, tags_list, word2idx, char2idx, tag2idx):\n",
    "        self.tokens_list = tokens_list\n",
    "        self.tags_list = tags_list\n",
    "        self.word2idx = word2idx\n",
    "        self.char2idx = char2idx\n",
    "        self.tag2idx = tag2idx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokens_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokens_list[idx]\n",
    "        tags = self.tags_list[idx]\n",
    "        \n",
    "        word_ids = [self.word2idx.get(token, self.word2idx['<UNK>']) for token in tokens]\n",
    "        \n",
    "        char_ids = []\n",
    "        for token in tokens:\n",
    "            chars = [self.char2idx.get(c, self.char2idx['<UNK>']) for c in token[:MAX_CHAR_LEN]]\n",
    "            if len(chars) < MAX_CHAR_LEN:\n",
    "                chars += [self.char2idx['<PAD>']] * (MAX_CHAR_LEN - len(chars))\n",
    "            char_ids.append(chars)\n",
    "        \n",
    "        tag_ids = [self.tag2idx[tag] for tag in tags]\n",
    "        \n",
    "        return {\n",
    "            'word_ids': torch.LongTensor(word_ids),\n",
    "            'char_ids': torch.LongTensor(char_ids),\n",
    "            'tag_ids': torch.LongTensor(tag_ids),\n",
    "            'length': len(tokens)\n",
    "        }\n",
    "\n",
    "train_dataset = NERDataset(train_tokens, train_tags, word2idx, char2idx, tag2idx)\n",
    "val_dataset = NERDataset(val_tokens, val_tags, word2idx, char2idx, tag2idx)\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"Val dataset: {len(val_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 2823\n",
      "Val batches: 314\n"
     ]
    }
   ],
   "source": [
    "# Collate function\n",
    "def collate_fn(batch):\n",
    "    batch = sorted(batch, key=lambda x: x['length'], reverse=True)\n",
    "    \n",
    "    word_ids = [item['word_ids'] for item in batch]\n",
    "    char_ids = [item['char_ids'] for item in batch]\n",
    "    tag_ids = [item['tag_ids'] for item in batch]\n",
    "    lengths = [item['length'] for item in batch]\n",
    "    \n",
    "    word_ids_padded = pad_sequence(word_ids, batch_first=True, padding_value=word2idx['<PAD>'])\n",
    "    tag_ids_padded = pad_sequence(tag_ids, batch_first=True, padding_value=tag2idx['O'])\n",
    "    \n",
    "    max_len = max(1, word_ids_padded.size(1))\n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    char_ids_padded = torch.full(\n",
    "        (batch_size, max_len, MAX_CHAR_LEN),\n",
    "        fill_value=char2idx['<PAD>'],\n",
    "        dtype=torch.long\n",
    "    )\n",
    "    \n",
    "    for i, chars in enumerate(char_ids):\n",
    "        seq_len = chars.size(0)\n",
    "        if seq_len > 0:\n",
    "            char_ids_padded[i, :seq_len, :] = chars\n",
    "    \n",
    "    mask = torch.zeros((batch_size, max_len), dtype=torch.bool)\n",
    "    for i, length in enumerate(lengths):\n",
    "        if length > 0:\n",
    "            mask[i, :length] = True\n",
    "    \n",
    "    return {\n",
    "        'word_ids': word_ids_padded,\n",
    "        'char_ids': char_ids_padded,\n",
    "        'tag_ids': tag_ids_padded,\n",
    "        'lengths': lengths,\n",
    "        'mask': mask\n",
    "    }\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Architecture (Improved)\n",
    "\n",
    "**IMPROVEMENTS #1 & #2:**\n",
    "- Higher dropout (0.6 instead of 0.5)\n",
    "- Dropout applied to embeddings (not just LSTM output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Improved model architecture defined!\n",
      "   - Higher dropout: 0.6 (was 0.5)\n",
      "   - Dropout on embeddings: Added\n",
      "   - GloVe 300d: Enabled\n"
     ]
    }
   ],
   "source": [
    "class CharCNN(nn.Module):\n",
    "    def __init__(self, char_vocab_size, char_emb_dim, char_hidden_dim, max_char_len, dropout=0.6):\n",
    "        super().__init__()\n",
    "        self.char_embedding = nn.Embedding(char_vocab_size, char_emb_dim, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(dropout)  # IMPROVEMENT: Dropout on char embeddings\n",
    "        \n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=char_emb_dim,\n",
    "            out_channels=char_hidden_dim,\n",
    "            kernel_size=3,\n",
    "            padding=1\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, char_ids):\n",
    "        batch_size, seq_len, max_char_len = char_ids.size()\n",
    "        \n",
    "        char_ids = char_ids.view(-1, max_char_len)\n",
    "        char_embeds = self.char_embedding(char_ids)\n",
    "        char_embeds = self.dropout(char_embeds)  # Apply dropout\n",
    "        char_embeds = char_embeds.transpose(1, 2)\n",
    "        \n",
    "        char_conv = self.relu(self.conv(char_embeds))\n",
    "        char_features = torch.max(char_conv, dim=2)[0]\n",
    "        char_features = char_features.view(batch_size, seq_len, -1)\n",
    "        \n",
    "        return char_features\n",
    "\n",
    "\n",
    "class BiLSTM_CRF_v2(nn.Module):\n",
    "    \"\"\"Improved BiLSTM-CRF with higher dropout and embedding dropout\"\"\"\n",
    "    def __init__(self, vocab_size, char_vocab_size, embedding_dim, char_emb_dim,\n",
    "                 char_hidden_dim, lstm_hidden_dim, num_tags, dropout=0.6, embedding_matrix=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Word embeddings\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        if embedding_matrix is not None:\n",
    "            self.word_embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "            self.word_embedding.weight.requires_grad = True\n",
    "        \n",
    "        # IMPROVEMENT #2: Dropout on word embeddings\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Character CNN (with dropout)\n",
    "        self.char_cnn = CharCNN(char_vocab_size, char_emb_dim, char_hidden_dim, MAX_CHAR_LEN, dropout=dropout)\n",
    "        \n",
    "        # BiLSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim + char_hidden_dim,\n",
    "            hidden_size=lstm_hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # IMPROVEMENT #1: Higher dropout (0.6 instead of 0.5)\n",
    "        self.lstm_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Linear layer\n",
    "        self.fc = nn.Linear(lstm_hidden_dim * 2, num_tags)\n",
    "        \n",
    "        # CRF\n",
    "        self.crf = CRF(num_tags, batch_first=True)\n",
    "    \n",
    "    def forward(self, word_ids, char_ids, tags=None, mask=None):\n",
    "        # Word embeddings with dropout\n",
    "        word_embeds = self.word_embedding(word_ids)\n",
    "        word_embeds = self.embedding_dropout(word_embeds)\n",
    "        \n",
    "        # Character features (already has dropout inside)\n",
    "        char_features = self.char_cnn(char_ids)\n",
    "        \n",
    "        # Concatenate\n",
    "        combined = torch.cat([word_embeds, char_features], dim=-1)\n",
    "        \n",
    "        # BiLSTM with dropout\n",
    "        lstm_out, _ = self.lstm(combined)\n",
    "        lstm_out = self.lstm_dropout(lstm_out)\n",
    "        \n",
    "        # Emission scores\n",
    "        emissions = self.fc(lstm_out)\n",
    "        \n",
    "        if tags is not None:\n",
    "            loss = -self.crf(emissions, tags, mask=mask, reduction='mean')\n",
    "            return loss\n",
    "        else:\n",
    "            predictions = self.crf.decode(emissions, mask=mask)\n",
    "            return predictions\n",
    "\n",
    "print(\"‚úÖ Improved model architecture defined!\")\n",
    "print(\"   - Higher dropout: 0.6 (was 0.5)\")\n",
    "print(\"   - Dropout on embeddings: Added\")\n",
    "print(\"   - GloVe 300d: Enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Initialize Model with Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Model initialized with improvements!\n",
      "Total parameters: 12,253,879\n",
      "\n",
      "üéØ Improvements Summary:\n",
      "  1. Dropout: 0.6 (was 0.5)\n",
      "  2. Embedding dropout: Enabled\n",
      "  3. LR scheduler: ReduceLROnPlateau (patience=2)\n",
      "  4. Patience: 7 epochs (was 3)\n",
      "  5. GloVe: 300d (was 100d)\n",
      "  6. Grad clipping: 1.0 (was 5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "CHAR_EMB_DIM = 25\n",
    "CHAR_HIDDEN_DIM = 30\n",
    "LSTM_HIDDEN_DIM = 256\n",
    "DROPOUT = 0.6  # IMPROVEMENT #1: Increased from 0.5\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "# Initialize model\n",
    "model = BiLSTM_CRF_v2(\n",
    "    vocab_size=vocab_size,\n",
    "    char_vocab_size=char_vocab_size,\n",
    "    embedding_dim=EMBEDDING_DIM,  # 300d now!\n",
    "    char_emb_dim=CHAR_EMB_DIM,\n",
    "    char_hidden_dim=CHAR_HIDDEN_DIM,\n",
    "    lstm_hidden_dim=LSTM_HIDDEN_DIM,\n",
    "    num_tags=num_tags,\n",
    "    dropout=DROPOUT,\n",
    "    embedding_matrix=embedding_matrix\n",
    ").to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# IMPROVEMENT #3: Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.5, patience=2, verbose=True\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"\\n‚úÖ Model initialized with improvements!\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"\\nüéØ Improvements Summary:\")\n",
    "print(f\"  1. Dropout: {DROPOUT} (was 0.5)\")\n",
    "print(f\"  2. Embedding dropout: Enabled\")\n",
    "print(f\"  3. LR scheduler: ReduceLROnPlateau (patience=2)\")\n",
    "print(f\"  4. Patience: 7 epochs (was 3)\")\n",
    "print(f\"  5. GloVe: 300d (was 100d)\")\n",
    "print(f\"  6. Grad clipping: 1.0 (was 5.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Loop (Improved)\n",
    "\n",
    "**IMPROVEMENTS #3, #4, #6:**\n",
    "- Learning rate scheduler (adaptive LR)\n",
    "- Increased patience (7 epochs)\n",
    "- More aggressive gradient clipping (1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training functions defined!\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, data_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    pbar = tqdm(data_loader, desc=\"Training\", leave=False)\n",
    "    \n",
    "    for batch in pbar:\n",
    "        word_ids = batch['word_ids'].to(device)\n",
    "        char_ids = batch['char_ids'].to(device)\n",
    "        tag_ids = batch['tag_ids'].to(device)\n",
    "        mask = batch['mask'].to(device)\n",
    "        lengths = batch['lengths']\n",
    "        \n",
    "        # Filter empty sequences\n",
    "        non_empty_indices = [i for i, length in enumerate(lengths) if length > 0]\n",
    "        \n",
    "        if len(non_empty_indices) == 0:\n",
    "            continue\n",
    "        \n",
    "        if len(non_empty_indices) < len(lengths):\n",
    "            word_ids = word_ids[non_empty_indices]\n",
    "            char_ids = char_ids[non_empty_indices]\n",
    "            tag_ids = tag_ids[non_empty_indices]\n",
    "            mask = mask[non_empty_indices]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = model(word_ids, char_ids, tag_ids, mask)\n",
    "        loss.backward()\n",
    "        \n",
    "        # IMPROVEMENT #6: More aggressive gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Was 5.0\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return total_loss / max(num_batches, 1)\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_true_tags = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(data_loader, desc=\"Evaluating\", leave=False)\n",
    "        \n",
    "        for batch in pbar:\n",
    "            word_ids = batch['word_ids'].to(device)\n",
    "            char_ids = batch['char_ids'].to(device)\n",
    "            tag_ids = batch['tag_ids'].to(device)\n",
    "            mask = batch['mask'].to(device)\n",
    "            lengths = batch['lengths']\n",
    "            \n",
    "            non_empty_indices = [i for i, length in enumerate(lengths) if length > 0]\n",
    "            \n",
    "            if len(non_empty_indices) > 0:\n",
    "                word_ids_non_empty = word_ids[non_empty_indices]\n",
    "                char_ids_non_empty = char_ids[non_empty_indices]\n",
    "                mask_non_empty = mask[non_empty_indices]\n",
    "                \n",
    "                predictions_non_empty = model(word_ids_non_empty, char_ids_non_empty, mask=mask_non_empty)\n",
    "            else:\n",
    "                predictions_non_empty = []\n",
    "            \n",
    "            predictions = []\n",
    "            non_empty_iter = iter(predictions_non_empty)\n",
    "            for i in range(len(lengths)):\n",
    "                if lengths[i] == 0:\n",
    "                    predictions.append([])\n",
    "                else:\n",
    "                    predictions.append(next(non_empty_iter))\n",
    "            \n",
    "            for i, (pred, length) in enumerate(zip(predictions, lengths)):\n",
    "                if length == 0:\n",
    "                    pred_tags = []\n",
    "                    true_tags = []\n",
    "                else:\n",
    "                    pred_tags = [idx2tag[idx] for idx in pred[:length]]\n",
    "                    true_tags = [idx2tag[tag_ids[i][j].item()] for j in range(length)]\n",
    "                \n",
    "                all_predictions.append(pred_tags)\n",
    "                all_true_tags.append(true_tags)\n",
    "    \n",
    "    return all_true_tags, all_predictions\n",
    "\n",
    "print(\"Training functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with IMPROVEMENTS...\n",
      "\n",
      "================================================================================\n",
      "üéØ v2 Improvements:\n",
      "  ‚úì Dropout: 0.6 (reduce overfitting)\n",
      "  ‚úì Embedding dropout: Applied\n",
      "  ‚úì LR scheduler: Adaptive decay\n",
      "  ‚úì Patience: 7 epochs (more training)\n",
      "  ‚úì GloVe 300d: Richer embeddings\n",
      "  ‚úì Gradient clipping: 1.0 (more aggressive)\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/20 | Loss: 3.4774 | Val P: 0.7058 R: 0.6531 F1: 0.6784 | LR: 0.001000 | Time: 207.5s\n",
      "  ‚Üí New best F1! Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2/20 | Loss: 1.7946 | Val P: 0.7473 R: 0.6955 F1: 0.7205 | LR: 0.001000 | Time: 200.4s\n",
      "  ‚Üí New best F1! Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3/20 | Loss: 1.4767 | Val P: 0.7552 R: 0.7129 F1: 0.7334 | LR: 0.001000 | Time: 202.0s\n",
      "  ‚Üí New best F1! Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4/20 | Loss: 1.3012 | Val P: 0.7638 R: 0.7270 F1: 0.7450 | LR: 0.001000 | Time: 199.3s\n",
      "  ‚Üí New best F1! Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5/20 | Loss: 1.1825 | Val P: 0.7624 R: 0.7338 F1: 0.7478 | LR: 0.001000 | Time: 201.3s\n",
      "  ‚Üí New best F1! Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6/20 | Loss: 1.0908 | Val P: 0.7690 R: 0.7365 F1: 0.7524 | LR: 0.001000 | Time: 208.1s\n",
      "  ‚Üí New best F1! Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7/20 | Loss: 1.0164 | Val P: 0.7729 R: 0.7444 F1: 0.7584 | LR: 0.001000 | Time: 201.8s\n",
      "  ‚Üí New best F1! Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8/20 | Loss: 0.9640 | Val P: 0.7726 R: 0.7389 F1: 0.7554 | LR: 0.001000 | Time: 203.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9/20 | Loss: 0.9009 | Val P: 0.7712 R: 0.7470 F1: 0.7589 | LR: 0.001000 | Time: 203.9s\n",
      "  ‚Üí New best F1! Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 | Loss: 0.8655 | Val P: 0.7711 R: 0.7415 F1: 0.7560 | LR: 0.001000 | Time: 206.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 | Loss: 0.8241 | Val P: 0.7734 R: 0.7433 F1: 0.7581 | LR: 0.001000 | Time: 205.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 | Loss: 0.7927 | Val P: 0.7750 R: 0.7439 F1: 0.7591 | LR: 0.001000 | Time: 214.1s\n",
      "  ‚Üí New best F1! Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 | Loss: 0.7585 | Val P: 0.7684 R: 0.7399 F1: 0.7539 | LR: 0.001000 | Time: 214.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 | Loss: 0.7296 | Val P: 0.7707 R: 0.7454 F1: 0.7578 | LR: 0.001000 | Time: 207.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 | Loss: 0.7078 | Val P: 0.7659 R: 0.7449 F1: 0.7552 | LR: 0.000500 | Time: 221.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 | Loss: 0.6404 | Val P: 0.7700 R: 0.7480 F1: 0.7589 | LR: 0.000500 | Time: 209.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 | Loss: 0.6089 | Val P: 0.7740 R: 0.7453 F1: 0.7594 | LR: 0.000500 | Time: 216.9s\n",
      "  ‚Üí New best F1! Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 | Loss: 0.5874 | Val P: 0.7739 R: 0.7421 F1: 0.7577 | LR: 0.000500 | Time: 210.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 | Loss: 0.5708 | Val P: 0.7660 R: 0.7464 F1: 0.7561 | LR: 0.000500 | Time: 212.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 | Loss: 0.5534 | Val P: 0.7725 R: 0.7455 F1: 0.7588 | LR: 0.000250 | Time: 209.0s\n",
      "================================================================================\n",
      "\n",
      "Training completed in 4158.2s (69.3 minutes)\n",
      "Best validation F1: 0.7594\n",
      "\n",
      "üéØ Target achieved: ‚ö†Ô∏è  Close! Try training longer.\n",
      "   v1 baseline: 0.7484\n",
      "   v2 result: 0.7594\n",
      "   Improvement: +1.10% absolute\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "print(\"Starting training with IMPROVEMENTS...\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(\"üéØ v2 Improvements:\")\n",
    "print(\"  ‚úì Dropout: 0.6 (reduce overfitting)\")\n",
    "print(\"  ‚úì Embedding dropout: Applied\")\n",
    "print(\"  ‚úì LR scheduler: Adaptive decay\")\n",
    "print(\"  ‚úì Patience: 7 epochs (more training)\")\n",
    "print(\"  ‚úì GloVe 300d: Richer embeddings\")\n",
    "print(\"  ‚úì Gradient clipping: 1.0 (more aggressive)\")\n",
    "print(\"=\"  * 80 + \"\\n\")\n",
    "\n",
    "best_f1 = 0\n",
    "patience = 7  # IMPROVEMENT #4: Increased from 3\n",
    "patience_counter = 0\n",
    "\n",
    "training_start = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    \n",
    "    # Evaluate\n",
    "    val_true_tags, val_pred_tags = evaluate(model, val_loader, device)\n",
    "    \n",
    "    # Calculate F1\n",
    "    results = evaluate_entity_spans(val_true_tags, val_pred_tags, val_tokens)\n",
    "    val_f1 = results['f1']\n",
    "    val_precision = results['precision']\n",
    "    val_recall = results['recall']\n",
    "    \n",
    "    # IMPROVEMENT #3: Update learning rate scheduler\n",
    "    scheduler.step(val_f1)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:2d}/{NUM_EPOCHS} | \"\n",
    "          f\"Loss: {train_loss:.4f} | \"\n",
    "          f\"Val P: {val_precision:.4f} R: {val_recall:.4f} F1: {val_f1:.4f} | \"\n",
    "          f\"LR: {current_lr:.6f} | \"\n",
    "          f\"Time: {epoch_time:.1f}s\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        patience_counter = 0\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "        torch.save(model.state_dict(), 'models/bilstm_crf_v2_best.pt')\n",
    "        print(f\"  ‚Üí New best F1! Model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping after {epoch+1} epochs (patience={patience})\")\n",
    "            break\n",
    "\n",
    "training_time = time.time() - training_start\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTraining completed in {training_time:.1f}s ({training_time/60:.1f} minutes)\")\n",
    "print(f\"Best validation F1: {best_f1:.4f}\")\n",
    "print(f\"\\nüéØ Target achieved: {'‚úÖ YES!' if best_f1 >= 0.78 else '‚ö†Ô∏è  Close! Try training longer.'}\")\n",
    "print(f\"   v1 baseline: 0.7484\")\n",
    "print(f\"   v2 result: {best_f1:.4f}\")\n",
    "print(f\"   Improvement: +{(best_f1 - 0.7484)*100:.2f}% absolute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Load Best Model and Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ENTITY-SPAN LEVEL EVALUATION REPORT: Character-CNN + BiLSTM-CRF v2 (Improved)\n",
      "================================================================================\n",
      "\n",
      "OVERALL METRICS:\n",
      "  Precision: 0.7740\n",
      "  Recall:    0.7453\n",
      "  F1 Score:  0.7594\n",
      "\n",
      "  True Positives:  8200\n",
      "  False Positives: 2394\n",
      "  False Negatives: 2802\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "PER-ENTITY-TYPE METRICS:\n",
      "--------------------------------------------------------------------------------\n",
      "Entity Type          Precision    Recall       F1           Support   \n",
      "--------------------------------------------------------------------------------\n",
      "Artist               0.7703       0.8144       0.7918       2430      \n",
      "Facility             0.7622       0.6667       0.7112       1173      \n",
      "HumanSettlement      0.9040       0.9077       0.9058       2697      \n",
      "ORG                  0.7486       0.6933       0.7199       1542      \n",
      "OtherPER             0.6122       0.5842       0.5979       1527      \n",
      "Politician           0.7399       0.6183       0.6736       1150      \n",
      "PublicCorp           0.7169       0.6605       0.6875       483       \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load('models/bilstm_crf_v2_best.pt'))\n",
    "model.eval()\n",
    "\n",
    "print(\"Best model loaded!\")\n",
    "\n",
    "# Final evaluation\n",
    "val_true_tags, val_pred_tags = evaluate(model, val_loader, device)\n",
    "\n",
    "# Comprehensive report\n",
    "print_evaluation_report(\n",
    "    val_true_tags,\n",
    "    val_pred_tags,\n",
    "    val_tokens,\n",
    "    model_name=\"Character-CNN + BiLSTM-CRF v2 (Improved)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabularies saved!\n",
      "Results saved!\n"
     ]
    }
   ],
   "source": [
    "# Save vocabularies\n",
    "vocab_data = {\n",
    "    'word2idx': word2idx,\n",
    "    'char2idx': char2idx,\n",
    "    'tag2idx': tag2idx,\n",
    "    'idx2word': idx2word,\n",
    "    'idx2char': idx2char,\n",
    "    'idx2tag': idx2tag\n",
    "}\n",
    "\n",
    "with open('models/bilstm_crf_v2_vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab_data, f)\n",
    "\n",
    "print(\"Vocabularies saved!\")\n",
    "\n",
    "# Save results\n",
    "final_results = evaluate_entity_spans(val_true_tags, val_pred_tags, val_tokens)\n",
    "\n",
    "results_summary = {\n",
    "    'model': 'Character-CNN + BiLSTM-CRF v2 (Improved)',\n",
    "    'version': 'v2',\n",
    "    'improvements': [\n",
    "        'Higher dropout (0.6)',\n",
    "        'Dropout on embeddings',\n",
    "        'Learning rate scheduler',\n",
    "        'Increased patience (7)',\n",
    "        'GloVe 300d',\n",
    "        'Gradient clipping (1.0)'\n",
    "    ],\n",
    "    'precision': final_results['precision'],\n",
    "    'recall': final_results['recall'],\n",
    "    'f1': final_results['f1'],\n",
    "    'training_time': training_time,\n",
    "    'num_epochs': epoch + 1,\n",
    "    'hyperparameters': {\n",
    "        'embedding_dim': EMBEDDING_DIM,\n",
    "        'char_emb_dim': CHAR_EMB_DIM,\n",
    "        'char_hidden_dim': CHAR_HIDDEN_DIM,\n",
    "        'lstm_hidden_dim': LSTM_HIDDEN_DIM,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'dropout': DROPOUT,\n",
    "        'patience': patience,\n",
    "        'gradient_clipping': 1.0\n",
    "    },\n",
    "    'v1_baseline_f1': 0.7484,\n",
    "    'improvement_over_v1': final_results['f1'] - 0.7484\n",
    "}\n",
    "\n",
    "with open('models/bilstm_crf_v2_results.json', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(\"Results saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "### Improvements Over v1:\n",
    "\n",
    "**v1 Issues:**\n",
    "- Peaked at epoch 3 (0.7484 F1)\n",
    "- Overfitting after epoch 3\n",
    "- Early stopping at epoch 6\n",
    "\n",
    "**v2 Solutions:**\n",
    "\n",
    "1. **‚úÖ Higher Dropout (0.6)**: Reduces overfitting\n",
    "   - Expected: +1-2% F1\n",
    "\n",
    "2. **‚úÖ Embedding Dropout**: Regularizes input representations\n",
    "   - Expected: +0.5-1% F1\n",
    "\n",
    "3. **‚úÖ Learning Rate Scheduler**: Adaptive LR decay\n",
    "   - Expected: +1-2% F1\n",
    "\n",
    "4. **‚úÖ Increased Patience (7)**: More training opportunities\n",
    "   - Expected: Better convergence\n",
    "\n",
    "5. **‚úÖ GloVe 300d**: Richer semantic representations\n",
    "   - Expected: +1-2% F1\n",
    "\n",
    "6. **‚úÖ Gradient Clipping (1.0)**: More stable training\n",
    "   - Expected: Better convergence\n",
    "\n",
    "**Total Expected Improvement:** +4-6% absolute F1\n",
    "\n",
    "**Target:** 0.78-0.80 F1 (excellent for 15 entity types!)\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "If still not reaching 0.78-0.80:\n",
    "1. Try stacked BiLSTM (2 layers)\n",
    "2. Experiment with different LSTM sizes (128, 384, 512)\n",
    "3. Add layer normalization\n",
    "4. Try focal loss for class imbalance\n",
    "5. Move to M7 (Attention-based BiLSTM-CRF)\n",
    "\n",
    "### Comparison:\n",
    "\n",
    "| Model | F1 Score | Notes |\n",
    "|-------|----------|-------|\n",
    "| M1 (CRF) | 0.6815 | Classical ML baseline |\n",
    "| M4 v1 | 0.7484 | Deep learning |\n",
    "| M4 v2 | **0.78-0.80** | Target with improvements |\n",
    "| M7 (Attention) | 0.82-0.87 | State-of-the-art |\n",
    "\n",
    "Good luck! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
