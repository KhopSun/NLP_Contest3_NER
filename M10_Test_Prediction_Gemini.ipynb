{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# M10: Test Prediction with Gemini-Augmented Data\n",
        "\n",
        "This notebook:\n",
        "1. Loads your trained KB-NER model from `models/kb_ner_best.pt`\n",
        "2. Uses **Gemini-augmented test data** (already has context!)\n",
        "3. Generates predictions\n",
        "4. Saves predictions\n",
        "\n",
        "**Advantage**: No Wikipedia retrieval needed - Gemini already provided context!\n",
        "\n",
        "**Expected F1**: ~82-83% (based on validation score of 82.6%)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install --upgrade transformers huggingface-hub\n",
        "!pip install pytorch-crf\n",
        "\n",
        "print(\"âœ“ All packages installed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import XLMRobertaTokenizerFast, XLMRobertaModel\n",
        "from torchcrf import CRF\n",
        "import json\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Load Saved Model Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load checkpoint\n",
        "checkpoint_path = 'models/kb_ner_best.pt'\n",
        "\n",
        "print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "# Extract configuration and mappings\n",
        "CONFIG = checkpoint['config']\n",
        "tag2id = checkpoint['tag2id']\n",
        "id2tag = checkpoint['id2tag']\n",
        "num_tags = len(tag2id)\n",
        "\n",
        "print(f\"\\nâœ“ Checkpoint loaded successfully!\")\n",
        "print(f\"  Model trained for: {checkpoint['epoch'] + 1} epochs\")\n",
        "if 'val_f1' in checkpoint:\n",
        "    print(f\"  Validation F1: {checkpoint['val_f1']:.4f} ({checkpoint['val_f1']*100:.2f}%)\")\n",
        "    print(f\"  Validation Precision: {checkpoint['val_precision']:.4f}\")\n",
        "    print(f\"  Validation Recall: {checkpoint['val_recall']:.4f}\")\n",
        "\n",
        "print(f\"\\n  Model configuration:\")\n",
        "print(f\"    Model: {CONFIG['model_name']}\")\n",
        "print(f\"    Max length: {CONFIG['max_length']}\")\n",
        "print(f\"    Number of entity types: {num_tags}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Define Model Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class KB_NER_Dataset(Dataset):\n",
        "    \"\"\"Dataset with knowledge-augmented inputs.\"\"\"\n",
        "    \n",
        "    def __init__(self, data_file, tokenizer, max_length=128, tag2id=None):\n",
        "        self.data = []\n",
        "        with open(data_file, 'r') as f:\n",
        "            for line in f:\n",
        "                self.data.append(json.loads(line))\n",
        "        \n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.tag2id = tag2id\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        example = self.data[idx]\n",
        "        tokens = example['tokens']\n",
        "        context = example.get('context', '')\n",
        "        \n",
        "        sentence_encoding = self.tokenizer(\n",
        "            tokens,\n",
        "            is_split_into_words=True,\n",
        "            add_special_tokens=False,\n",
        "            truncation=False,\n",
        "            return_tensors=None\n",
        "        )\n",
        "        \n",
        "        if context:\n",
        "            context_encoding = self.tokenizer(\n",
        "                context,\n",
        "                is_split_into_words=False,\n",
        "                add_special_tokens=False,\n",
        "                truncation=True,\n",
        "                max_length=50,\n",
        "                return_tensors=None\n",
        "            )\n",
        "            \n",
        "            input_ids = (\n",
        "                [self.tokenizer.cls_token_id] + \n",
        "                sentence_encoding['input_ids'] + \n",
        "                [self.tokenizer.sep_token_id] + \n",
        "                context_encoding['input_ids'] + \n",
        "                [self.tokenizer.sep_token_id]\n",
        "            )\n",
        "        else:\n",
        "            input_ids = (\n",
        "                [self.tokenizer.cls_token_id] + \n",
        "                sentence_encoding['input_ids'] + \n",
        "                [self.tokenizer.sep_token_id]\n",
        "            )\n",
        "        \n",
        "        if len(input_ids) > self.max_length:\n",
        "            input_ids = input_ids[:self.max_length]\n",
        "        \n",
        "        attention_mask = [1] * len(input_ids)\n",
        "        padding_length = self.max_length - len(input_ids)\n",
        "        input_ids = input_ids + [self.tokenizer.pad_token_id] * padding_length\n",
        "        attention_mask = attention_mask + [0] * padding_length\n",
        "        \n",
        "        word_ids_sentence = sentence_encoding.word_ids()\n",
        "        word_ids = [None]\n",
        "        word_ids.extend(word_ids_sentence)\n",
        "        word_ids.append(None)\n",
        "        \n",
        "        if context:\n",
        "            context_length = len(context_encoding['input_ids']) + 1\n",
        "            word_ids.extend([None] * context_length)\n",
        "        \n",
        "        word_ids.extend([None] * padding_length)\n",
        "        \n",
        "        return {\n",
        "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
        "            'word_ids': word_ids,\n",
        "            'original_tokens': tokens,\n",
        "            'example_id': example['id']\n",
        "        }\n",
        "\n",
        "print(\"âœ“ KB_NER_Dataset class defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class XLMRobertaCRF(nn.Module):\n",
        "    \"\"\"XLM-RoBERTa + CRF for NER.\"\"\"\n",
        "    \n",
        "    def __init__(self, model_name, num_tags, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.xlmr = XLMRobertaModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Linear(self.xlmr.config.hidden_size, num_tags)\n",
        "        self.crf = CRF(num_tags, batch_first=True)\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask, labels=None, word_mask=None):\n",
        "        outputs = self.xlmr(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        sequence_output = outputs.last_hidden_state\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        \n",
        "        emissions = self.classifier(sequence_output)\n",
        "        \n",
        "        if labels is not None:\n",
        "            mask = (labels != -100).byte()\n",
        "            mask[:, 0] = 1\n",
        "            \n",
        "            labels_for_crf = labels.clone()\n",
        "            labels_for_crf[labels_for_crf == -100] = 0\n",
        "            \n",
        "            log_likelihood = self.crf(emissions, labels_for_crf, mask=mask, reduction='mean')\n",
        "            loss = -log_likelihood\n",
        "            \n",
        "            return loss, emissions\n",
        "        else:\n",
        "            if word_mask is not None:\n",
        "                mask = word_mask.byte()\n",
        "            else:\n",
        "                mask = attention_mask.byte()\n",
        "            \n",
        "            predictions = self.crf.decode(emissions, mask=mask)\n",
        "            return predictions\n",
        "\n",
        "print(\"âœ“ XLMRobertaCRF model defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Load Model and Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "model = XLMRobertaCRF(\n",
        "    CONFIG['model_name'],\n",
        "    num_tags,\n",
        "    dropout=CONFIG['dropout']\n",
        ").to(device)\n",
        "\n",
        "# Load trained weights\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()  # Set to evaluation mode\n",
        "\n",
        "print(f\"âœ“ Model loaded and ready for prediction\")\n",
        "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom collate function\n",
        "def collate_fn(batch):\n",
        "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
        "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
        "    word_ids = [item['word_ids'] for item in batch]\n",
        "    original_tokens = [item['original_tokens'] for item in batch]\n",
        "    example_ids = [item['example_id'] for item in batch]\n",
        "    \n",
        "    return {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attention_mask,\n",
        "        'word_ids': word_ids,\n",
        "        'original_tokens': original_tokens,\n",
        "        'example_ids': example_ids\n",
        "    }\n",
        "\n",
        "# Load test dataset (Gemini-augmented!)\n",
        "tokenizer = XLMRobertaTokenizerFast.from_pretrained(CONFIG['model_name'])\n",
        "\n",
        "test_dataset = KB_NER_Dataset(\n",
        "    'test_data_gemini.jsonl',  # Using Gemini-augmented data!\n",
        "    tokenizer,\n",
        "    max_length=CONFIG['max_length'],\n",
        "    tag2id=tag2id\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "print(f\"âœ“ Test dataset loaded (Gemini-augmented)\")\n",
        "print(f\"  Test examples: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Generate Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_test_data(model, dataloader, device, id2tag):\n",
        "    \"\"\"\n",
        "    Generate predictions for test data.\n",
        "    Returns predictions in the format: [(id, tokens, predicted_tags), ...]\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    all_predictions = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Predicting\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            word_ids_batch = batch['word_ids']\n",
        "            \n",
        "            # Create word_mask for CRF decoding\n",
        "            word_mask = torch.zeros_like(attention_mask)\n",
        "            for i, word_ids in enumerate(word_ids_batch):\n",
        "                for j, word_id in enumerate(word_ids):\n",
        "                    if word_id is not None:\n",
        "                        word_mask[i, j] = 1\n",
        "            \n",
        "            # CRF requires first position to be valid\n",
        "            word_mask[:, 0] = 1\n",
        "            word_mask = word_mask.to(device)\n",
        "            \n",
        "            # Get predictions\n",
        "            predictions = model(input_ids, attention_mask, word_mask=word_mask)\n",
        "            \n",
        "            # Convert to tags\n",
        "            for pred, word_ids, orig_tokens, example_id in zip(\n",
        "                predictions,\n",
        "                batch['word_ids'],\n",
        "                batch['original_tokens'],\n",
        "                batch['example_ids']\n",
        "            ):\n",
        "                # Align predictions with original tokens\n",
        "                token_preds = []\n",
        "                prev_word_idx = None\n",
        "                \n",
        "                for pred_id, word_idx in zip(pred, word_ids):\n",
        "                    if word_idx is not None and word_idx != prev_word_idx:\n",
        "                        if word_idx < len(orig_tokens):\n",
        "                            token_preds.append(id2tag[pred_id])\n",
        "                    prev_word_idx = word_idx\n",
        "                \n",
        "                # Ensure same length\n",
        "                if len(token_preds) != len(orig_tokens):\n",
        "                    token_preds = token_preds[:len(orig_tokens)]\n",
        "                    if len(token_preds) < len(orig_tokens):\n",
        "                        token_preds.extend(['O'] * (len(orig_tokens) - len(token_preds)))\n",
        "                \n",
        "                all_predictions.append({\n",
        "                    'id': example_id,\n",
        "                    'tokens': list(orig_tokens),\n",
        "                    'ner_tags': token_preds\n",
        "                })\n",
        "    \n",
        "    return all_predictions\n",
        "\n",
        "print(\"âœ“ Prediction function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate predictions\n",
        "print(\"\\nGenerating predictions on Gemini-augmented test data...\")\n",
        "predictions = predict_test_data(model, test_loader, device, id2tag)\n",
        "\n",
        "print(f\"\\nâœ“ Predictions generated for {len(predictions)} examples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Save Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save predictions in .jsonl format\n",
        "output_file = 'test_predictions_m10_gemini.jsonl'\n",
        "\n",
        "with open(output_file, 'w') as f:\n",
        "    for pred in predictions:\n",
        "        f.write(json.dumps(pred) + '\\n')\n",
        "\n",
        "print(f\"âœ“ Predictions saved to {output_file}\")\n",
        "print(f\"  Total predictions: {len(predictions)}\")\n",
        "\n",
        "# Show sample predictions\n",
        "print(\"\\nðŸ“ Sample predictions:\")\n",
        "for i in range(min(5, len(predictions))):\n",
        "    pred = predictions[i]\n",
        "    print(f\"\\nExample {i+1} (ID: {pred['id']}):\")\n",
        "    print(f\"  Tokens: {' '.join(pred['tokens'][:10])}...\" if len(pred['tokens']) > 10 else f\"  Tokens: {' '.join(pred['tokens'])}\")\n",
        "    print(f\"  Tags:   {' '.join(pred['ner_tags'][:10])}...\" if len(pred['ner_tags']) > 10 else f\"  Tags:   {' '.join(pred['ner_tags'])}\")\n",
        "    \n",
        "    # Count entities\n",
        "    num_entities = sum(1 for tag in pred['ner_tags'] if tag.startswith('B-'))\n",
        "    print(f\"  Entities found: {num_entities}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "âœ… **Complete!** Your test predictions are saved to `test_predictions_m10_gemini.jsonl`\n",
        "\n",
        "### What was done:\n",
        "1. Loaded trained KB-NER model with **82.6% validation F1**\n",
        "2. Used **Gemini-augmented test data** (context already provided by Gemini!)\n",
        "3. Generated predictions using KB-NER model\n",
        "4. Saved predictions in `.jsonl` format\n",
        "\n",
        "### Advantages of Gemini augmentation:\n",
        "- âœ… No Wikipedia API rate limits\n",
        "- âœ… High-quality context from Gemini\n",
        "- âœ… Fast - no retrieval delays\n",
        "- âœ… Consistent context quality\n",
        "\n",
        "### Expected performance:\n",
        "- **~82-83% F1** on test set (similar to validation)\n",
        "- Should beat your friends' 77-79% BERT scores! ðŸŽ‰\n",
        "\n",
        "### Files created:\n",
        "- `test_predictions_m10_gemini.jsonl` - Final predictions using Gemini context\n",
        "\n",
        "### Next steps:\n",
        "Submit `test_predictions_m10_gemini.jsonl` to your evaluation system!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
