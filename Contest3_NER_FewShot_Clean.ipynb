{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiCoNER 2 - Few-Shot NER with Gemini API\n",
    "\n",
    "This notebook uses Google's Gemini API for few-shot Named Entity Recognition on the MultiCoNER 2 dataset.\n",
    "\n",
    "## Overview\n",
    "- **Model**: Gemini 2.5 Flash\n",
    "- **Approach**: Few-shot prompting with entity verification\n",
    "- **Dataset**: MultiCoNER 2 English (7 entity types)\n",
    "- **Entity Types**: Artist, Politician, HumanSettlement, PublicCorp, ORG, Facility, OtherPER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import google.generativeai as genai\n",
    "from google.colab import userdata\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Enable tqdm for pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "print(\"All libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Gemini API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get API key from Colab secrets\n",
    "GOOGLE_API_KEY = userdata.get('gemini_api_key')\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# Initialize Gemini model\n",
    "gemini_model = genai.GenerativeModel('gemini-2.5-flash')\n",
    "\n",
    "print(\"Gemini API configured successfully.\")\n",
    "print(f\"Model: {gemini_model._model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define System Prompt\n",
    "\n",
    "This prompt instructs the model to:\n",
    "- Act as a slow, deliberate reasoning agent\n",
    "- Verify ambiguous entities by \"searching the internet\"\n",
    "- Apply strict BIO tagging rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"### SYSTEM INSTRUCTION\n",
    "**MODE: ACCURACY-FIRST / RESEARCH-ENABLED**\n",
    "You are a slow, deliberate reasoning agent. You must NOT guess.\n",
    "1. **Scan**: Identify every proper noun or capitalized token in the input.\n",
    "2. **Verify**: If you are not 100% sure of an entity's type (e.g., is \"Finisterre\" a place or a book?), you MUST pause and search the internet.\n",
    "3. **Classify**: Apply the strict class definitions below based on your search results.\n",
    "4. **Format**: Output only the final JSON list.\n",
    "\n",
    "---\n",
    "\n",
    "### Role\n",
    "You are an expert linguist and data labeling specialist specifically trained for the MultiCoNER 2 Shared Task. You possess deep knowledge of fine-grained Named Entity Recognition (NER). **Crucially, you act as a researcher who verifies facts using the internet when faced with ambiguity.**\n",
    "\n",
    "### Context\n",
    "The user will provide a list of text tokens (words/sub-words) derived from search queries, social media, or noisy web text. Your task is to analyze these tokens and map each one to a specific Named Entity Recognition tag. The data contains ambiguity, typos (e.g., \"united stats\"), and lacks capitalization cues.\n",
    "\n",
    "### Rules\n",
    "1. **Input**: A JSON list of tokens (e.g., `[\"new\", \"york\", \"is\", \"big\"]`).\n",
    "2. **Task**: Assign a BIO (Begin, Inside, Outside) tag to every single token.\n",
    "3. **Classes**: You must strictly use only the following 7 entity categories:\n",
    "   - **Artist**: Musicians, bands, actors, authors, directors, painters. (e.g., \"simon mayo\", \"picasso\")\n",
    "   - **Politician**: Government officials, politicians, heads of state. (e.g., \"obama\", \"frank d. o'connor\")\n",
    "   - **HumanSettlement**: Cities, towns, villages, states, countries, counties. (e.g., \"busan\", \"cleveland\", \"ohio\")\n",
    "   - **PublicCorp**: Commercial companies, businesses, brands. (e.g., \"safeway\", \"mcdonald 's\", \"s&p global ratings\")\n",
    "   - **ORG**: Non-commercial organizations, government agencies, political parties, sports teams, unions. (e.g., \"democrat\", \"united stats census bureau\", \"real madrid\")\n",
    "   - **Facility**: Buildings, stadiums, airports, highways, public places. (e.g., \"village hall\", \"lanxess arena\")\n",
    "   - **OtherPER**: Persons who are not artists or politicians (e.g., athletes, scientists, soldiers, fictional characters, or general people). (e.g., \"zcrny\", \"peter bourne\")\n",
    "   - **O**: Tokens that are not part of a named entity (CRITICAL: This includes Books, Movies, Songs, Albums, and Products).\n",
    "4. **Tagging Scheme**:\n",
    "   - Use `B-<Category>` for the first token of an entity.\n",
    "   - Use `I-<Category>` for all subsequent tokens of the same entity.\n",
    "   - Use `O` for non-entities.\n",
    "\n",
    "### Verification Strategy (CRITICAL)\n",
    "**If you are unsure about a proper noun, you MUST SEARCH THE INTERNET.**\n",
    "* **Ambiguity**: If a word looks like a name (e.g., \"finisterre\", \"wclv\", \"zcrny\") but you don't know it, pause and search for it.\n",
    "* **Distinction Logic**:\n",
    "    * If search shows it is a **Book, Movie, Album, or Product** -> Tag as **O**. (We do not have tags for these in this specific task).\n",
    "    * If search shows it is a **Company** -> Check if it is commercial (**PublicCorp**) or non-profit/sports (**ORG**).\n",
    "    * If search shows it is a **Person** -> Check if they are a politician (**Politician**), creator (**Artist**), or other (**OtherPER**).\n",
    "\n",
    "### Constraints\n",
    "1. **Length Consistency**: The output list MUST have exactly the same number of items as the input list.\n",
    "2. **Format**: Output ONLY a raw JSON list of strings. Do not include markdown formatting, explanations, or notes.\n",
    "3. **Robustness**: Treat lower-cased proper nouns as entities (e.g., \"paris\" -> B-HumanSettlement). Context is king.\n",
    "\n",
    "### Examples\n",
    "**Input**:\n",
    "[\"frank\", \"d.\", \"o'connor\", \"(\", \"1909\", \"–\", \"1992\", \")\", \"lawyer\", \"judge\", \"and\", \"politician\", \"−\", \"head\", \"trauma\"]\n",
    "**Output**:\n",
    "[\"B-Politician\", \"I-Politician\", \"I-Politician\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"]\n",
    "\n",
    "**Input**:\n",
    "[\"prior\", \"to\", \"the\", \"stabbings\", \"he\", \"was\", \"an\", \"employee\", \"of\", \"safeway\", \".\"]\n",
    "**Output**:\n",
    "[\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-PublicCorp\", \"O\"]\n",
    "\n",
    "**Input**:\n",
    "[\"its\", \"current\", \"representative\", \"is\", \"democrat\", \"bruce\", \"antone\", \".\"]\n",
    "**Output**:\n",
    "[\"O\", \"O\", \"O\", \"O\", \"B-ORG\", \"B-Politician\", \"I-Politician\", \"O\"]\n",
    "\n",
    "**Input**:\n",
    "[\"finisterre\", \"a\", \"1943\", \"poetry\", \"collection\", \"by\", \"eugenio\", \"montale\"]\n",
    "**Output**:\n",
    "[\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-Artist\", \"I-Artist\"]\n",
    "*(Note: Search reveals 'Finisterre' is a book, so it is tagged 'O')*\n",
    "\"\"\"\n",
    "\n",
    "print(\"System prompt defined.\")\n",
    "print(f\"Prompt length: {len(SYSTEM_PROMPT)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_few_shot_prediction_with_delay(model, system_prompt, tokens, delay_seconds=6.0):\n",
    "    \"\"\"\n",
    "    Generates NER predictions using Gemini API with rate limiting.\n",
    "    \n",
    "    Args:\n",
    "        model: The initialized Gemini generative model\n",
    "        system_prompt: The complete prompt with instructions and examples\n",
    "        tokens: List of tokens for which to generate predictions\n",
    "        delay_seconds: Delay between API calls (default 6.0 to avoid 429 errors)\n",
    "    \n",
    "    Returns:\n",
    "        list: Predicted BIO tags, or error message string if parsing fails\n",
    "    \"\"\"\n",
    "    # Add delay to avoid rate limiting\n",
    "    time.sleep(delay_seconds)\n",
    "    \n",
    "    # Construct final prompt\n",
    "    final_prompt = f\"{system_prompt}\\n\\nNew Tokens: {json.dumps(tokens)}\\nTags:\"\n",
    "    \n",
    "    try:\n",
    "        # Call Gemini API\n",
    "        response = model.generate_content(final_prompt)\n",
    "        raw_response = response.text.strip()\n",
    "        \n",
    "        # Clean response if wrapped in markdown code blocks\n",
    "        if raw_response.startswith(\"```json\") and raw_response.endswith(\"```\"):\n",
    "            raw_response = raw_response[7:-3].strip()  # Remove ```json and ```\n",
    "        elif raw_response.startswith(\"```\") and raw_response.endswith(\"```\"):\n",
    "            raw_response = raw_response[3:-3].strip()  # Remove ``` and ```\n",
    "        \n",
    "        # Parse JSON response\n",
    "        predicted_tags = json.loads(raw_response)\n",
    "        \n",
    "        # Validate response format\n",
    "        if isinstance(predicted_tags, list) and all(isinstance(tag, str) for tag in predicted_tags):\n",
    "            return predicted_tags\n",
    "        else:\n",
    "            return f\"Error: Invalid format. Raw: {raw_response}\"\n",
    "            \n",
    "    except json.JSONDecodeError as e:\n",
    "        return f\"Error: JSON parsing failed: {e}. Raw: {raw_response}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: API call failed: {e}\"\n",
    "\n",
    "print(\"Prediction function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation split\n",
    "try:\n",
    "    val_data = pd.read_json('val_split.jsonl', lines=True)\n",
    "    print(f\"✓ Loaded val_split.jsonl: {len(val_data)} examples\")\n",
    "    print(f\"  Columns: {list(val_data.columns)}\")\n",
    "    display(val_data.head(3))\n",
    "except FileNotFoundError:\n",
    "    print(\"✗ Error: val_split.jsonl not found\")\n",
    "    val_data = pd.DataFrame()\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading val_split.jsonl: {e}\")\n",
    "    val_data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "try:\n",
    "    test_data = pd.read_json('test_data.jsonl', lines=True)\n",
    "    print(f\"✓ Loaded test_data.jsonl: {len(test_data)} examples\")\n",
    "    print(f\"  Columns: {list(test_data.columns)}\")\n",
    "    display(test_data.head(3))\n",
    "except FileNotFoundError:\n",
    "    print(\"✗ Error: test_data.jsonl not found\")\n",
    "    test_data = pd.DataFrame()\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading test_data.jsonl: {e}\")\n",
    "    test_data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Predictions on Validation Set\n",
    "\n",
    "**Note**: This may take a long time due to:\n",
    "- 6 second delay per example (to avoid rate limits)\n",
    "- Gemini API processing time\n",
    "\n",
    "Estimated time: ~6-10 seconds per example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not val_data.empty and 'tokens' in val_data.columns:\n",
    "    print(\"Generating predictions for validation set...\")\n",
    "    print(f\"Total examples: {len(val_data)}\")\n",
    "    print(f\"Estimated time: ~{len(val_data) * 6 / 60:.1f} minutes\\n\")\n",
    "    \n",
    "    # Generate predictions with progress bar\n",
    "    val_data['predicted_tags'] = val_data['tokens'].progress_apply(\n",
    "        lambda tokens: get_few_shot_prediction_with_delay(\n",
    "            gemini_model, \n",
    "            SYSTEM_PROMPT, \n",
    "            tokens\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(\"\\n✓ Predictions complete!\")\n",
    "    display(val_data.head())\n",
    "else:\n",
    "    print(\"✗ Cannot generate predictions: val_data is empty or missing 'tokens' column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Validation Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not val_data.empty and 'predicted_tags' in val_data.columns:\n",
    "    output_file = 'val_split_predictions.jsonl'\n",
    "    val_data.to_json(output_file, orient='records', lines=True)\n",
    "    print(f\"✓ Validation predictions saved to {output_file}\")\n",
    "    print(f\"  Total examples: {len(val_data)}\")\n",
    "else:\n",
    "    print(\"✗ No predictions to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import utils\n",
    "    print(\"✓ utils.py imported successfully\")\n",
    "    \n",
    "    # Load predictions\n",
    "    predictions_df = pd.read_json('val_split_predictions.jsonl', lines=True)\n",
    "    \n",
    "    # Extract ground truth and predicted labels\n",
    "    ground_truth = predictions_df['ner_tags'].tolist()\n",
    "    predicted = predictions_df['predicted_tags'].tolist()\n",
    "    \n",
    "    # Filter out error messages (keep only valid predictions)\n",
    "    valid_pairs = [\n",
    "        (gt, pred) for gt, pred in zip(ground_truth, predicted)\n",
    "        if isinstance(pred, list)\n",
    "    ]\n",
    "    \n",
    "    if valid_pairs:\n",
    "        ground_truth_clean = [pair[0] for pair in valid_pairs]\n",
    "        predicted_clean = [pair[1] for pair in valid_pairs]\n",
    "        \n",
    "        print(f\"\\nEvaluating {len(valid_pairs)} valid predictions...\")\n",
    "        print(f\"Skipped {len(ground_truth) - len(valid_pairs)} errors\\n\")\n",
    "        \n",
    "        # Evaluate\n",
    "        if hasattr(utils, 'evaluate_ner'):\n",
    "            results = utils.evaluate_ner(ground_truth_clean, predicted_clean)\n",
    "            print(\"\\n=== Evaluation Results ===\")\n",
    "            print(results)\n",
    "        else:\n",
    "            print(\"✗ Error: evaluate_ner function not found in utils.py\")\n",
    "    else:\n",
    "        print(\"✗ No valid predictions to evaluate\")\n",
    "        \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"✗ File not found: {e}\")\n",
    "except ImportError:\n",
    "    print(\"✗ Error: utils.py not found\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error during evaluation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Generate Predictions on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not test_data.empty and 'tokens' in test_data.columns:\n",
    "    print(\"Generating predictions for test set...\")\n",
    "    print(f\"Total examples: {len(test_data)}\")\n",
    "    print(f\"Estimated time: ~{len(test_data) * 6 / 60:.1f} minutes\\n\")\n",
    "    \n",
    "    # Generate predictions with progress bar\n",
    "    test_data['predicted_tags'] = test_data['tokens'].progress_apply(\n",
    "        lambda tokens: get_few_shot_prediction_with_delay(\n",
    "            gemini_model, \n",
    "            SYSTEM_PROMPT, \n",
    "            tokens\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(\"\\n✓ Predictions complete!\")\n",
    "    display(test_data.head())\n",
    "else:\n",
    "    print(\"✗ Cannot generate predictions: test_data is empty or missing 'tokens' column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not test_data.empty and 'predicted_tags' in test_data.columns:\n",
    "    output_file = 'test_data_predictions.jsonl'\n",
    "    test_data.to_json(output_file, orient='records', lines=True)\n",
    "    print(f\"✓ Test predictions saved to {output_file}\")\n",
    "    print(f\"  Total examples: {len(test_data)}\")\n",
    "else:\n",
    "    print(\"✗ No predictions to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Files Generated:\n",
    "- `val_split_predictions.jsonl` - Validation set with predictions\n",
    "- `test_data_predictions.jsonl` - Test set with predictions\n",
    "\n",
    "### Next Steps:\n",
    "1. Review evaluation results on validation set\n",
    "2. Analyze errors (look for patterns in misclassifications)\n",
    "3. Optionally refine the system prompt based on error analysis\n",
    "4. Submit test predictions for final evaluation\n",
    "\n",
    "### Notes:\n",
    "- This is a **few-shot learning** approach (no fine-tuning required)\n",
    "- Expected performance: Lower than fine-tuned models (M8/M9) but requires no training\n",
    "- Main advantage: Quick experimentation and no GPU needed\n",
    "- Main disadvantage: Slow inference (6+ seconds per example) and API costs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
