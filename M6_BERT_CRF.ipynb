{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 6: BERT + CRF Hybrid for Named Entity Recognition\n",
    "\n",
    "This notebook implements a BERT model with a CRF layer on top for structured prediction. The CRF layer ensures valid BIO sequences and typically improves performance by 1-2% F1 over BERT alone.\n",
    "\n",
    "**Key Improvements over BERT alone:**\n",
    "- CRF learns transition constraints between tags\n",
    "- Viterbi decoding ensures globally optimal tag sequences\n",
    "- Prevents invalid BIO sequences (e.g., O ‚Üí I-Person)\n",
    "- Better boundary detection\n",
    "\n",
    "**Expected Performance:** 90-92% F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch and Transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BertModel,\n",
    "    BertConfig,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from datasets import Dataset as HFDataset\n",
    "\n",
    "# CRF layer\n",
    "# Install if not already installed: pip install pytorch-crf\n",
    "from torchcrf import CRF\n",
    "\n",
    "# Import utils for evaluation\n",
    "from utils import extract_entities, evaluate_entity_spans, print_evaluation_report\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(file_path):\n",
    "    \"\"\"Load JSONL file into a list of dictionaries\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.strip()))\n",
    "    return data\n",
    "\n",
    "def clean_data(data):\n",
    "    \"\"\"Remove samples with invalid BIO sequences\"\"\"\n",
    "    cleaned_data = []\n",
    "    invalid_count = 0\n",
    "\n",
    "    for sample in data:\n",
    "        prev_tag = 'O'\n",
    "        has_issue = False\n",
    "\n",
    "        for tag in sample['ner_tags']:\n",
    "            if tag.startswith('I-'):\n",
    "                entity_type = tag[2:]\n",
    "                # Check if I- follows B- or I- of same type\n",
    "                if not (prev_tag == f'B-{entity_type}' or prev_tag == f'I-{entity_type}'):\n",
    "                    has_issue = True\n",
    "                    break\n",
    "            prev_tag = tag\n",
    "\n",
    "        if not has_issue:\n",
    "            cleaned_data.append(sample)\n",
    "        else:\n",
    "            invalid_count += 1\n",
    "\n",
    "    print(f\"Removed {invalid_count} samples with invalid BIO sequences\")\n",
    "    return cleaned_data\n",
    "\n",
    "# Load all training data and test data\n",
    "train_data_all = load_jsonl('train_data.jsonl')\n",
    "test_data = load_jsonl('test_data.jsonl')\n",
    "\n",
    "print(f\"Total training samples: {len(train_data_all):,}\")\n",
    "print(f\"Test samples: {len(test_data):,}\")\n",
    "\n",
    "# Clean training data\n",
    "train_data_cleaned = clean_data(train_data_all)\n",
    "print(f\"Training samples after cleaning: {len(train_data_cleaned):,}\")\n",
    "\n",
    "# Create stratification labels based on presence of entities\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "stratify_labels = []\n",
    "for sample in train_data_cleaned:\n",
    "    has_entities = any(tag != 'O' for tag in sample['ner_tags'])\n",
    "    stratify_labels.append(int(has_entities))\n",
    "\n",
    "# Split into train and validation (90/10 split, same as other models)\n",
    "train_data, val_data = train_test_split(\n",
    "    train_data_cleaned,\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    stratify=stratify_labels\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining samples: {len(train_data):,}\")\n",
    "print(f\"Validation samples: {len(val_data):,}\")\n",
    "print(f\"Test samples: {len(test_data):,}\")\n",
    "\n",
    "# Show example\n",
    "print(\"\\nExample training sample:\")\n",
    "print(json.dumps(train_data[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyze Tag Set and Initialize BERT+CRF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique tags\n",
    "all_tags = set()\n",
    "tag_counts = Counter()\n",
    "\n",
    "for sample in train_data:\n",
    "    all_tags.update(sample['ner_tags'])\n",
    "    tag_counts.update(sample['ner_tags'])\n",
    "\n",
    "print(f\"Unique tags: {len(all_tags)}\")\n",
    "print(\"\\nTag distribution:\")\n",
    "for tag, count in sorted(tag_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    percentage = count / sum(tag_counts.values()) * 100\n",
    "    print(f\"  {tag:20s}: {count:8,} ({percentage:5.2f}%)\")\n",
    "\n",
    "# Create tag lists\n",
    "label_list = sorted(list(all_tags))\n",
    "num_labels = len(label_list)\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "print(f\"\\nNumber of labels: {num_labels}\")\n",
    "print(f\"Labels: {label_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 BERT+CRF Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertCRF(nn.Module):\n",
    "    \"\"\"BERT model with CRF layer for sequence labeling\"\"\"\n",
    "    \n",
    "    def __init__(self, bert_model_name, num_tags):\n",
    "        super(BertCRF, self).__init__()\n",
    "        \n",
    "        # BERT encoder\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # Emission layer (maps BERT hidden states to tag scores)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_tags)\n",
    "        \n",
    "        # CRF layer\n",
    "        self.crf = CRF(num_tags, batch_first=True)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        \"\"\"Forward pass\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Token IDs (batch_size, seq_len)\n",
    "            attention_mask: Attention mask (batch_size, seq_len)\n",
    "            labels: Optional label IDs (batch_size, seq_len)\n",
    "            \n",
    "        Returns:\n",
    "            If labels provided: loss (negative log-likelihood)\n",
    "            If labels not provided: list of predicted tag sequences\n",
    "        \"\"\"\n",
    "        # BERT forward pass\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        sequence_output = outputs[0]  # (batch, seq_len, hidden_size)\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        \n",
    "        # Emission scores\n",
    "        emissions = self.classifier(sequence_output)  # (batch, seq_len, num_tags)\n",
    "        \n",
    "        if labels is not None:\n",
    "            # Training: compute CRF loss\n",
    "            # Convert attention_mask to byte mask for CRF\n",
    "            mask = attention_mask.type(torch.uint8)\n",
    "            \n",
    "            # Filter out -100 labels (ignored tokens)\n",
    "            labels_masked = labels.clone()\n",
    "            labels_masked[labels == -100] = 0  # CRF requires valid indices\n",
    "            \n",
    "            # Compute negative log-likelihood\n",
    "            loss = -self.crf(emissions, labels_masked, mask=mask, reduction='mean')\n",
    "            return loss\n",
    "        else:\n",
    "            # Inference: CRF decoding\n",
    "            mask = attention_mask.type(torch.uint8)\n",
    "            predictions = self.crf.decode(emissions, mask=mask)\n",
    "            return predictions\n",
    "\n",
    "# Initialize tokenizer\n",
    "model_name = \"bert-base-cased\"  # Use cased for NER (preserves capitalization)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Loading model: {model_name}\")\n",
    "\n",
    "# Initialize BERT+CRF model\n",
    "model = BertCRF(model_name, num_tags=num_labels)\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")\n",
    "print(f\"Tokenizer vocab size: {tokenizer.vocab_size:,}\")\n",
    "print(f\"Number of tags: {num_tags}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Dataset and Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "    \"\"\"Dataset for NER with BERT tokenization\"\"\"\n",
    "    \n",
    "    def __init__(self, data, tokenizer, label2id, max_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        tokens = item['tokens']\n",
    "        labels = item['ner_tags']\n",
    "        \n",
    "        # Tokenize\n",
    "        tokenized = self.tokenizer(\n",
    "            tokens,\n",
    "            is_split_into_words=True,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Align labels with tokenized input\n",
    "        word_ids = tokenized.word_ids()\n",
    "        aligned_labels = []\n",
    "        previous_word_idx = None\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                # Special tokens\n",
    "                aligned_labels.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # First token of word\n",
    "                aligned_labels.append(self.label2id[labels[word_idx]])\n",
    "            else:\n",
    "                # Subsequent tokens of same word\n",
    "                aligned_labels.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        # Convert to tensors\n",
    "        input_ids = tokenized['input_ids'].squeeze()\n",
    "        attention_mask = tokenized['attention_mask'].squeeze()\n",
    "        labels = torch.tensor(aligned_labels, dtype=torch.long)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = NERDataset(train_data, tokenizer, label2id)\n",
    "val_dataset = NERDataset(val_data, tokenizer, label2id)\n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataset):,}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset):,}\")\n",
    "\n",
    "# Check an example\n",
    "example = train_dataset[0]\n",
    "print(\"\\nExample from dataset:\")\n",
    "print(f\"  Input IDs shape: {example['input_ids'].shape}\")\n",
    "print(f\"  Attention mask shape: {example['attention_mask'].shape}\")\n",
    "print(f\"  Labels shape: {example['labels'].shape}\")\n",
    "print(f\"  Number of -100 labels: {(example['labels'] == -100).sum().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    \"\"\"Train model for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        # Move to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        loss = model(input_ids, attention_mask, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Log progress\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f\"  Batch {batch_idx + 1}/{num_batches}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device, id2label):\n",
    "    \"\"\"Evaluate model and compute metrics\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Move to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Get loss\n",
    "            loss = model(input_ids, attention_mask, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Get predictions\n",
    "            predictions = model(input_ids, attention_mask)\n",
    "            \n",
    "            # Convert to tag sequences\n",
    "            for i, (pred_seq, label_seq) in enumerate(zip(predictions, labels)):\n",
    "                pred_tags = []\n",
    "                true_tags = []\n",
    "                \n",
    "                for j, (pred, label) in enumerate(zip(pred_seq, label_seq.tolist())):\n",
    "                    if label != -100 and j < len(pred_seq):  # Valid token\n",
    "                        pred_tags.append(id2label[pred])\n",
    "                        true_tags.append(id2label[label])\n",
    "                \n",
    "                if len(true_tags) > 0:  # Only add non-empty sequences\n",
    "                    all_predictions.append(pred_tags)\n",
    "                    all_labels.append(true_tags)\n",
    "    \n",
    "    # Compute metrics\n",
    "    results = evaluate_entity_spans(all_labels, all_predictions)\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'precision': results['precision'],\n",
    "        'recall': results['recall'],\n",
    "        'f1': results['f1']\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Training functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train BERT+CRF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Data loaders created with batch size {batch_size}\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "\n",
    "# Initialize optimizer and scheduler\n",
    "num_epochs = 4\n",
    "learning_rate = 5e-5\n",
    "weight_decay = 0.01\n",
    "warmup_steps = 500\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining configuration:\")\n",
    "print(f\"  Learning rate: {learning_rate}\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Epochs: {num_epochs}\")\n",
    "print(f\"  Warmup steps: {warmup_steps}\")\n",
    "print(f\"  Weight decay: {weight_decay}\")\n",
    "print(f\"  Total training steps: {total_steps:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "import time\n",
    "\n",
    "best_f1 = 0\n",
    "patience = 2\n",
    "patience_counter = 0\n",
    "training_start_time = time.time()\n",
    "\n",
    "print(\"Starting BERT+CRF training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
    "    \n",
    "    # Evaluate\n",
    "    val_metrics = evaluate(model, val_loader, device, id2label)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch + 1} Results:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss:   {val_metrics['loss']:.4f}\")\n",
    "    print(f\"  Precision:  {val_metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall:     {val_metrics['recall']:.4f}\")\n",
    "    print(f\"  F1 Score:   {val_metrics['f1']:.4f}\")\n",
    "    print(f\"  Time:       {epoch_time:.1f}s\")\n",
    "    \n",
    "    # Check if this is the best model\n",
    "    if val_metrics['f1'] > best_f1:\n",
    "        best_f1 = val_metrics['f1']\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Save best model\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'f1_score': best_f1,\n",
    "            'label2id': label2id,\n",
    "            'id2label': id2label\n",
    "        }, 'bert_crf_best_model.pt')\n",
    "        print(f\"  ‚úÖ New best model saved! (F1: {best_f1:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  ‚è≥ No improvement. Patience: {patience_counter}/{patience}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"\\nEarly stopping triggered after {epoch + 1} epochs\")\n",
    "        break\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "print(f\"\\nTraining completed in {training_time/60:.2f} minutes\")\n",
    "print(f\"Best validation F1: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Best Model and Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load('bert_crf_best_model.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "print(f\"Loaded best model from epoch {checkpoint['epoch']} with F1: {checkpoint['f1_score']:.4f}\")\n",
    "\n",
    "# Final evaluation\n",
    "print(\"\\nFinal evaluation on validation set:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "val_metrics = evaluate(model, val_loader, device, id2label)\n",
    "\n",
    "print(\"\\nBERT+CRF Final Results:\")\n",
    "print(f\"  Precision: {val_metrics['precision']:.4f}\")\n",
    "print(f\"  Recall:    {val_metrics['recall']:.4f}\")\n",
    "print(f\"  F1 Score:  {val_metrics['f1']:.4f}\")\n",
    "print(f\"  Validation loss: {val_metrics['loss']:.4f}\")\n",
    "\n",
    "# Store results for comparison\n",
    "bert_crf_results = {\n",
    "    'model': 'BERT+CRF',\n",
    "    'precision': val_metrics['precision'],\n",
    "    'recall': val_metrics['recall'],\n",
    "    'f1': val_metrics['f1'],\n",
    "    'training_time': training_time,\n",
    "    'parameters': model.num_parameters()\n",
    "}\n",
    "\n",
    "# Compare with expected from implementation plan\n",
    "expected_f1_min = 0.90\n",
    "actual_f1 = val_metrics['f1']\n",
    "\n",
    "print(f\"\\nPerformance Analysis:\")\n",
    "print(f\"  Expected F1 minimum: {expected_f1_min:.2f}\")\n",
    "print(f\"  Actual F1 score:     {actual_f1:.4f}\")\n",
    "\n",
    "if actual_f1 >= expected_f1_min:\n",
    "    print(\"  ‚úÖ F1 score meets expectations!\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è  F1 score below expected minimum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyze Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed predictions on validation set for analysis\n",
    "print(\"Generating detailed predictions for analysis...\")\n",
    "\n",
    "model.eval()\n",
    "val_sentences = [sample['tokens'] for sample in val_data]\n",
    "val_true_tags = [sample['ner_tags'] for sample in val_data]\n",
    "val_pred_tags = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = model(input_ids, attention_mask)\n",
    "        \n",
    "        # Convert to tag sequences\n",
    "        for i, (pred_seq, label_seq) in enumerate(zip(predictions, labels)):\n",
    "            pred_tags = []\n",
    "            \n",
    "            for j, (pred, label) in enumerate(zip(pred_seq, label_seq.tolist())):\n",
    "                if label != -100 and j < len(pred_seq):  # Valid token\n",
    "                    pred_tags.append(id2label[pred])\n",
    "            \n",
    "            val_pred_tags.append(pred_tags)\n",
    "\n",
    "# Show examples\n",
    "print(\"\\nPrediction examples:\")\n",
    "for i in range(5):\n",
    "    if i < len(val_sentences):\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        tokens = val_sentences[i][:15]\n",
    "        true_tags = val_true_tags[i][:15]\n",
    "        pred_tags = val_pred_tags[i][:15]\n",
    "        \n",
    "        print(f\"  Tokens:    {tokens}\")\n",
    "        print(f\"  True:      {true_tags}\")\n",
    "        print(f\"  Predicted: {pred_tags}\")\n",
    "        \n",
    "        # Count correct predictions\n",
    "        correct = sum(1 for t, p in zip(true_tags, pred_tags) if t == p)\n",
    "        total = len(true_tags)\n",
    "        print(f\"  Accuracy:  {correct}/{total} ({correct/total:.2%})\")\n",
    "\n",
    "# Check for valid BIO sequences\n",
    "print(\"\\nChecking BIO sequence validity...\")\n",
    "invalid_count = 0\n",
    "for pred_tags in val_pred_tags:\n",
    "    prev_tag = 'O'\n",
    "    for tag in pred_tags:\n",
    "        if tag.startswith('I-'):\n",
    "            entity_type = tag[2:]\n",
    "            if not (prev_tag == f'B-{entity_type}' or prev_tag == f'I-{entity_type}'):\n",
    "                invalid_count += 1\n",
    "                break\n",
    "        prev_tag = tag\n",
    "\n",
    "print(f\"Invalid BIO sequences: {invalid_count}/{len(val_pred_tags)}\")\n",
    "if invalid_count == 0:\n",
    "    print(\"‚úÖ All predicted BIO sequences are valid!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_errors(true_tags_list, pred_tags_list, sentences_list, num_examples=5):\n",
    "    \"\"\"Analyze common error patterns\"\"\"\n",
    "    \n",
    "    false_positives = []  # Predicted entity that's not actually an entity\n",
    "    false_negatives = []  # Missed entity\n",
    "    wrong_type = []       # Correct span but wrong entity type\n",
    "    \n",
    "    for true_tags, pred_tags, tokens in zip(true_tags_list, pred_tags_list, sentences_list):\n",
    "        true_entities = extract_entities(tokens, true_tags)\n",
    "        pred_entities = extract_entities(tokens, pred_tags)\n",
    "        \n",
    "        true_spans = {(start, end, entity_type) for _, entity_type, start, end in true_entities}\n",
    "        pred_spans = {(start, end, entity_type) for _, entity_type, start, end in pred_entities}\n",
    "        \n",
    "        # False positives: predicted but not true\n",
    "        fp_spans = pred_spans - true_spans\n",
    "        for start, end, entity_type in fp_spans:\n",
    "            entity_text = ' '.join(tokens[start:end+1])\n",
    "            false_positives.append((entity_text, entity_type, tokens))\n",
    "        \n",
    "        # False negatives: true but not predicted\n",
    "        fn_spans = true_spans - pred_spans\n",
    "        for start, end, entity_type in fn_spans:\n",
    "            entity_text = ' '.join(tokens[start:end+1])\n",
    "            false_negatives.append((entity_text, entity_type, tokens))\n",
    "        \n",
    "        # Wrong type: same span but different type\n",
    "        true_span_dict = {(start, end): entity_type for start, end, entity_type in true_spans}\n",
    "        pred_span_dict = {(start, end): entity_type for start, end, entity_type in pred_spans}\n",
    "        \n",
    "        common_spans = set(true_span_dict.keys()) & set(pred_span_dict.keys())\n",
    "        for start, end in common_spans:\n",
    "            true_type = true_span_dict[(start, end)]\n",
    "            pred_type = pred_span_dict[(start, end)]\n",
    "            if true_type != pred_type:\n",
    "                entity_text = ' '.join(tokens[start:end+1])\n",
    "                wrong_type.append((entity_text, true_type, pred_type, tokens))\n",
    "    \n",
    "    return false_positives, false_negatives, wrong_type\n",
    "\n",
    "# Analyze errors\n",
    "fp, fn, wt = analyze_errors(val_true_tags, val_pred_tags, val_sentences)\n",
    "\n",
    "print(\"\\nError Analysis:\")\n",
    "print(f\"False Positives: {len(fp):,} (predicted entities that shouldn't exist)\")\n",
    "print(f\"False Negatives: {len(fn):,} (missed entities)\")\n",
    "print(f\"Wrong Type:      {len(wt):,} (correct span, wrong entity type)\")\n",
    "\n",
    "# Show examples of each error type\n",
    "print(\"\\nExample False Positives:\")\n",
    "for i, (text, pred_type, tokens) in enumerate(fp[:3]):\n",
    "    print(f\"  {i+1}. '{text}' ‚Üí predicted as {pred_type}\")\n",
    "\n",
    "print(\"\\nExample False Negatives:\")\n",
    "for i, (text, true_type, tokens) in enumerate(fn[:3]):\n",
    "    print(f\"  {i+1}. '{text}' ‚Üí missed {true_type}\")\n",
    "\n",
    "print(\"\\nExample Wrong Types:\")\n",
    "for i, (text, true_type, pred_type, tokens) in enumerate(wt[:3]):\n",
    "    print(f\"  {i+1}. '{text}' ‚Üí true: {true_type}, predicted: {pred_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Generate Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_bert_crf(tokens_list, model, tokenizer, batch_size=32, max_length=128):\n",
    "    \"\"\"Predict NER tags using BERT+CRF model\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(tokens_list), batch_size):\n",
    "        if i % 500 == 0:\n",
    "            print(f\"  Processing {i:,}/{len(tokens_list):,}\")\n",
    "        \n",
    "        batch_tokens = tokens_list[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize batch\n",
    "        tokenized = tokenizer(\n",
    "            batch_tokens,\n",
    "            is_split_into_words=True,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        for k, v in tokenized.items():\n",
    "            tokenized[k] = v.to(device)\n",
    "        \n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            pred_sequences = model(\n",
    "                tokenized['input_ids'],\n",
    "                tokenized['attention_mask']\n",
    "            )\n",
    "        \n",
    "        # Convert predictions back to tag sequences\n",
    "        for j, (tokens, pred_seq) in enumerate(zip(batch_tokens, pred_sequences)):\n",
    "            word_ids = tokenized.word_ids(batch_index=j)\n",
    "            pred_tags = []\n",
    "            \n",
    "            for k, word_id in enumerate(word_ids):\n",
    "                if word_id is not None and word_id < len(tokens):\n",
    "                    # Only keep prediction for first subword token\n",
    "                    if k == 0 or word_ids[k-1] != word_id:\n",
    "                        if k < len(pred_seq):\n",
    "                            pred_tags.append(id2label[pred_seq[k]])\n",
    "            \n",
    "            predictions.append(pred_tags)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Prepare test data\n",
    "test_sentences = [sample['tokens'] for sample in test_data]\n",
    "\n",
    "print(f\"\\nGenerating predictions for {len(test_sentences):,} test sentences...\")\n",
    "test_pred_tags = predict_bert_crf(test_sentences, model, tokenizer)\n",
    "\n",
    "print(f\"\\nTest predictions complete!\")\n",
    "print(f\"Generated predictions for {len(test_pred_tags):,} sentences\")\n",
    "\n",
    "# Verify format\n",
    "print(\"\\nTest prediction examples:\")\n",
    "for i in range(3):\n",
    "    sample = test_data[i]\n",
    "    tokens = sample['tokens']\n",
    "    pred_tags = test_pred_tags[i]\n",
    "    print(f\"\\nExample {i+1} (ID: {sample['id']}):\")\n",
    "    print(f\"  Tokens:    {tokens[:10]}{'...' if len(tokens) > 10 else ''}\")\n",
    "    print(f\"  Predicted: {pred_tags[:10]}{'...' if len(pred_tags) > 10 else ''}\")\n",
    "    print(f\"  Length match: {len(tokens) == len(pred_tags)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predictions to test data\n",
    "test_data_with_predictions = []\n",
    "for sample, pred_tags in zip(test_data, test_pred_tags):\n",
    "    sample_copy = sample.copy()\n",
    "    sample_copy['ner_tags'] = pred_tags\n",
    "    test_data_with_predictions.append(sample_copy)\n",
    "\n",
    "# Validate predictions\n",
    "print(\"Validating test predictions...\")\n",
    "\n",
    "validation_errors = []\n",
    "for i, sample in enumerate(test_data_with_predictions):\n",
    "    # Check length match\n",
    "    if len(sample['tokens']) != len(sample['ner_tags']):\n",
    "        validation_errors.append(f\"Sample {i}: Length mismatch\")\n",
    "    \n",
    "    # Check for valid tags\n",
    "    valid_tags = set(label_list)\n",
    "    for tag in sample['ner_tags']:\n",
    "        if tag not in valid_tags:\n",
    "            validation_errors.append(f\"Sample {i}: Invalid tag '{tag}'\")\n",
    "            break\n",
    "\n",
    "if validation_errors:\n",
    "    print(f\"Found {len(validation_errors)} validation errors:\")\n",
    "    for error in validation_errors[:5]:\n",
    "        print(f\"  - {error}\")\n",
    "else:\n",
    "    print(\"‚úì All validations passed!\")\n",
    "\n",
    "# Check BIO sequence validity\n",
    "bio_errors = 0\n",
    "for sample in test_data_with_predictions:\n",
    "    prev_tag = 'O'\n",
    "    for tag in sample['ner_tags']:\n",
    "        if tag.startswith('I-'):\n",
    "            entity_type = tag[2:]\n",
    "            if not (prev_tag == f'B-{entity_type}' or prev_tag == f'I-{entity_type}'):\n",
    "                bio_errors += 1\n",
    "                break\n",
    "        prev_tag = tag\n",
    "\n",
    "print(f\"\\nBIO sequence errors: {bio_errors}\")\n",
    "if bio_errors == 0:\n",
    "    print(\"‚úÖ All predicted BIO sequences are valid!\")\n",
    "\n",
    "# Save predictions\n",
    "output_file = 'test_data_bert_crf_predictions.jsonl'\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for sample in test_data_with_predictions:\n",
    "        f.write(json.dumps(sample) + '\\n')\n",
    "\n",
    "print(f\"\\nSaved predictions to: {output_file}\")\n",
    "\n",
    "# Generate statistics\n",
    "all_test_tags = []\n",
    "for sample in test_data_with_predictions:\n",
    "    all_test_tags.extend(sample['ner_tags'])\n",
    "\n",
    "tag_counts = Counter(all_test_tags)\n",
    "print(f\"\\nTest prediction statistics:\")\n",
    "print(f\"  Total tokens: {len(all_test_tags):,}\")\n",
    "print(f\"  Tag distribution:\")\n",
    "for tag, count in sorted(tag_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    percentage = count / len(all_test_tags) * 100\n",
    "    print(f\"    {tag:20s}: {count:8,} ({percentage:5.2f}%)\")\n",
    "\n",
    "# Count predicted entities\n",
    "test_entities = []\n",
    "for sample in test_data_with_predictions:\n",
    "    entities = extract_entities(sample['tokens'], sample['ner_tags'])\n",
    "    test_entities.extend(entities)\n",
    "\n",
    "entity_type_counts = Counter(entity_type for _, entity_type, _, _ in test_entities)\n",
    "print(f\"\\nPredicted entities: {len(test_entities):,}\")\n",
    "print(f\"  Entity type distribution:\")\n",
    "for entity_type, count in sorted(entity_type_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"    {entity_type:20s}: {count:6,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"BERT+CRF HYBRID MODEL - SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüìä Model Performance:\")\n",
    "print(f\"   Precision: {bert_crf_results['precision']:.4f}\")\n",
    "print(f\"   Recall:    {bert_crf_results['recall']:.4f}\")\n",
    "print(f\"   F1 Score:  {bert_crf_results['f1']:.4f}\")\n",
    "\n",
    "print(f\"\\nüîß Model Details:\")\n",
    "print(f\"   Base model:           {model_name}\")\n",
    "print(f\"   Architecture:         BERT + CRF\")\n",
    "print(f\"   Parameters:           {bert_crf_results['parameters']:,}\")\n",
    "print(f\"   Number of labels:     {num_labels}\")\n",
    "print(f\"   Training time:        {bert_crf_results['training_time']/60:.1f} minutes\")\n",
    "print(f\"   Training samples:     {len(train_dataset):,}\")\n",
    "print(f\"   Validation samples:   {len(val_dataset):,}\")\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è Training Configuration:\")\n",
    "print(f\"   Learning rate:        {learning_rate}\")\n",
    "print(f\"   Batch size:           {batch_size}\")\n",
    "print(f\"   Epochs:               {num_epochs} (with early stopping)\")\n",
    "print(f\"   Warmup steps:         {warmup_steps}\")\n",
    "print(f\"   Weight decay:         {weight_decay}\")\n",
    "print(f\"   Max sequence length:  128\")\n",
    "print(f\"   Gradient clipping:    1.0\")\n",
    "\n",
    "print(f\"\\nüìã Test Predictions:\")\n",
    "print(f\"   Test sentences:       {len(test_data):,}\")\n",
    "print(f\"   Predicted entities:   {len(test_entities):,}\")\n",
    "print(f\"   Output file:          {output_file}\")\n",
    "print(f\"   Model saved:          bert_crf_best_model.pt\")\n",
    "\n",
    "print(f\"\\n‚úÖ Implementation Status:\")\n",
    "print(f\"   ‚úì BERT + CRF architecture implemented\")\n",
    "print(f\"   ‚úì CRF enforces valid BIO sequences\")\n",
    "print(f\"   ‚úì Viterbi decoding for optimal tag sequences\")\n",
    "print(f\"   ‚úì Evaluated with entity-span level metrics\")\n",
    "print(f\"   ‚úì Generated test predictions\")\n",
    "print(f\"   ‚úì All BIO sequences are valid (checked: {bio_errors == 0})\")\n",
    "\n",
    "# Performance analysis\n",
    "expected_range = (0.90, 0.92)\n",
    "actual_f1 = bert_crf_results['f1']\n",
    "\n",
    "print(f\"\\nüéØ Performance Analysis:\")\n",
    "print(f\"   Expected F1 range:  {expected_range[0]:.2f} - {expected_range[1]:.2f}\")\n",
    "print(f\"   Actual F1 score:    {actual_f1:.4f}\")\n",
    "\n",
    "if actual_f1 >= expected_range[0]:\n",
    "    if actual_f1 <= expected_range[1]:\n",
    "        print(f\"   ‚úÖ Performance meets expectations!\")\n",
    "    else:\n",
    "        print(f\"   üöÄ Performance exceeds expectations!\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  Performance below expected range\")\n",
    "\n",
    "print(f\"\\nüí° Key Strengths:\")\n",
    "print(f\"   ‚Ä¢ BERT's contextual representations + CRF's structure\")\n",
    "print(f\"   ‚Ä¢ Guaranteed valid BIO sequences via CRF\")\n",
    "print(f\"   ‚Ä¢ Better boundary detection\")\n",
    "print(f\"   ‚Ä¢ Global sequence optimization\")\n",
    "print(f\"   ‚Ä¢ Research-proven 1-2% F1 improvement over BERT alone\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  Limitations:\")\n",
    "print(f\"   ‚Ä¢ Slightly slower training than BERT alone\")\n",
    "print(f\"   ‚Ä¢ More complex implementation\")\n",
    "print(f\"   ‚Ä¢ Still large memory footprint\")\n",
    "print(f\"   ‚Ä¢ Requires careful label alignment\")\n",
    "\n",
    "print(f\"\\nüîú Possible Improvements:\")\n",
    "print(f\"   ‚Ä¢ Use larger BERT model (bert-large-cased)\")\n",
    "print(f\"   ‚Ä¢ Add character-level CNN for OOV words\")\n",
    "print(f\"   ‚Ä¢ Implement weighted loss for class imbalance\")\n",
    "print(f\"   ‚Ä¢ Try different learning rate schedules\")\n",
    "print(f\"   ‚Ä¢ Add self-attention layer\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BERT+CRF HYBRID MODEL COMPLETE!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}